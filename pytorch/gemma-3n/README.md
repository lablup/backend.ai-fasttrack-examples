# General language model Fine-tuning Pipeline

ì´ í”„ë¡œì íŠ¸ëŠ” General language modelì— ëŒ€í•´ Hugging Face ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ê³  í‰ê°€í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ Quick Start

### í™˜ê²½ ì„¤ì •

**1. ìë™ í™˜ê²½ ì„¤ì • (ê¶Œì¥)**
```bash
# gemma-3n í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
bash setup_env.sh
```
ì´ë¯¸ ê°€ìƒí™˜ê²½ì´ ì¡´ì¬í•œë‹¤ë©´ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤.

**2. ìˆ˜ë™ í™˜ê²½ ì„¤ì •**
```bash
cd pipeline-code
python -m venv .gemma3n
source .gemma3n/bin/activate
pip install -r requirements.txt
```

**3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
`pipeline-code/.env` íŒŒì¼ì—ì„œ ë‹¤ìŒ ê°’ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:
- `HF_TOKEN`: Hugging Face í† í°
- `MODEL_ID`: ì‚¬ìš©í•  Hugging Faceì˜ model repository ì´ë¦„. ê¸°ë³¸ê°’ì€ `google/gemma-3n-e2b-it`
- `DATASET`: ì‚¬ìš©í•  Hugging Faceì˜ dataset repository ì´ë¦„. ê¸°ë³¸ê°’ì€ `TheFinAI/Fino1_Reasoning_Path_FinQA`
- `WANDB_API_KEY`: Weights & Biases API í‚¤ (ì„ íƒì‚¬í•­)

### ğŸ“š ì‚¬ìš©ì ì •ì˜ í•™ìŠµ ë°ì´í„°ì…‹ ì„¤ì • ê°€ì´ë“œ

ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ íŒŒì´í”„ë¼ì¸ì— ì ìš©í•˜ëŠ” ë°©ë²•ì€ [`pipeline-code/configs/README.md`](pipeline-code/configs/README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

#### ë°©ë²• 1: ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰
```bash
cd pipeline-code
python scripts/cli.py pipeline \
    --train_config_path train_config.yaml \
    --peft_config_path peft_config.yaml
```

#### ë°©ë²• 2: ê°œë³„ íƒœìŠ¤í¬ ì‹¤í–‰ (ê¶Œì¥)
```bash
# Task 1a: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
python scripts/cli.py download-dataset

# Task 1b: ë°ì´í„°ì…‹ ì „ì²˜ë¦¬  
python scripts/cli.py preprocess-dataset

# Task 1b (ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì‚¬ìš©):
python scripts/cli.py preprocess-dataset --config my_messages_format.yaml

# Task 1c: ë°ì´í„°ì…‹ í¬ë§·íŒ…
python scripts/cli.py format-dataset

# Task 2: ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€
python scripts/cli.py eval-base

# Task 3: ëª¨ë¸ íŒŒì¸íŠœë‹
python scripts/cli.py train \
    --train_config_path train_config.yaml \
    --peft_config_path peft_config.yaml

# Task 4: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í‰ê°€
python scripts/cli.py eval-finetuned
```

## ğŸ“ íŒŒì¼ êµ¬ì¡° ë° ë°ì´í„° ê²½ë¡œ

### ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
- **ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ**: `{í”„ë¡œì íŠ¸_ë£¨íŠ¸}/dataset/`
- **PEFT ì–´ëŒ‘í„° ì €ì¥ ê²½ë¡œ**: `{í”„ë¡œì íŠ¸_ë£¨íŠ¸}/results/model/`
- **ë°°í¬ìš© ëª¨ë¸ ì €ì¥ ê²½ë¡œ**: `{í”„ë¡œì íŠ¸_ë£¨íŠ¸}/results/deployment_model/`
- **í‰ê°€ ê²°ê³¼ ì €ì¥**: `{í”„ë¡œì íŠ¸_ë£¨íŠ¸}/results/evaluation/`

- (FastTrack pipelineìš© í´ë” ì„¤ê³„) í™•ì¥ ì˜ˆì •

### íŒŒì¼ êµ¬ì¡°
```
gemma-3n/
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ cli.py              # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ CLI
â””â”€â”€ pipeline-code
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ settings.py          # ì „ì—­ ì„¤ì •
    â”‚   â”œâ”€â”€ train_config.yaml    # í›ˆë ¨ ì¸ì ì„¤ì •
    â”‚   â”œâ”€â”€ peft_config.yaml     # PEFT(LoRA) ì„¤ì •
    â”‚   â””â”€â”€ messages_format.yaml # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì„¤ì •
    â”œâ”€â”€ data/                    # ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥ì†Œ
    â”œâ”€â”€ results/
    â”‚   â”œâ”€â”€ model/              # PEFT ì–´ëŒ‘í„° ì €ì¥ì†Œ
    â”‚   â”œâ”€â”€ deployment_model/   # LoRA ê°€ì¤‘ì¹˜ê°€ ë³‘í•©ëœ ë°°í¬ìš© ëª¨ë¸ ì €ì¥ì†Œ
    â”‚   â””â”€â”€ evaluation/         # í‰ê°€ ê²°ê³¼ ì €ì¥ì†Œ
    â”œâ”€â”€ logs/                   # í›ˆë ¨ ë¡œê·¸ ì €ì¥ì†Œ
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”œâ”€â”€ download_dataset.py # Task 1a: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    â”‚   â”‚   â”œâ”€â”€ preprocess_dataset.py # Task 1b: ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    â”‚   â”‚   â”œâ”€â”€ format_dataset.py   # Task 1c: ë°ì´í„°ì…‹ í¬ë§·íŒ…
    â”‚   â”‚   â””â”€â”€ dataset.py          # í†µí•© ë°ì´í„°ì…‹ íŒŒì´í”„ë¼ì¸ (í˜¸í™˜ì„± ìœ ì§€)
    â”‚   â”œâ”€â”€ evaluation/
    â”‚   â”‚   â””â”€â”€ evaluation.py       # ëª¨ë¸ í‰ê°€
    â”‚   â”œâ”€â”€ models/
    â”‚   â”‚   â””â”€â”€ model.py           # ëª¨ë¸ ë¡œë”©
    â”‚   â””â”€â”€ training/
    â”‚       â””â”€â”€ trainer.py         # ëª¨ë¸ íŒŒì¸íŠœë‹
    â”œâ”€â”€ .env                    # í™˜ê²½ ë³€ìˆ˜
    â””â”€â”€ requirements.txt        # ì˜ì¡´ì„±

```

## ğŸ”§ íƒœìŠ¤í¬ ìƒì„¸ ì„¤ëª…

### Task 1: Dataset Pipeline (ì„¸ë¶„í™”ëœ íƒœìŠ¤í¬)

#### Task 1a: Dataset Download
- **íŒŒì¼**: `src/data/download_dataset.py`
- **ì…ë ¥**: Hugging Face ë°ì´í„°ì…‹ (`TheFinAI/Fino1_Reasoning_Path_FinQA`)
- **ì²˜ë¦¬**: ë°ì´í„°ì…‹ì„ train/validation/testë¡œ ë¶„í• 
- **ì¶œë ¥**: `dataset/raw/` í´ë”ì— ì›ë³¸ ë°ì´í„°ì…‹ ì €ì¥

#### Task 1b: Dataset Preprocessing  
- **íŒŒì¼**: `src/data/preprocess_dataset.py`
- **ì…ë ¥**: `dataset/raw/` í´ë”ì˜ ì›ë³¸ ë°ì´í„°ì…‹
- **ì„¤ì •**: `configs/messages_format.yaml` (ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥)
- **ì²˜ë¦¬**: 
  - ì„¤ì • íŒŒì¼ ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬ (ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ì§€ì›)
  - ì»¬ëŸ¼ ë§¤í•‘ì„ í†µí•œ ì¼ë°˜í™”ëœ ì „ì²˜ë¦¬
  - ë°ì´í„° í’ˆì§ˆ í•„í„°ë§, ì •ì œ
  - messages êµ¬ì¡°ë¡œ ë³€í™˜ (train/validation: í•™ìŠµìš©, test: í‰ê°€ìš©)
  - system prompt, user, assistant content êµ¬ì¡°í™”
- **ì¶œë ¥**: `dataset/preprocessed/` í´ë”ì— messages êµ¬ì¡°ì˜ ë°ì´í„°ì…‹ ì €ì¥

#### âš™ï¸ ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ì„¤ì •
`configs/messages_format.yaml` íŒŒì¼ì„ í†µí•´ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ì ìš© ê°€ëŠ¥:
ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ íŒŒì´í”„ë¼ì¸ì— ì ìš©í•˜ëŠ” ìì„¸í•œ ë°©ë²•ì€ [`pipeline-code/configs/README.md`](pipeline-code/configs/README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```yaml
# ì»¬ëŸ¼ ë§¤í•‘ (key: ë³€ìˆ˜ëª…, value: ì‹¤ì œ ë°ì´í„°ì…‹ ì»¬ëŸ¼ëª…)
training_columns:
  question: "Open-ended Verifiable Question"
  cot: "Complex_CoT" 
  response: "Response"

# messages í¬ë§· í…œí”Œë¦¿
messages_format:
  system_prompt: |
    Below is an instruction that describes a task...
  messages:
    - role: "system"
      content: "{system_prompt}"
    - role: "user"
      content: "{question}"
    - role: "assistant"
      content: "<think>\n{cot}\n</think>\n{response}"

# í‰ê°€ìš© ì»¬ëŸ¼ ë§¤í•‘
evaluate_columns:
  query: "Open-ended Verifiable Question"
  response: "Response"
```

#### Task 1c: Dataset Formatting
- **íŒŒì¼**: `src/data/format_dataset.py`
- **ì…ë ¥**: `dataset/preprocessed/` í´ë”ì˜ messages êµ¬ì¡° ë°ì´í„°ì…‹
- **ì²˜ë¦¬**: 
  - processor.apply_chat_template ì ìš©
  - train/validation: í•™ìŠµìš© í…ìŠ¤íŠ¸ ìƒì„±
  - test: í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ì™€ reference ìƒì„±
- **ì¶œë ¥**: `dataset/formatted/` í´ë”ì— ìµœì¢… í¬ë§·íŒ…ëœ ë°ì´í„°ì…‹ ì €ì¥

### Task 2: í•™ìŠµ ì „ Base Model Evaluation
- **ì…ë ¥**: ì›ë³¸ Gemma-3n ëª¨ë¸, ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
- **ì²˜ë¦¬**: ROUGE, BERTScore ë“±ì˜ ì§€í‘œë¡œ ë² ì´ìŠ¤ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- **ì¶œë ¥**: `base_model_evaluation.json`

### Task 3: Model Fine-tuning
- **ì…ë ¥**: ë² ì´ìŠ¤ ëª¨ë¸, í¬ë§·íŒ…ëœ í›ˆë ¨/ê²€ì¦ ë°ì´í„°, ì„¤ì • íŒŒì¼ë“¤
- **ì²˜ë¦¬**: LoRAë¥¼ ì‚¬ìš©í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- **ì¶œë ¥**: 
  - PEFT ì–´ëŒ‘í„°: `results/model/` í´ë”
  - ë°°í¬ìš© ì™„ì „í•œ ëª¨ë¸: `results/deployment_model/` í´ë” (LoRA ê°€ì¤‘ì¹˜ ë³‘í•©ë¨)

### Task 4: Fine-tuned Model Evaluation
- **ì…ë ¥**: íŒŒì¸íŠœë‹ëœ ëª¨ë¸, ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
- **ì²˜ë¦¬**: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€
- **ì¶œë ¥**: `finetuned_model_evaluation.json`

## ğŸ“Š ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

### í›ˆë ¨ ì¸ì ìˆ˜ì • (`configs/train_config.yaml`)
- ë°°ì¹˜ í¬ê¸°, í•™ìŠµë¥ , ì—í¬í¬ ìˆ˜ ë“± ì¡°ì • ê°€ëŠ¥
- Weights & Biases ë¡œê¹… ì„¤ì •

### PEFT ì„¤ì • ìˆ˜ì • (`configs/peft_config.yaml`)
- LoRA rank, alpha, dropout ë“± ì¡°ì • ê°€ëŠ¥
- íƒ€ê²Ÿ ëª¨ë“ˆ ì„ íƒ

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **GPU ë©”ëª¨ë¦¬**: Gemma-3nì€ ëŒ€ìš©ëŸ‰ ëª¨ë¸ì´ë¯€ë¡œ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
2. **Hugging Face í† í°**: ëª¨ë¸ ì ‘ê·¼ì„ ìœ„í•´ ìœ íš¨í•œ HF í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.
3. **ë°ì´í„° ì €ì¥ ê³µê°„**: ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ì„ í™•ë³´í•˜ì„¸ìš”.

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

- Weights & Biasesë¥¼ í†µí•œ ì‹¤ì‹œê°„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§
- í‰ê°€ ê²°ê³¼ëŠ” JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥

## ğŸš€ ë°°í¬ ë° ì‚¬ìš©

### íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ë°©ë²•

íŒŒì¸íŠœë‹ ì™„ë£Œ í›„, ë³‘í•©ëœ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from transformers import AutoModelForCausalLM, AutoProcessor

# 1. ë¡œì»¬ì—ì„œ ë°°í¬ìš© ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("./results/deployment_model/")
processor = AutoProcessor.from_pretrained("./results/deployment_model/")

# 2. ì¶”ë¡  ì‹¤í–‰
messages = [{"role": "user", "content": "What is the impact of inflation on stock prices?"}]
prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
response = processor.decode(outputs[0], skip_special_tokens=True)
```

### Hugging Face Hubì— ë°°í¬

```bash
# Hugging Face CLI ì„¤ì¹˜ ë° ë¡œê·¸ì¸
pip install huggingface_hub
huggingface-cli login

# ëª¨ë¸ ì—…ë¡œë“œ
huggingface-cli upload your-username/gemma-3n-financial-qa ./results/deployment_model/
```

### ëª¨ë¸ êµ¬ì¡°

- **PEFT ì–´ëŒ‘í„°**: `results/model/` - LoRA ê°€ì¤‘ì¹˜ë§Œ í¬í•¨, ì‘ì€ íŒŒì¼ í¬ê¸°
- **ë°°í¬ìš© ëª¨ë¸**: `results/deployment_model/` - LoRA ê°€ì¤‘ì¹˜ê°€ ë³‘í•©ëœ ì™„ì „í•œ ëª¨ë¸, ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
