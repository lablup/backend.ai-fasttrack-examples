import os
import argparse
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer

def parse_args():
    parser = argparse.ArgumentParser(description="Model Loader")
    parser.add_argument('--model_id', type=str, default=os.getenv('MODEL_ID'),
                        help='ID of the model to load from Hugging Face Hub')
    
    return parser.parse_args()

def load_model(model_id):
    """
    Load a model from Hugging Face Hub using AutoModelForCausalLM for universal compatibility.
    """
    try:
        print(f"Loading model: {model_id}")
        model = AutoModelForCausalLM.from_pretrained(
            model_id, 
            device_map="auto", 
            torch_dtype=torch.bfloat16, 
            token=os.getenv('HF_TOKEN')
        )
        print(f"âœ… Successfully loaded model: {model_id}")
        return model
    except Exception as e:
        print(f"âŒ Error loading model {model_id}: {e}")
        return None

def load_processor_and_tokenizer(model_id):
    """
    Load processor and extract tokenizer for both multimodal and text-only models.
    Supports fallback to AutoTokenizer for deployment models without preprocessor_config.json
    
    Returns:
        tuple: (processor, tokenizer) where tokenizer is always accessible
    """
    # 1ì°¨ ì‹œë„: AutoProcessor (ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›)
    try:
        print(f"Loading processor for model: {model_id}")
        processor = AutoProcessor.from_pretrained(model_id, token=os.getenv('HF_TOKEN'))
        
        # ë©€í‹°ëª¨ë‹¬ vs í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ êµ¬ë¶„
        try:
            # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì¸ ê²½ìš° processor.tokenizer ì†ì„±ì´ ì¡´ì¬
            tokenizer = processor.tokenizer
            print("âœ… Multimodal model detected. Extracted tokenizer from processor.")
            return processor, tokenizer
        except AttributeError:
            # í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ì¸ ê²½ìš° processor ìì²´ê°€ tokenizer
            print("âœ… Text-only model detected. Using processor as tokenizer.")
            return processor, processor
            
    except Exception as e:
        print(f"âš ï¸ AutoProcessor loading failed: {e}")
        print("ğŸ”„ Falling back to AutoTokenizer...")
        
        # 2ì°¨ ì‹œë„: AutoTokenizer (deployment ëª¨ë¸ ë“±ì—ì„œ ì‚¬ìš©)
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.getenv('HF_TOKEN'))
            print("âœ… Successfully loaded tokenizer using AutoTokenizer fallback.")
            return None, tokenizer  # processorëŠ” None, tokenizerë§Œ ë°˜í™˜
            
        except Exception as tokenizer_error:
            print(f"âŒ AutoTokenizer fallback also failed: {tokenizer_error}")
            return None, None

class ModelLoader:
    def __init__(self, model_id):
        self.model_id = model_id
        self.model = None
        self.processor = None
        self.tokenizer = None

        if self.model_id:
            self.model = load_model(self.model_id)
            self.processor, self.tokenizer = load_processor_and_tokenizer(self.model_id)

            if not self.tokenizer:
                print(f"âŒ Failed to load tokenizer for {self.model_id}")
            elif not self.model:
                print(f"âŒ Failed to load model for {self.model_id}")
            else:
                # ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì„¤ì •
                self.tokenizer.padding_side = "left"
                print(f"âœ… Successfully loaded model and tokenizer for {self.model_id}")
                
                # processorê°€ ì—†ì–´ë„ tokenizerê°€ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                if not self.processor:
                    print("â„¹ï¸ Processor not available, but tokenizer loaded successfully (AutoTokenizer fallback)")
        else:
            print("âŒ Model ID is not provided. Please set the MODEL_ID environment variable.")