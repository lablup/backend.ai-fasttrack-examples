import os
import argparse
import json
import torch
import logging
import sys
import re
from pathlib import Path
from datasets import load_from_disk  # 'load_dataset' ëŒ€ì‹  'load_from_disk'ë¥¼ ì‚¬ìš©
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    pipeline,
)
from peft import PeftModel
from tqdm import tqdm
import evaluate

# # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
# project_root = Path(__file__).parent.parent.parent
# sys.path.insert(0, str(project_root))

from src.models.model import ModelLoader
from configs.settings import settings

# tqdmì„ í†µí•œ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ ë¡œê¹… ë ˆë²¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.
logging.getLogger("transformers.pipelines.pt_utils").setLevel(logging.ERROR)

def detect_language(text_sample):
    """
    í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë¶„ì„í•˜ì—¬ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” í•¨ìˆ˜ (í•œêµ­ì–´/ì˜ì–´/ê¸°íƒ€ë¡œ ê°„ì†Œí™”)
    
    Args:
        text_sample (str): ë¶„ì„í•  í…ìŠ¤íŠ¸ ìƒ˜í”Œ
        
    Returns:
        dict: {"language": str, "confidence": str}
    """
    if not text_sample or len(text_sample.strip()) == 0:
        return {"language": "other", "confidence": "low"}
    
    text = text_sample[:500]  # ì²˜ìŒ 500ìë§Œ ë¶„ì„
    
    # í•œê¸€ ë¬¸ì ê°ì§€
    korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
    
    # ì˜ì–´ ë¬¸ì ê°ì§€ (ë¼í‹´ ë¬¸ì + ì˜ì–´ ë‹¨ì–´ íŒ¨í„´)
    latin_chars = len(re.findall(r'[a-zA-Z]', text))
    english_words = len(re.findall(r'\b(the|and|or|but|in|on|at|to|for|of|with|by|is|are|was|were|a|an|this|that|will|have|has|can|could|would|should)\b', text.lower()))
    
    total_chars = len(re.findall(r'[^\s\d\W]', text))
    total_words = len(re.findall(r'\b\w+\b', text))
    
    if total_chars == 0:
        return {"language": "other", "confidence": "low"}
    
    # í•œêµ­ì–´ ë¹„ìœ¨ ê³„ì‚°
    korean_ratio = korean_chars / total_chars
    
    # ì˜ì–´ ë¹„ìœ¨ ê³„ì‚° (ë¼í‹´ ë¬¸ì ë¹„ìœ¨ + ì˜ì–´ ë‹¨ì–´ ë¹„ìœ¨)
    latin_ratio = latin_chars / total_chars
    english_word_ratio = english_words / total_words if total_words > 0 else 0
    
    # ì–¸ì–´ ê²°ì • ë¡œì§
    if korean_ratio > 0.3:  # í•œêµ­ì–´ ë¬¸ìê°€ 30% ì´ìƒ
        return {"language": "korean", "confidence": "high" if korean_ratio > 0.7 else "medium"}
    
    elif latin_ratio > 0.5 and english_word_ratio > 0.05:  # ë¼í‹´ ë¬¸ì 50% + ì˜ì–´ ë‹¨ì–´ 5% ì´ìƒ
        confidence = "high" if english_word_ratio > 0.15 else "medium"
        return {"language": "english", "confidence": confidence}
    
    else:  # ê¸°íƒ€ ì–¸ì–´ (ì¤‘êµ­ì–´, ì¼ë³¸ì–´, ì•„ëì–´ ë“±)
        return {"language": "other", "confidence": "medium"}

def get_bertscore_model_config(language_info):
    """
    ì–¸ì–´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ BERTScore ëª¨ë¸ ì„¤ì •ì„ ë°˜í™˜ (ê°„ì†Œí™”ëœ ë²„ì „)
    
    Args:
        language_info (dict): detect_language í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
        
    Returns:
        dict: {"model_type": str, "lang": str, "description": str}
    """
    language = language_info["language"]
    confidence = language_info["confidence"]
    
    # í•œêµ­ì–´: ë‹¤êµ­ì–´ BERT ì‚¬ìš© (í•œêµ­ì–´ì— ìµœì í™”)
    if language == "korean":
        return {
            "model_type": "bert-base-multilingual-cased",
            "lang": "ko",
            "description": "Multilingual BERT for Korean"
        }
    
    # ì˜ì–´: DeBERTa-v3 ì‚¬ìš© (ì˜ì–´ ì„±ëŠ¥ ìš°ìˆ˜)
    elif language == "english" and confidence == "high":
        return {
            "model_type": "microsoft/deberta-v3-large",
            "lang": "en", 
            "description": "DeBERTa-v3 for English"
        }
    
    # ê¸°íƒ€ ëª¨ë“  ì–¸ì–´ ë˜ëŠ” ë‚®ì€ ì‹ ë¢°ë„: ë‹¤êµ­ì–´ BERT ì‚¬ìš©
    else:
        return {
            "model_type": "bert-base-multilingual-cased",
            "lang": None,  # ìë™ ì–¸ì–´ ê°ì§€
            "description": "Multilingual BERT for other languages"
        }

def parse_args():
    """CLI ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Evaluate a model on a pre-processed local dataset.")
    parser.add_argument('--model_name_or_path', type=str, default=os.getenv('MODEL_ID'),required=True, help='Base model identifier')
    # '--dataset_name'ì„ '--dataset_path'ë¡œ ë³€ê²½í•˜ì—¬ ë¡œì»¬ ê²½ë¡œë¥¼ ë°›ìŠµë‹ˆë‹¤.
    parser.add_argument('--output_path', type=str, default="output.json", help='Path to save the evaluation results.')
    parser.add_argument('--max_samples', type=int, default=None, help='Optional: Maximum number of samples for quick testing.')
    parser.add_argument('--use_adapter', action='store_true', 
                        help='Use a PEFT adapter if this flag is set. If not, evaluates the base model.')
    return parser.parse_args()

def load_model_for_evaluation(model_name_or_path, use_finetuned=False):
    """ë² ì´ìŠ¤ ëª¨ë¸ ë˜ëŠ” íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (AutoModelForCausalLM ì‚¬ìš©)"""
    
    if use_finetuned:
        # 1. ë¨¼ì € ë°°í¬ìš© ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        deployment_model_path = settings.deployment_model_path
        if deployment_model_path.exists() and (deployment_model_path / "config.json").exists():
            print(f"Found deployment-ready fine-tuned model at: {deployment_model_path}")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    str(deployment_model_path),
                        device_map="auto", 
                        torch_dtype=torch.float16,
                        local_files_only=True  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                    )
                print(f"âœ… Successfully loaded deployment-ready fine-tuned model from {deployment_model_path}")
                return model
            except Exception as e:
                print(f"âš ï¸ Failed to load deployment model: {e}")
                print("Falling back to adapter-based loading...")
        
        # 2. ë³‘í•©ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì–´ëŒ‘í„° ë°©ì‹ìœ¼ë¡œ ë¡œë”©
        adapter_path = settings.save_model_path / model_name_or_path
        if adapter_path.exists():
            print(f"Loading base model and applying PEFT adapter from: {adapter_path}")
            
            # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            base_model = AutoModelForCausalLM.from_pretrained(
                model_name_or_path, 
                device_map="auto", 
                torch_dtype=torch.float16, 
                token=os.getenv('HF_TOKEN')
            )
            
            # PEFT ì–´ëŒ‘í„° ì ìš©
            try:
                model = PeftModel.from_pretrained(
                    base_model, 
                    str(adapter_path), 
                    device_map='auto', 
                    torch_dtype=torch.float16
                )
                print(f"âœ… Successfully loaded model with PEFT adapter from {adapter_path}")
                return model
            except Exception as e:
                print(f"âŒ Failed to load PEFT adapter: {e}")
                print("Using base model instead")
                return base_model
        else:
            print(f"âŒ No fine-tuned model found at {adapter_path}")
            print("Using base model instead")
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
    print(f"Loading base model: {model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path, 
        device_map="auto", 
        torch_dtype=torch.float16, 
        token=os.getenv('HF_TOKEN')
    )
    
    print(f"âœ… Successfully loaded base model: {model_name_or_path}")
    return model

def main():
    args = parse_args()
    print("--- Starting Evaluation Script on Local Dataset ---")
    
    # 1. í‰ê°€ ì§€í‘œ ë¡œë“œ - í•œêµ­ì–´ í…ìŠ¤íŠ¸ í‰ê°€ì— ì í•©í•œ ë©”íŠ¸ë¦­ ì‚¬ìš©
    print("Loading evaluation metrics...")
    rouge_metric = evaluate.load("rouge")
    bertscore_metric = evaluate.load("bertscore")
    
    # BLEU ë©”íŠ¸ë¦­ ì¶”ê°€ (ë‹¤êµ­ì–´ ì§€ì›)
    try:
        bleu_metric = evaluate.load("bleu")
        print("âœ… BLEU metric loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to load BLEU metric: {e}")
        bleu_metric = None
    
    # Perplexity ë©”íŠ¸ë¦­ ì¶”ê°€ (ì–¸ì–´ ëª¨ë¸ í’ˆì§ˆ í‰ê°€)
    try:
        perplexity_metric = evaluate.load("perplexity", module_type="metric")
        print("âœ… Perplexity metric loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to load Perplexity metric: {e}")
        perplexity_metric = None
    
    # 2. ëª¨ë¸ ë¡œë”ë¥¼ í†µí•´ tokenizer ë¡œë“œ
    processor_path = args.model_name_or_path
    if args.use_adapter and settings.deployment_model_path.exists() and (settings.deployment_model_path / "tokenizer_config.json").exists():
        processor_path = str(settings.deployment_model_path)
        print(f"Loading tokenizer from fine-tuned model: {processor_path}")
    else:
        print(f"Loading tokenizer from base model: {processor_path}")
    
    # ModelLoader ì‚¬ìš© (ëª¨ë¸ì€ ë”°ë¡œ ë¡œë“œí•  ì˜ˆì •ì´ë¯€ë¡œ tokenizerë§Œ í•„ìš”)
    model_loader = ModelLoader(processor_path)
    
    if not model_loader.tokenizer:
        print(f"âŒ Failed to load tokenizer from {processor_path}")
        return
    
    tokenizer = model_loader.tokenizer
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • - íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì´ì „ taskì˜ outputì„ input1ì—ì„œ ì½ìŒ
    if settings.is_pipeline_env:
        readonly_dataset_path = settings.pipeline_input_path
        # ì½ê¸° ì „ìš© ê²½ë¡œë¥¼ ì“°ê¸° ê°€ëŠ¥í•œ ì„ì‹œ ê²½ë¡œë¡œ ë³µì‚¬
        dataset_path = settings.copy_readonly_to_writable(readonly_dataset_path, 'evaluation')
    else:
        dataset_path = settings.save_dataset_path_formatted
        
    try:
        # load_from_diskë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ëœ ë°ì´í„°ì…‹ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        full_dataset = load_from_disk(dataset_path)
        # í‰ê°€ì—ëŠ” 'test' ìŠ¤í”Œë¦¿ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        test_dataset = full_dataset['test']
        print(f"Successfully loaded test split from {dataset_path}")
        print(f"Test dataset columns: {test_dataset.column_names}")
    except FileNotFoundError:
        print(f"Error: Dataset directory not found at {dataset_path}")
        return
    except Exception as e:
        print(f"Failed to load dataset from disk: {e}")
        return

    # 3. í‰ê°€í•  ëª¨ë¸ ë¡œë“œ - ë³‘í•©ëœ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
    model = load_model_for_evaluation(args.model_name_or_path, use_finetuned=args.use_adapter)
    # 4. íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipe = pipeline("text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    torch_dtype=torch.float16, # ë°ì´í„° íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
                    max_new_tokens=256,
                    device_map="auto"
                    )

    # 5. í‰ê°€ ì‹¤í–‰: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì§ì ‘ ì‚¬ìš©
    if args.max_samples:
        test_dataset = test_dataset.select(range(args.max_samples))
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
    batch_size = int(os.getenv('EVAL_BATCH_SIZE', 8))
    
    all_predictions = []
    all_references = []
    all_prompts = []
    
    print(f"Evaluating on {len(test_dataset)} samples with batch size {batch_size}...")
    
    # tqdmìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì§„í–‰ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    for i in tqdm(range(0, len(test_dataset), batch_size)):
        # 1. í˜„ì¬ ë°°ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°(ë”•ì…”ë„ˆë¦¬)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        batch = test_dataset[i : i + batch_size]
        
        # 2. ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ promptì™€ reference ì§ì ‘ ì¶”ì¶œ
        batch_prompts = batch.get("prompt", [])
        batch_references = batch.get("reference", [])
        
        # ë¹ˆ ê°’ í•„í„°ë§ ë° ê²€ì¦
        valid_indices = []
        filtered_prompts = []
        filtered_references = []
        
        for idx, (prompt, ref) in enumerate(zip(batch_prompts, batch_references)):
            if prompt and ref and len(str(prompt).strip()) > 0 and len(str(ref).strip()) > 0:
                valid_indices.append(idx)
                filtered_prompts.append(str(prompt).strip())
                filtered_references.append(str(ref).strip())
        
        if not filtered_prompts:
            print(f"âš ï¸ Skipping batch {i//batch_size + 1}: No valid prompt-reference pairs")
            continue

        # 3. íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í˜„ì¬ ë°°ì¹˜ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
        try:
            generated_outputs = pipe(filtered_prompts, batch_size=len(filtered_prompts), eos_token_id=tokenizer.eos_token_id)
        except Exception as e:
            print(f"âš ï¸ Generation failed for batch {i//batch_size + 1}: {e}")
            continue
        
        # 4. ìƒì„±ëœ ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        batch_predictions = []
        for out, prompt in zip(generated_outputs, filtered_prompts):
            try:
                generated_text = out[0]['generated_text']
                # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ì—¬ ìˆœìˆ˜ ìƒì„± í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                prediction = generated_text.replace(prompt, '').strip()
                batch_predictions.append(prediction if prediction else "[EMPTY_GENERATION]")
            except Exception as e:
                print(f"âš ï¸ Failed to extract prediction: {e}")
                batch_predictions.append("[EXTRACTION_ERROR]")
        
        all_predictions.extend(batch_predictions)
        all_references.extend(filtered_references)
        all_prompts.extend(filtered_prompts)

    # 5. ì •ëŸ‰ì  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ë° ê²°ê³¼ ì €ì¥ (ê°œì„ ëœ í‰ê°€ ë©”íŠ¸ë¦­)
    print("\nCalculating quantitative metrics...")
    
    # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
    if not all_predictions or not all_references:
        print("âŒ No valid predictions or references found. Evaluation cannot proceed.")
        return
    
    if len(all_predictions) != len(all_references):
        print(f"âš ï¸ Mismatch in prediction/reference counts: {len(all_predictions)} vs {len(all_references)}")
        min_len = min(len(all_predictions), len(all_references))
        all_predictions = all_predictions[:min_len]
        all_references = all_references[:min_len]
        all_prompts = all_prompts[:min_len]
    
    print(f"ğŸ“Š Valid samples for evaluation: {len(all_predictions)}")
    
    # ROUGE ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ìš”ì•½ í‰ê°€ì˜ í‘œì¤€ ë©”íŠ¸ë¦­)
    print("Computing ROUGE scores...")
    try:
        rouge_scores = rouge_metric.compute(predictions=all_predictions, references=all_references)
        
        # ROUGE ì ìˆ˜ ê²€ì¦
        for metric_name, score in rouge_scores.items():
            if not (0 <= score <= 1):
                print(f"âš ï¸ Unusual ROUGE {metric_name} score: {score:.4f}")
        
        print(f"âœ… ROUGE scores computed successfully")
        
    except Exception as e:
        print(f"âŒ ROUGE calculation failed: {e}")
        rouge_scores = {
            "rouge1": 0.0,
            "rouge2": 0.0,
            "rougeL": 0.0,
            "rougeLsum": 0.0,
            "error": str(e)
        }
    
    # BERTScore ê³„ì‚° - ë‹¤êµ­ì–´ ì§€ì› ë° ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
    print("Computing BERTScore...")
    bert_scores_result = None
    try:
        # ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¶„ì„ (ë” ë§ì€ ìƒ˜í”Œ ì‚¬ìš©)
        sample_texts = []
        sample_size = min(10, len(all_references), len(all_predictions))
        for i in range(sample_size):
            if all_references[i] and all_predictions[i]:
                sample_texts.append(all_references[i][:100] + " " + all_predictions[i][:100])
        
        combined_sample = " ".join(sample_texts)[:1000]  # ìµœëŒ€ 1000ì ë¶„ì„
        
        # ì–¸ì–´ ê°ì§€ ì‹¤í–‰
        language_info = detect_language(combined_sample)
        print(f"ï¿½ Language detection: {language_info['language']} (confidence: {language_info['confidence']})")
        print(f"ğŸ“Š Script ratios: {language_info.get('ratios', {})}")
        
        # ìµœì  BERTScore ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        model_config = get_bertscore_model_config(language_info)
        print(f"ğŸ¯ Selected model: {model_config['description']}")
        
        # BERTScore ê³„ì‚° ì‹¤í–‰
        bert_compute_kwargs = {
            "predictions": all_predictions,
            "references": all_references,
            "model_type": model_config["model_type"]
        }
        
        # ì–¸ì–´ ì½”ë“œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        if model_config["lang"]:
            bert_compute_kwargs["lang"] = model_config["lang"]
        
        bert_scores = bertscore_metric.compute(**bert_compute_kwargs)
        
        # BERTScore ê²°ê³¼ ì²˜ë¦¬ ë° ê²€ì¦
        if bert_scores and 'f1' in bert_scores and len(bert_scores['f1']) > 0:
            # ìœ íš¨í•˜ì§€ ì•Šì€ ì ìˆ˜ í•„í„°ë§ (NaN, ë¬´í•œëŒ€ ë“±)
            valid_f1 = [score for score in bert_scores['f1'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            valid_precision = [score for score in bert_scores['precision'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            valid_recall = [score for score in bert_scores['recall'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            
            if len(valid_f1) == 0:
                raise ValueError("All BERTScore F1 scores are invalid (NaN/Inf)")
            
            avg_bert_f1 = sum(valid_f1) / len(valid_f1)
            avg_bert_precision = sum(valid_precision) / len(valid_precision)
            avg_bert_recall = sum(valid_recall) / len(valid_recall)
            
            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (BERTScoreëŠ” ë³´í†µ 0~1 ë²”ìœ„)
            if not (0 <= avg_bert_f1 <= 1) or not (0 <= avg_bert_precision <= 1) or not (0 <= avg_bert_recall <= 1):
                print(f"âš ï¸ Unusual BERTScore values detected: F1={avg_bert_f1:.4f}, P={avg_bert_precision:.4f}, R={avg_bert_recall:.4f}")
            
            bert_scores_result = {
                "f1_avg": avg_bert_f1,
                "precision_avg": avg_bert_precision,
                "recall_avg": avg_bert_recall,
                "f1_scores": valid_f1[:5],  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œì˜ ê°œë³„ ì ìˆ˜
                "model_used": model_config["model_type"],
                "language_detected": language_info["language"],
                "language_confidence": language_info["confidence"],
                "valid_samples": len(valid_f1),
                "total_samples": len(bert_scores['f1'])
            }
            print(f"âœ… BERTScore computed successfully (F1: {avg_bert_f1:.4f}, P: {avg_bert_precision:.4f}, R: {avg_bert_recall:.4f})")
            print(f"ğŸ“ˆ Valid/Total samples: {len(valid_f1)}/{len(bert_scores['f1'])}")
        else:
            raise ValueError("Empty or invalid BERTScore results")
            
    except Exception as e:
        print(f"âš ï¸ Primary BERTScore calculation failed: {e}")
        print("ğŸ”„ Trying fallback models...")
        
        # í´ë°± ì‹œí€€ìŠ¤: ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„
        fallback_models = [
            {"model_type": "bert-base-multilingual-cased", "lang": None, "desc": "Multilingual BERT"},
            {"model_type": "distilbert-base-uncased", "lang": "en", "desc": "DistilBERT English"},
            {"model_type": "distilbert-base-multilingual-cased", "lang": None, "desc": "DistilBERT Multilingual"}
        ]
        
        for fallback in fallback_models:
            try:
                print(f"ğŸ”„ Trying {fallback['desc']}...")
                fallback_kwargs = {
                    "predictions": all_predictions,
                    "references": all_references,
                    "model_type": fallback["model_type"]
                }
                if fallback["lang"]:
                    fallback_kwargs["lang"] = fallback["lang"]
                
                bert_scores = bertscore_metric.compute(**fallback_kwargs)
                
                if bert_scores and 'f1' in bert_scores and len(bert_scores['f1']) > 0:
                    avg_bert_f1 = sum(bert_scores['f1']) / len(bert_scores['f1'])
                    avg_bert_precision = sum(bert_scores['precision']) / len(bert_scores['precision'])
                    avg_bert_recall = sum(bert_scores['recall']) / len(bert_scores['recall'])
                    
                    bert_scores_result = {
                        "f1_avg": avg_bert_f1,
                        "precision_avg": avg_bert_precision,
                        "recall_avg": avg_bert_recall,
                        "model_used": fallback["model_type"],
                        "language_detected": "fallback",
                        "language_confidence": "unknown",
                        "fallback_used": True
                    }
                    print(f"âœ… Fallback BERTScore computed with {fallback['desc']} (F1: {avg_bert_f1:.4f})")
                    break
                    
            except Exception as fallback_error:
                print(f"âŒ {fallback['desc']} failed: {fallback_error}")
                continue
        
        # ëª¨ë“  í´ë°± ì‹¤íŒ¨
        if bert_scores_result is None:
            print(f"âŒ All BERTScore models failed")
            bert_scores_result = {
                "f1_avg": 0.0,
                "precision_avg": 0.0,
                "recall_avg": 0.0,
                "error": "All models failed",
                "model_used": "none",
                "language_detected": "unknown",
                "language_confidence": "unknown"
            }
    
    # BLEU ì ìˆ˜ ê³„ì‚° (ê¸°ê³„ ë²ˆì—­/ìƒì„± íƒœìŠ¤í¬ì˜ í‘œì¤€ ë©”íŠ¸ë¦­)
    bleu_scores = None
    if bleu_metric:
        print("Computing BLEU scores...")
        try:
            # BLEUëŠ” referencesë¥¼ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ìš”êµ¬í•¨
            references_for_bleu = [[ref] for ref in all_references]
            bleu_scores = bleu_metric.compute(predictions=all_predictions, references=references_for_bleu)
            
            # BLEU ì ìˆ˜ ê²€ì¦ (0~1 ë²”ìœ„ì—¬ì•¼ í•¨)
            bleu_score = bleu_scores.get('bleu', 0)
            if not (0 <= bleu_score <= 1):
                print(f"âš ï¸ Unusual BLEU score: {bleu_score:.4f}")
            
            print(f"âœ… BLEU score computed: {bleu_score:.4f}")
            
        except Exception as e:
            print(f"âš ï¸ BLEU calculation failed: {e}")
            bleu_scores = {"bleu": 0.0, "error": str(e)}
    else:
        print("âš ï¸ BLEU metric not available")
        bleu_scores = {"bleu": 0.0, "error": "BLEU metric not loaded"}
    
    # ì¶”ê°€ í‰ê°€ ë©”íŠ¸ë¦­ë“¤
    print("Computing additional metrics...")
    
    # 1. í‰ê·  ê¸¸ì´ ë¹„êµ (ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ì¤‘ìš”í•œ ë©”íŠ¸ë¦­)
    pred_lengths = [len(pred.split()) for pred in all_predictions]
    ref_lengths = [len(ref.split()) for ref in all_references]
    
    avg_pred_length = sum(pred_lengths) / len(pred_lengths)
    avg_ref_length = sum(ref_lengths) / len(ref_lengths)
    length_ratio = avg_pred_length / avg_ref_length if avg_ref_length > 0 else 0.0
    
    # ê¸¸ì´ ë¶„í¬ ë¶„ì„
    import statistics
    pred_length_stats = {
        "mean": avg_pred_length,
        "median": statistics.median(pred_lengths),
        "std": statistics.stdev(pred_lengths) if len(pred_lengths) > 1 else 0,
        "min": min(pred_lengths),
        "max": max(pred_lengths)
    }
    
    ref_length_stats = {
        "mean": avg_ref_length,
        "median": statistics.median(ref_lengths),
        "std": statistics.stdev(ref_lengths) if len(ref_lengths) > 1 else 0,
        "min": min(ref_lengths),
        "max": max(ref_lengths)
    }
    
    # 2. Exact Match ë¹„ìœ¨ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ê·œí™”)
    exact_matches = 0
    normalized_exact_matches = 0
    
    for pred, ref in zip(all_predictions, all_references):
        # ì™„ì „ ì¼ì¹˜
        if pred.strip() == ref.strip():
            exact_matches += 1
        
        # ì •ê·œí™”ëœ ì¼ì¹˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ê·œí™”)
        pred_normalized = re.sub(r'\s+', ' ', pred.strip().lower())
        ref_normalized = re.sub(r'\s+', ' ', ref.strip().lower())
        if pred_normalized == ref_normalized:
            normalized_exact_matches += 1
    
    exact_match_ratio = exact_matches / len(all_predictions)
    normalized_exact_match_ratio = normalized_exact_matches / len(all_predictions)
    
    # 3. ë¹ˆ ìƒì„± ë¹„ìœ¨ ì²´í¬
    empty_predictions = sum(1 for pred in all_predictions if len(pred.strip()) == 0)
    empty_prediction_ratio = empty_predictions / len(all_predictions)
    
    # 4. ì—ëŸ¬ ìƒì„± ë¹„ìœ¨ ì²´í¬
    error_predictions = sum(1 for pred in all_predictions if pred.strip() in ["[EMPTY_GENERATION]", "[EXTRACTION_ERROR]"])
    error_prediction_ratio = error_predictions / len(all_predictions)
    
    print(f"âœ… Additional metrics computed:")
    print(f"  - Empty predictions: {empty_predictions}/{len(all_predictions)} ({empty_prediction_ratio:.4f})")
    print(f"  - Error predictions: {error_predictions}/{len(all_predictions)} ({error_prediction_ratio:.4f})")
    print(f"  - Exact matches: {exact_matches}/{len(all_predictions)} ({exact_match_ratio:.4f})")
    print(f"  - Normalized matches: {normalized_exact_matches}/{len(all_predictions)} ({normalized_exact_match_ratio:.4f})")
    
    # ê²°ê³¼ ì •ë¦¬
    summary_metrics = {
        "rouge": rouge_scores,
        "bertscore": bert_scores_result,
        "bleu": bleu_scores,
        "length_analysis": {
            "prediction_stats": pred_length_stats,
            "reference_stats": ref_length_stats,
            "length_ratio": length_ratio
        },
        "exact_match_analysis": {
            "exact_match_ratio": exact_match_ratio,
            "normalized_exact_match_ratio": normalized_exact_match_ratio,
            "exact_matches": exact_matches,
            "normalized_exact_matches": normalized_exact_matches
        },
        "quality_analysis": {
            "empty_prediction_ratio": empty_prediction_ratio,
            "error_prediction_ratio": error_prediction_ratio,
            "empty_predictions": empty_predictions,
            "error_predictions": error_predictions
        },
        "total_samples": len(all_predictions)
    }
    
    sample_results = [
        {
            "prompt": p, 
            "ground_truth": r, 
            "model_prediction": pred,
            "prediction_length": len(pred.split()),
            "reference_length": len(r.split())
        } 
        for p, r, pred in zip(all_prompts, all_references, all_predictions)
    ]

    final_results = {
        "summary_metrics": summary_metrics,
        "sample_results": sample_results
    }
    
    print("\n--- Evaluation Metrics Summary ---")
    print(f"ğŸ“Š Total Samples Evaluated: {len(all_predictions)}")
    
    print(f"\nğŸ” ROUGE Scores:")
    if 'error' in rouge_scores:
        print(f"  - âŒ ROUGE calculation failed: {rouge_scores['error']}")
    else:
        for metric, score in rouge_scores.items():
            print(f"  - {metric.upper()}: {score:.4f}")
    
    print(f"\nğŸ¯ BERTScore:")
    if bert_scores_result:
        print(f"  - Language Detected: {bert_scores_result.get('language_detected', 'unknown')}")
        print(f"  - Language Confidence: {bert_scores_result.get('language_confidence', 'unknown')}")
        print(f"  - Model Used: {bert_scores_result.get('model_used', 'unknown')}")
        print(f"  - F1 Average: {bert_scores_result.get('f1_avg', 0):.4f}")
        print(f"  - Precision Average: {bert_scores_result.get('precision_avg', 0):.4f}")
        print(f"  - Recall Average: {bert_scores_result.get('recall_avg', 0):.4f}")
        if 'valid_samples' in bert_scores_result:
            print(f"  - Valid Samples: {bert_scores_result['valid_samples']}/{bert_scores_result.get('total_samples', 'unknown')}")
        if 'fallback_used' in bert_scores_result:
            print(f"  - âš ï¸ Fallback model used")
        if 'error' in bert_scores_result:
            print(f"  - âŒ Error: {bert_scores_result['error']}")
    else:
        print("  - âŒ BERTScore calculation completely failed")
    
    if bleu_scores and 'bleu' in bleu_scores:
        print(f"\nğŸ“ BLEU Score: {bleu_scores['bleu']:.4f}")
        if 'error' in bleu_scores:
            print(f"  - âš ï¸ Error: {bleu_scores['error']}")
    else:
        print(f"\nğŸ“ BLEU Score: Failed to compute")
    
    print(f"\nğŸ“ Length Analysis:")
    print(f"  - Average Prediction Length: {pred_length_stats['mean']:.1f} words")
    print(f"  - Average Reference Length: {ref_length_stats['mean']:.1f} words")
    print(f"  - Length Ratio (pred/ref): {length_ratio:.2f}")
    print(f"  - Prediction Length Range: {pred_length_stats['min']}-{pred_length_stats['max']} words")
    print(f"  - Reference Length Range: {ref_length_stats['min']}-{ref_length_stats['max']} words")
    
    print(f"\nâœ… Match Analysis:")
    print(f"  - Exact Match Ratio: {exact_match_ratio:.4f} ({exact_matches}/{len(all_predictions)})")
    print(f"  - Normalized Match Ratio: {normalized_exact_match_ratio:.4f} ({normalized_exact_matches}/{len(all_predictions)})")
    
    print(f"\nğŸ” Quality Analysis:")
    print(f"  - Empty Predictions: {empty_prediction_ratio:.4f} ({empty_predictions}/{len(all_predictions)})")
    print(f"  - Error Predictions: {error_prediction_ratio:.4f} ({error_predictions}/{len(all_predictions)})")
    
    if empty_prediction_ratio > 0.1:
        print(f"  - âš ï¸ High empty prediction ratio detected!")
    if error_prediction_ratio > 0.05:
        print(f"  - âš ï¸ High error prediction ratio detected!")
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì • - íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì ì ˆí•œ ê²½ë¡œ ì‚¬ìš©
    if settings.is_pipeline_env:
        # íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” evaluation ê²°ê³¼ë¥¼ vfrootì— ì €ì¥ (ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
        output_path = settings.evaluation_output_path / args.output_path
    else:
        output_path = settings.evaluation_output_path / args.output_path

    if not output_path.parent.exists():
        output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=4)
        
    print(f"\nEvaluation complete. Results saved to {output_path}")
    
    print("\nClearing GPU memory...")
    del model
    del pipe
    torch.cuda.empty_cache()
    print("GPU memory cleared.")

if __name__ == "__main__":
    main()