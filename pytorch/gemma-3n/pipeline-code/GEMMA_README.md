# Gemma-3n Financial Q&A Fine-tuning Pipeline

μ΄ ν”„λ΅μ νΈλ” Gemma-3n λ¨λΈμ„ financial Q&A λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•κ³  ν‰κ°€ν•λ” μ™„μ „ν• νμ΄ν”„λΌμΈμ„ μ κ³µν•©λ‹λ‹¤.

## π€ Quick Start

### ν™κ²½ μ„¤μ •

1. μμ΅΄μ„± μ„¤μΉ:
```bash
pip install -r requirements.txt
```

2. ν™κ²½ λ³€μ μ„¤μ •:
`.env` νμΌμ—μ„ λ‹¤μ κ°’λ“¤μ„ ν™•μΈν•μ„Έμ”:
- `HF_TOKEN`: Hugging Face ν† ν°
- `MODEL_ID`: κΈ°λ³Έκ°’μ€ `google/gemma-3n-e2b-it`
- `DATASET`: κΈ°λ³Έκ°’μ€ `TheFinAI/Fino1_Reasoning_Path_FinQA`
- `WANDB_API_KEY`: Weights & Biases API ν‚¤ (μ„ νƒμ‚¬ν•­)

### νμ΄ν”„λΌμΈ μ‹¤ν–‰

#### λ°©λ²• 1: μ „μ²΄ νμ΄ν”„λΌμΈ ν• λ²μ— μ‹¤ν–‰
```bash
python scripts/cli.py pipeline \
    --training_args_path configs/training_args.yaml \
    --peft_config_path configs/peft_config.yaml
```

#### λ°©λ²• 2: κ°λ³„ νƒμ¤ν¬ μ‹¤ν–‰
```bash
# 1. λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“ λ° μ „μ²λ¦¬
python scripts/cli.py dataset

# 2. λ² μ΄μ¤ λ¨λΈ ν‰κ°€
python scripts/cli.py eval-base

# 3. λ¨λΈ νμΈνλ‹
python scripts/cli.py train \
    --training_args_path training_args.yaml \
    --peft_config_path peft_config.yaml

# 4. νμΈνλ‹λ λ¨λΈ ν‰κ°€
python scripts/cli.py eval-finetuned
```

## π“ νμΌ κµ¬μ΅° λ° λ°μ΄ν„° κ²½λ΅

### κΈ°λ³Έ κ²½λ΅ μ„¤μ •
- **λ°μ΄ν„°μ…‹ μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/dataset/`
- **PEFT μ–΄λ‘ν„° μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/model/`
- **λ³‘ν•©λ λ¨λΈ μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/merged_model/`
- **ν‰κ°€ κ²°κ³Ό μ €μ¥**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/evaluation/`

### νμΌ κµ¬μ΅°
```
gemma-3n/
β”β”€β”€ README.md
β”β”€β”€ scripts/
β”‚   β””β”€β”€ cli.py              # νμ΄ν”„λΌμΈ μ‹¤ν–‰ CLI
β””β”€β”€ pipeline-code
    β”β”€β”€ configs/
    β”‚   β”β”€β”€ settings.py          # μ „μ—­ μ„¤μ •
    β”‚   β”β”€β”€ training_args.yaml   # ν›λ ¨ μΈμ μ„¤μ •
    β”‚   β””β”€β”€ peft_config.yaml     # PEFT(LoRA) μ„¤μ •
    β”β”€β”€ data/                    # μ²λ¦¬λ λ°μ΄ν„°μ…‹ μ €μ¥μ†
    β”β”€β”€ results/
    β”‚   β”β”€β”€ model/              # PEFT μ–΄λ‘ν„° μ €μ¥μ†
    β”‚   β”β”€β”€ merged_model/       # λ³‘ν•©λ λ¨λΈ μ €μ¥μ† (λ°°ν¬μ©)
    β”‚   β””β”€β”€ evaluation/         # ν‰κ°€ κ²°κ³Ό μ €μ¥μ†
    β”β”€β”€ logs/                   # ν›λ ¨ λ΅κ·Έ μ €μ¥μ†
    β”β”€β”€ src/
    β”‚   β”β”€β”€ data/
    β”‚   β”‚   β””β”€β”€ dataset.py      # λ°μ΄ν„°μ…‹ λ΅λ”© λ° μ „μ²λ¦¬
    β”‚   β”β”€β”€ evaluation/
    β”‚   β”‚   β””β”€β”€ evaluation.py   # λ¨λΈ ν‰κ°€
    β”‚   β”β”€β”€ models/
    β”‚   β”‚   β””β”€β”€ model.py        # λ¨λΈ λ΅λ”©
    β”‚   β””β”€β”€ training/
    β”‚       β””β”€β”€ trainer.py      # λ¨λΈ νμΈνλ‹
    β”β”€β”€ .env                    # ν™κ²½ λ³€μ
    β””β”€β”€ requirements.txt        # μμ΅΄μ„±

```

## π”§ νƒμ¤ν¬ μƒμ„Έ μ„¤λ…

### Task 1: Dataset Download & Preprocessing
- **μ…λ ¥**: Hugging Face λ°μ΄ν„°μ…‹ (`TheFinAI/Fino1_Reasoning_Path_FinQA`)
- **μ²λ¦¬**: λ°μ΄ν„°μ…‹μ„ train/validation/testλ΅ λ¶„ν• ν•κ³  μ±„ν… ν…ν”λ¦Ώ μ μ©
- **μ¶λ ¥**: `dataset/` ν΄λ”μ— μ „μ²λ¦¬λ λ°μ΄ν„°μ…‹ μ €μ¥

### Task 2: Base Model Evaluation
- **μ…λ ¥**: μ›λ³Έ Gemma-3n λ¨λΈ, μ „μ²λ¦¬λ ν…μ¤νΈ λ°μ΄ν„°
- **μ²λ¦¬**: ROUGE, BERTScore λ“±μ μ§€ν‘λ΅ λ² μ΄μ¤ λ¨λΈ μ„±λ¥ ν‰κ°€
- **μ¶λ ¥**: `base_model_evaluation.json`

### Task 3: Model Fine-tuning
- **μ…λ ¥**: λ² μ΄μ¤ λ¨λΈ, μ „μ²λ¦¬λ ν›λ ¨/κ²€μ¦ λ°μ΄ν„°, μ„¤μ • νμΌλ“¤
- **μ²λ¦¬**: LoRAλ¥Ό μ‚¬μ©ν• νλΌλ―Έν„° ν¨μ¨μ  νμΈνλ‹
- **μ¶λ ¥**: 
  - PEFT μ–΄λ‘ν„°: `results/model/` ν΄λ”
  - λ³‘ν•©λ μ™„μ „ν• λ¨λΈ: `results/merged_model/` ν΄λ” (λ°°ν¬μ©)

### Task 4: Fine-tuned Model Evaluation
- **μ…λ ¥**: νμΈνλ‹λ λ¨λΈ, μ „μ²λ¦¬λ ν…μ¤νΈ λ°μ΄ν„°
- **μ²λ¦¬**: νμΈνλ‹λ λ¨λΈμ μ„±λ¥ ν‰κ°€
- **μ¶λ ¥**: `finetuned_model_evaluation.json`

## π“ μ„¤μ • μ»¤μ¤ν„°λ§μ΄μ§•

### ν›λ ¨ μΈμ μμ • (`configs/training_args.yaml`)
- λ°°μΉ ν¬κΈ°, ν•™μµλ¥ , μ—ν¬ν¬ μ λ“± μ΅°μ • κ°€λ¥
- Weights & Biases λ΅κΉ… μ„¤μ •

### PEFT μ„¤μ • μμ • (`configs/peft_config.yaml`)
- LoRA rank, alpha, dropout λ“± μ΅°μ • κ°€λ¥
- νƒ€κ² λ¨λ“ μ„ νƒ

## π¨ μ£Όμμ‚¬ν•­

1. **GPU λ©”λ¨λ¦¬**: Gemma-3nμ€ λ€μ©λ‰ λ¨λΈμ΄λ―€λ΅ μ¶©λ¶„ν• GPU λ©”λ¨λ¦¬κ°€ ν•„μ”ν•©λ‹λ‹¤.
2. **Hugging Face ν† ν°**: λ¨λΈ μ ‘κ·Όμ„ μ„ν•΄ μ ν¨ν• HF ν† ν°μ΄ ν•„μ”ν•©λ‹λ‹¤.
3. **λ°μ΄ν„° μ €μ¥ κ³µκ°„**: λ°μ΄ν„°μ…‹κ³Ό λ¨λΈ μ €μ¥μ„ μ„ν• μ¶©λ¶„ν• λ””μ¤ν¬ κ³µκ°„μ„ ν™•λ³΄ν•μ„Έμ”.

## π“ μ„±λ¥ λ¨λ‹ν„°λ§

- Weights & Biasesλ¥Ό ν†µν• μ‹¤μ‹κ°„ ν›λ ¨ λ¨λ‹ν„°λ§
- ν‰κ°€ κ²°κ³Όλ” JSON νμΌλ΅ μ €μ¥λμ–΄ μ„±λ¥ λΉ„κµ κ°€λ¥

## π€ λ°°ν¬ λ° μ‚¬μ©

### νμΈνλ‹λ λ¨λΈ μ‚¬μ© λ°©λ²•

νμΈνλ‹ μ™„λ£ ν›„, λ³‘ν•©λ λ¨λΈμ„ λ‹¤μκ³Ό κ°™μ΄ μ‚¬μ©ν•  μ μμµλ‹λ‹¤:

```python
from transformers import AutoModelForCausalLM, AutoProcessor

# 1. λ΅μ»¬μ—μ„ λ³‘ν•©λ λ¨λΈ λ΅λ“
model = AutoModelForCausalLM.from_pretrained("./results/merged_model/")
processor = AutoProcessor.from_pretrained("./results/merged_model/")

# 2. μ¶”λ΅  μ‹¤ν–‰
messages = [{"role": "user", "content": "What is the impact of inflation on stock prices?"}]
prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
response = processor.decode(outputs[0], skip_special_tokens=True)
```

### Hugging Face Hubμ— λ°°ν¬

```bash
# Hugging Face CLI μ„¤μΉ λ° λ΅κ·ΈμΈ
pip install huggingface_hub
huggingface-cli login

# λ¨λΈ μ—…λ΅λ“
huggingface-cli upload your-username/gemma-3n-financial-qa ./results/merged_model/
```

### λ¨λΈ κµ¬μ΅°

- **PEFT μ–΄λ‘ν„°**: `results/model/` - LoRA κ°€μ¤‘μΉλ§ ν¬ν•¨, μ‘μ€ νμΌ ν¬κΈ°
- **λ³‘ν•©λ λ¨λΈ**: `results/merged_model/` - μ™„μ „ν• λ¨λΈ, λ…λ¦½μ μΌλ΅ μ‚¬μ© κ°€λ¥
