# # PEFT (LoRA) Configuration for language model Fine-tuning
task_type: "CAUSAL_LM"
r: 16
target_modules: ["q_proj", "v_proj"]
lora_alpha: 16
lora_dropout: 0.05
bias: "none"
use_rslora: false
use_dora: false
