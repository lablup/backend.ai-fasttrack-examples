# # PEFT (LoRA) Configuration for Gemma-3n Fine-tuning
# task_type: "CAUSAL_LM"
# r: 64
# target_modules: "all-linear"
# lora_alpha: 16
# lora_dropout: 0.05
# bias: "none"
# use_rslora: false
# use_dora: false

task_type: "CAUSAL_LM"
r: 64  # 64 -> 16으로 줄임
target_modules: "all-linear"
lora_alpha: 16
lora_dropout: 0.05
bias: "none"
use_rslora: false
use_dora: false