# VLM Model Configuration
# ë‹¤ì–‘í•œ VLM ëª¨ë¸ì— ëŒ€í•œ í´ë˜ìŠ¤ ë§¤í•‘ ì„¤ì •
#
# ğŸ“ Processor ì„¤ì • ë°©ì‹:
# - processor_classê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ AutoProcessor ì‚¬ìš© (ê¶Œì¥)
# - íŠ¹ìˆ˜í•œ processorê°€ í•„ìš”í•œ ëª¨ë¸ë§Œ processor_class ëª…ì‹œì  ì§€ì •
# - ëŒ€ë¶€ë¶„ì˜ VLM ëª¨ë¸ì€ AutoProcessorë¡œ ì¶©ë¶„íˆ í˜¸í™˜ë©ë‹ˆë‹¤

# ëª¨ë¸ë³„ í´ë˜ìŠ¤ ë§¤í•‘
model_classes:
    # Qwen2-VL ëª¨ë¸ë“¤ (íŠ¹ìˆ˜ processor í•„ìš”)
    "Qwen/Qwen2-VL-2B-Instruct":
        model_class: "Qwen2VLForConditionalGeneration"
        processor_class: "Qwen2VLProcessor" # íŠ¹ìˆ˜ processor ëª…ì‹œì  ì§€ì •
        import_path: "transformers"

    "Qwen/Qwen2-VL-7B-Instruct":
        model_class: "Qwen2VLForConditionalGeneration"
        processor_class: "Qwen2VLProcessor" # íŠ¹ìˆ˜ processor ëª…ì‹œì  ì§€ì •
        import_path: "transformers"

    # LLaVA ëª¨ë¸ë“¤ (AutoProcessor ì‚¬ìš©)
    "llava-hf/llava-1.5-7b-hf":
        model_class: "LlavaForConditionalGeneration"
        import_path: "transformers"
        # processor_class ë¯¸ì§€ì • -> AutoProcessor ì‚¬ìš©

    "llava-hf/llava-1.5-13b-hf":
        model_class: "LlavaForConditionalGeneration"
        import_path: "transformers"
        # processor_class ë¯¸ì§€ì • -> AutoProcessor ì‚¬ìš©

    # InternVL ëª¨ë¸ë“¤ (AutoProcessor ì‚¬ìš©)
    "OpenGVLab/InternVL2-2B":
        model_class: "InternVLChatModel"
        import_path: "transformers"
        # processor_class ë¯¸ì§€ì • -> AutoProcessor ì‚¬ìš©

    # PaliGemma ëª¨ë¸ë“¤ (íŠ¹ìˆ˜ processor í•„ìš”)
    "google/paligemma-3b-pt-448":
        model_class: "PaliGemmaForConditionalGeneration"
        processor_class: "PaliGemmaProcessor" # íŠ¹ìˆ˜ processor ëª…ì‹œì  ì§€ì •
        import_path: "transformers"

    # Phi-3-Vision ëª¨ë¸ë“¤ (AutoProcessor ì‚¬ìš©)
    "microsoft/Phi-3-vision-128k-instruct":
        model_class: "Phi3VForCausalLM"
        import_path: "transformers"
        # processor_class ë¯¸ì§€ì • -> AutoProcessor ì‚¬ìš©

# ê¸°ë³¸ fallback ì„¤ì • (VLMìš© AutoModel ì‚¬ìš©)
default_fallback:
    model_class: "AutoModelForVision2Seq"
    import_path: "transformers"
    # processor_class ë¯¸ì§€ì • -> AutoProcessor ì‚¬ìš©

# ê³µí†µ ë¡œë”© íŒŒë¼ë¯¸í„°
loading_params:
    torch_dtype: "torch.bfloat16"
    device_map: "auto"
    trust_remote_code: true

# í”„ë¡œì„¸ì„œ ê³µí†µ ì„¤ì •
processor_params:
    trust_remote_code: true
    use_fast: true # fast processor ì§€ì› ëª¨ë¸ìš© (Qwen2-VL, SmolVLM, GLM-4V, LLaVA ë“±)
