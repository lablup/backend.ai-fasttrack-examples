# ğŸ“š VLM Configuration ê°€ì´ë“œ

ì´ í´ë”ì—ëŠ” Vision-Language Model (VLM) íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ ì„¤ì • íŒŒì¼ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ VLM ì„¤ì • íŒŒì¼ ê°œìš”

### í•µì‹¬ VLM ì„¤ì • íŒŒì¼ë“¤

-   **`vlm_model_config.yaml`**: VLM ëª¨ë¸ë³„ í´ë˜ìŠ¤ ë§¤í•‘ ë° ë¡œë”© ì„¤ì •
-   **`vlm_collator_config.yaml`**: VLM ë°ì´í„° ì²˜ë¦¬ ë° ì½œë ˆì´í„° ì„¤ì •
-   **`train_config.yaml`**: í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •
-   **`peft_config.yaml`**: LoRA ë“± PEFT ì„¤ì •

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1ï¸âƒ£ ëª¨ë¸ ì„¤ì • (`vlm_model_config.yaml`)

```yaml
# ì‚¬ìš©í•˜ê³  ì‹¶ì€ ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
model_classes:
    "Qwen/Qwen2-VL-2B-Instruct":
        model_class: "Qwen2VLForConditionalGeneration"
        processor_class: "Qwen2VLProcessor"
        import_path: "transformers"
```

### 2ï¸âƒ£ ë°ì´í„° ì²˜ë¦¬ ì„¤ì • (`vlm_collator_config.yaml`)

```yaml
# ë°ì´í„°ì…‹ ì»¬ëŸ¼ëª… ë§¤í•‘
dataset_columns:
    image_column: "image" # ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ ì»¬ëŸ¼ëª…
    question_column: "question" # ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì§ˆë¬¸ ì»¬ëŸ¼ëª…
    answer_column: "answer" # ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ë‹µë³€ ì»¬ëŸ¼ëª…

# ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì²˜ë¦¬ í™œì„±í™” ì„¤ì •
data_processing:
    image_data: true # ì´ë¯¸ì§€ ì²˜ë¦¬ í™œì„±í™”
    video_data: false # ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¹„í™œì„±í™” (ê¸°ë³¸ê°’)
```

---

## ğŸ”§ ìƒì„¸ ì„¤ì • ê°€ì´ë“œ

## ğŸ“± VLM ëª¨ë¸ ì„¤ì • (`vlm_model_config.yaml`)

### ì§€ì›ë˜ëŠ” VLM ëª¨ë¸ë“¤

í˜„ì¬ ì‚¬ì „ ì„¤ì •ëœ ëª¨ë¸ë“¤:

```yaml
model_classes:
    # Qwen2-VL ì‹œë¦¬ì¦ˆ
    "Qwen/Qwen2-VL-2B-Instruct":
        model_class: "Qwen2VLForConditionalGeneration"
        processor_class: "Qwen2VLProcessor"
        import_path: "transformers"

    # LLaVA ì‹œë¦¬ì¦ˆ
    "llava-hf/llava-1.5-7b-hf":
        model_class: "LlavaForConditionalGeneration"
        processor_class: "LlavaProcessor"
        import_path: "transformers"

    # InternVL ì‹œë¦¬ì¦ˆ
    "OpenGVLab/InternVL2-2B":
        model_class: "InternVLChatModel"
        processor_class: "InternVLChatProcessor"
        import_path: "transformers"

    # PaliGemma ì‹œë¦¬ì¦ˆ
    "google/paligemma-3b-pt-448":
        model_class: "PaliGemmaForConditionalGeneration"
        processor_class: "PaliGemmaProcessor"
        import_path: "transformers"

    # Phi-3-Vision ì‹œë¦¬ì¦ˆ
    "microsoft/Phi-3-vision-128k-instruct":
        model_class: "Phi3VForCausalLM"
        processor_class: "Phi3VProcessor"
        import_path: "transformers"
```

### ìƒˆë¡œìš´ VLM ëª¨ë¸ ì¶”ê°€í•˜ê¸°

**Step 1: ëª¨ë¸ í´ë˜ìŠ¤ ì •ë³´ í™•ì¸**

```python
# Hugging Faceì—ì„œ ëª¨ë¸ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ì—¬ í´ë˜ìŠ¤ëª… ì°¾ê¸°
from transformers import AutoModelForCausalLM, AutoProcessor
```

**Step 2: ì„¤ì • ì¶”ê°€**

```yaml
model_classes:
    "your-org/your-vlm-model":
        model_class: "YourVLMModelClass" # ëª¨ë¸ í´ë˜ìŠ¤ëª…
        processor_class: "YourVLMProcessorClass" # í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ëª…
        import_path: "transformers" # ì„í¬íŠ¸ ê²½ë¡œ
```

### ë¡œë”© íŒŒë¼ë¯¸í„° ì»¤ìŠ¤í„°ë§ˆì´ì§•

```yaml
# ê³µí†µ ë¡œë”© íŒŒë¼ë¯¸í„°
loading_params:
    torch_dtype: "torch.bfloat16" # ë°ì´í„° íƒ€ì…: torch.float16, torch.bfloat16
    device_map: "auto" # ë””ë°”ì´ìŠ¤ ë§¤í•‘: auto, cuda, cpu
    trust_remote_code: true # ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€

# í”„ë¡œì„¸ì„œ ê³µí†µ ì„¤ì •
processor_params:
    trust_remote_code: true # í”„ë¡œì„¸ì„œ ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€
```

---

## ğŸ–¼ï¸ VLM ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • (`vlm_collator_config.yaml`)

### ë°ì´í„°ì…‹ ì»¬ëŸ¼ ë§¤í•‘

**ì‹¤ì œ ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸ ë°©ë²•:**

```python
from datasets import load_dataset
dataset = load_dataset("your-dataset-name")
print("Available columns:", dataset["train"].column_names)
print("Sample data:", dataset["train"][0])
```

**ì»¬ëŸ¼ ë§¤í•‘ ì„¤ì •:**
**ì£¼ì˜ : dataset_columnsì˜ key ê°’ì¸ image_column, question_column, answer_column ë“±ì€ ì•„ë˜ message formatì—ì„œ ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ì˜ ì´ë¦„ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.**

```yaml
dataset_columns:
    image_column: "image" # ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ ì»¬ëŸ¼ëª…
    question_column: "question" # ë°ì´í„°ì…‹ì˜ ì§ˆë¬¸ ì»¬ëŸ¼ëª…
    answer_column: "answer" # ë°ì´í„°ì…‹ì˜ ë‹µë³€ ì»¬ëŸ¼ëª…
    video_column: "video" # ë°ì´í„°ì…‹ì˜ ë¹„ë””ì˜¤ ì»¬ëŸ¼ëª… (ì„ íƒì‚¬í•­)
    context_column: null # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì»¬ëŸ¼ (ì„ íƒì‚¬í•­)
```

### ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì²˜ë¦¬ ì œì–´

**ğŸ†• ìƒˆë¡œìš´ í”Œë˜ê·¸ ê¸°ë°˜ ì²˜ë¦¬ ì œì–´:**

```yaml
data_processing:
    image_data: true # ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬ í™œì„±í™” (ê¸°ë³¸ê°’: true)
    video_data: false # ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ í™œì„±í™” (ê¸°ë³¸ê°’: false)
```

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:**

1. **ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬** (ëŒ€ë¶€ë¶„ì˜ VLM ëª¨ë¸):

```yaml
data_processing:
    image_data: true
    video_data: false
```

2. **ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬** (ë¹„ë””ì˜¤ ì§€ì› ëª¨ë¸):

```yaml
data_processing:
    image_data: false
    video_data: true
```

3. **ë‘˜ ë‹¤ í™œì„±í™”** (ë¹„ë””ì˜¤ ìš°ì„  ì²˜ë¦¬):

```yaml
data_processing:
    image_data: true
    video_data: true
# âš ï¸ ì£¼ì˜: ëŒ€ë¶€ë¶„ì˜ í”„ë¡œì„¸ì„œëŠ” í•˜ë‚˜ì˜ íƒ€ì…ë§Œ ì§€ì›í•˜ë¯€ë¡œ ë¹„ë””ì˜¤ê°€ ìš°ì„  ì²˜ë¦¬ë©ë‹ˆë‹¤
```

### ë©”ì‹œì§€ í¬ë§· ì„¤ì •

**ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ì˜ ì´ë¦„ì€ dataset_columnsì—ì„œ ì •ì˜í•œ key ê°’ê³¼ ë§¤í•‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.**

```yaml
message_format:
    system_prompt: "Answer briefly." # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

    # í•™ìŠµìš© ë©”ì‹œì§€ êµ¬ì¡°
    training_messages:
        - role: "user"
          content:
              - type: "text"
                text: "{system_prompt}"
              - type: "image" # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë”
              - type: "text"
                text: "{question_column}"
        - role: "assistant"
          content:
              - type: "text"
                text: "{answer_column}"
```

### ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •

```yaml
image_processing:
    convert_to_rgb: true # RGB ë³€í™˜ (ê¶Œì¥: true)
    resize_mode: "keep_aspect" # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ: keep_aspect, crop, stretch
    max_size: null # ìµœëŒ€ í¬ê¸° ì œí•œ (í”½ì…€, null=ì œí•œì—†ìŒ)
```

### ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •

```yaml
video_processing:
    enabled: true # ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ëŠ¥ í™œì„±í™”

    # í”„ë ˆì„ ì¶”ì¶œ ì„¤ì •
    frame_extraction:
        library: "decord" # ë¹„ë””ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬: decord (ê¶Œì¥), cv2
        sampling_strategy: "uniform" # ìƒ˜í”Œë§ ì „ëµ: uniform (ê· ë“± ìƒ˜í”Œë§)
        num_frames: 8 # ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜

    # ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    file_processing:
        convert_to_rgb: true # RGB ë³€í™˜
        max_duration: null # ìµœëŒ€ ë¹„ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
```

### í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„¤ì •

```yaml
text_processing:
    padding: true # íŒ¨ë”© ì ìš©
    truncation: true # ìë¥´ê¸° ì ìš©
    max_length: 2048 # ìµœëŒ€ í† í° ê¸¸ì´
    add_generation_prompt: false # generation prompt ì¶”ê°€ (í•™ìŠµì‹œ false)
```

### ë ˆì´ë¸” ë§ˆìŠ¤í‚¹ ì„¤ì •

```yaml
label_masking:
    mask_pad_token: true # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹
    mask_image_token: true # ì´ë¯¸ì§€ í† í° ë§ˆìŠ¤í‚¹
    mask_input_tokens: false # ì…ë ¥ í† í° ë§ˆìŠ¤í‚¹ (instruction tuningì‹œ false)
    ignore_index: -100 # ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ ë¼ë²¨ ê°’
```

---

## âš™ï¸ í›ˆë ¨ ì„¤ì • (`train_config.yaml`)

### ê¸°ë³¸ í›ˆë ¨ íŒŒë¼ë¯¸í„°

```yaml
# ë°°ì¹˜ í¬ê¸° ì„¤ì •
per_device_train_batch_size: 8 # í›ˆë ¨ ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
per_device_eval_batch_size: 8 # í‰ê°€ ë°°ì¹˜ í¬ê¸°
gradient_accumulation_steps: 2 # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…

# í•™ìŠµë¥  ë° ì—í¬í¬
learning_rate: 1.0e-05 # í•™ìŠµë¥  (VLMì€ ë³´í†µ ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©)
num_train_epochs: 1.0 # í›ˆë ¨ ì—í¬í¬ ìˆ˜

# ë¡œê¹… ë° ì €ì¥
logging_steps: 0.1 # ë¡œê¹… ì£¼ê¸° (0.1 = 10%ë§ˆë‹¤)
eval_steps: 0.1 # í‰ê°€ ì£¼ê¸°
save_steps: 0.1 # ëª¨ë¸ ì €ì¥ ì£¼ê¸°
save_total_limit: 2 # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜

# ë©”ëª¨ë¦¬ ìµœì í™”
gradient_checkpointing: true # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ì ˆì•½)
bf16: true # bfloat16 ì‚¬ìš© (ê¶Œì¥)
fp16: false # float16 ì‚¬ìš© (bf16ê³¼ ë°°íƒ€ì )

# ë°ì´í„° ì²˜ë¦¬
group_by_length: true # ê¸¸ì´ë³„ ê·¸ë£¹í™” (íš¨ìœ¨ì„± í–¥ìƒ)
remove_unused_columns: false # VLMì—ì„œëŠ” falseë¡œ ì„¤ì •
dataloader_pin_memory: false # ë©”ëª¨ë¦¬ í•€ë‹
```

### WandB ì„¤ì •

```yaml
# Weights & Biases ë¡œê¹…
report_to: ["wandb"] # ë¡œê¹… ì„œë¹„ìŠ¤
run_name: "qwen2-vl-2b-vqa-finetuning" # ì‹¤í—˜ ì´ë¦„
```

---

## ğŸ¯ PEFT ì„¤ì • (`peft_config.yaml`)

### LoRA ì„¤ì •

```yaml
# PEFT (LoRA) Configuration
task_type: "CAUSAL_LM" # íƒœìŠ¤í¬ íƒ€ì…
r: 64 # LoRA rank (ë†’ì„ìˆ˜ë¡ ë” ë§ì€ íŒŒë¼ë¯¸í„°)
target_modules: "all-linear" # íƒ€ê²Ÿ ëª¨ë“ˆ (all-linear ê¶Œì¥)
lora_alpha: 16 # LoRA alpha (ì¼ë°˜ì ìœ¼ë¡œ rì˜ 1/4)
lora_dropout: 0.05 # LoRA ë“œë¡­ì•„ì›ƒ
bias: "none" # ë°”ì´ì–´ìŠ¤ ì„¤ì •
use_rslora: false # RS-LoRA ì‚¬ìš© ì—¬ë¶€
use_dora: false # DoRA ì‚¬ìš© ì—¬ë¶€
```

---

## ğŸ› ï¸ ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

### ì‹œë‚˜ë¦¬ì˜¤ 1: ìƒˆë¡œìš´ VQA ë°ì´í„°ì…‹ ì‚¬ìš©í•˜ê¸°

**Step 1: ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸**

```python
from datasets import load_dataset
dataset = load_dataset("your-vqa-dataset")
print(dataset["train"].column_names)
# ì¶œë ¥ ì˜ˆ: ['image_path', 'query', 'response', 'metadata']
```

**Step 2: `vlm_collator_config.yaml` ìˆ˜ì •**

```yaml
dataset_columns:
    image_column: "image_path" # ì‹¤ì œ ì´ë¯¸ì§€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
    question_column: "query" # ì‹¤ì œ ì§ˆë¬¸ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
    answer_column: "response" # ì‹¤ì œ ë‹µë³€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë¹„ë””ì˜¤ VLM ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

**Step 1: `vlm_collator_config.yaml`ì—ì„œ ë¹„ë””ì˜¤ í™œì„±í™”**

```yaml
data_processing:
    image_data: false # ì´ë¯¸ì§€ ë¹„í™œì„±í™”
    video_data: true # ë¹„ë””ì˜¤ í™œì„±í™”

dataset_columns:
    video_column: "video_path" # ë¹„ë””ì˜¤ ì»¬ëŸ¼ëª… ì„¤ì •
```

**Step 2: ë¹„ë””ì˜¤ ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ì¡°ì •**

```yaml
video_processing:
    frame_extraction:
        num_frames: 16 # ë” ë§ì€ í”„ë ˆì„ ì¶”ì¶œ
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìµœì í™”

**Step 1: `train_config.yaml` ìˆ˜ì •**

```yaml
per_device_train_batch_size: 4 # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
gradient_accumulation_steps: 4 # ëˆ„ì  ìŠ¤í… ëŠ˜ë¦¬ê¸°
gradient_checkpointing: true # ì²´í¬í¬ì¸íŒ… í™œì„±í™”
```

**Step 2: `peft_config.yaml`ì—ì„œ LoRA rank ì¤„ì´ê¸°**

```yaml
r: 32 # rank ì¤„ì´ê¸° (64 â†’ 32)
lora_alpha: 8 # alphaë„ ë¹„ë¡€í•´ì„œ ì¤„ì´ê¸°
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­ ë° íŒ

### âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë™ì‹œ í™œì„±í™”**: ëŒ€ë¶€ë¶„ì˜ VLM processorëŠ” í•œ ê°€ì§€ íƒ€ì…ë§Œ ì§€ì›
2. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: VLMì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì‹¬
3. **ì»¬ëŸ¼ëª… í™•ì¸**: ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª…ê³¼ ì„¤ì •ì´ ì¼ì¹˜í•´ì•¼ í•¨
4. **ëª¨ë¸ í˜¸í™˜ì„±**: ëª¨ë“  VLMì´ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ

### ğŸ’¡ ìµœì í™” íŒ

1. **ì„±ëŠ¥ ìµœì í™”**: `bf16=true`, `gradient_checkpointing=true` ì‚¬ìš©
2. **ì‹¤í—˜ ê´€ë¦¬**: WandBë¡œ ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ
3. **ì ì§„ì  í…ŒìŠ¤íŠ¸**: ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
4. **ì—ëŸ¬ ë””ë²„ê¹…**: ê° ë‹¨ê³„ë³„ë¡œ ê°œë³„ ì‹¤í–‰í•´ì„œ ë¬¸ì œ íŒŒì•…

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

**Q: ìƒˆë¡œìš´ ëª¨ë¸ì´ ì¸ì‹ë˜ì§€ ì•Šì•„ìš”**

```yaml
# vlm_model_config.yamlì— ëª¨ë¸ ì¶”ê°€ í•„ìš”
model_classes:
    "your-model-id":
        model_class: "YourModelClass"
        processor_class: "YourProcessorClass"
        import_path: "transformers"
```

**Q: ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ìš”**

```yaml
# ì»¬ëŸ¼ëª… í™•ì¸ ë° ìˆ˜ì •
dataset_columns:
    image_column: "ì‹¤ì œ_ì´ë¯¸ì§€_ì»¬ëŸ¼ëª…" # ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •
```

**Q: ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ê°€ ë°œìƒí•´ìš”**

```yaml
# ë°°ì¹˜ í¬ê¸° ë° LoRA rank ì¤„ì´ê¸°
per_device_train_batch_size: 2 # ë” ì‘ê²Œ
gradient_accumulation_steps: 8 # ë” í¬ê²Œ
r: 16 # LoRA rank ì¤„ì´ê¸°
```

ì´ ê°€ì´ë“œë¥¼ í†µí•´ VLM íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì„¤ì •í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
