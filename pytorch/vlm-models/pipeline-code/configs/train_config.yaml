# Training Arguments Configuration for vlm Fine-tuning
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 2
gradient_checkpointing: true
learning_rate: 1.0e-05
num_train_epochs: 1.0
logging_steps: 0.1
eval_steps: 0.1
save_steps: 0.1
logging_strategy: "steps"
eval_strategy: "steps"
save_total_limit: 2
dataset_text_field: "text"
load_best_model_at_end: true
metric_for_best_model: "eval_loss" # 또는 사용하는 평가 메트릭
greater_is_better: false # loss의 경우 false, accuracy 등은 true
report_to: ["wandb"]
run_name: "gemma-2-2b-it-news-sumarization"
fp16: false
bf16: true
group_by_length: true
remove_unused_columns: false
dataloader_pin_memory: false
