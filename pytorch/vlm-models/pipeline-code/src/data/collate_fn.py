#!/usr/bin/env python3
"""
VLM Data Collator
VLM ëª¨ë¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì½œë ˆì´í„°
ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ë©°, ì„¤ì • íŒŒì¼ì„ í†µí•´ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
"""

import os
import re
import yaml
import torch
from pathlib import Path
from PIL import Image
from typing import List, Dict, Any, Optional, Union

# ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ íƒì  import
try:
    import decord
    DECORD_AVAILABLE = True
except ImportError:
    DECORD_AVAILABLE = False
    decord = None

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

def load_collator_config(config_path: str) -> dict:
    """ì½œë ˆì´í„° ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    from configs.settings import settings
    
    config_file = settings.config_path / config_path
    print(f"Loading VLM collator config from: {config_file}")
    
    if not config_file.exists():
        raise FileNotFoundError(f"Collator config file not found: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config

class VLMDataCollator:
    """
    VLM ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì½œë ˆì´í„°
    ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ë©°, ì„¤ì • íŒŒì¼ì„ í†µí•´ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
    """
    
    def __init__(self, processor: Any, config: dict):
        """
        VLMDataCollator ì´ˆê¸°í™”
        
        Args:
            processor: VLM processor (tokenizer + image processor)
            config: ì½œë ˆì´í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.processor = processor
        self.config = config
        
        # ì„¤ì •ê°’ë“¤ì„ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì €ì¥ (íŠ¹ìˆ˜ í† í° ì„¤ì • ì „ì— ë¨¼ì € ì´ˆê¸°í™”)
        self.dataset_columns = self.config.get('dataset_columns', {})
        self.message_format = self.config.get('message_format', {})
        self.data_processing = self.config.get('data_processing', {})  # ìƒˆë¡œ ì¶”ê°€
        self.image_processing = self.config.get('image_processing', {})
        self.text_processing = self.config.get('text_processing', {})
        self.label_masking = self.config.get('label_masking', {})
        self.batch_processing = self.config.get('batch_processing', {})
        self.special_tokens_config = self.config.get('special_tokens', {})
        self.video_processing = self.config.get('video_processing', {})  # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€
        
        # íŠ¹ìˆ˜ í† í° ID ë¯¸ë¦¬ ê³„ì‚° (ì„¤ì • ì´ˆê¸°í™” í›„)
        self._setup_special_tokens()
    
    def _setup_special_tokens(self):
        """í† í¬ë‚˜ì´ì €ì˜ ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
        print("ğŸ” Auto-detecting special tokens from tokenizer...")
        self.special_token_ids = {}
        self.ignore_in_loss_ids = set()  # ì†ì‹¤ ê³„ì‚° ì‹œ ë¬´ì‹œí•  í† í° ID ì§‘í•©

        # tokenizer ê°ì²´ ê°€ì ¸ì˜¤ê¸° (getattrë¡œ í†µì¼)
        tokenizer = getattr(self.processor, 'tokenizer', self.processor)
        print(f"ğŸ“Š Tokenizer type: {type(tokenizer).__name__}")
        
        # 1. special_tokens_mapì˜ ëª¨ë“  í† í° ì²˜ë¦¬ (additional_special_tokens í¬í•¨)
        self._process_all_special_tokens(tokenizer)
        
        # 2. apply_chat_template í˜¸í™˜ì„± ê²€ì¦
        self._verify_chat_template_compatibility(tokenizer)
        
        # 3. Manual config ì²˜ë¦¬ (ê³ ê¸‰ ì‚¬ìš©ììš© override)
        self._process_manual_config_if_enabled(tokenizer)
        
        print(f"âœ… Auto-detection complete.")
        print(f"   ğŸ“‹ Total special tokens found: {len(self.special_token_ids)}")
        print(f"   ğŸš« Tokens to ignore in loss: {len(self.ignore_in_loss_ids)}")

    def _process_all_special_tokens(self, tokenizer):
        """special_tokens_mapì˜ ëª¨ë“  íŠ¹ìˆ˜ í† í°ê³¼ additional_special_tokensë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        print("  ğŸ”§ Processing all special tokens from special_tokens_map...")
        
        special_tokens_map = getattr(tokenizer, 'special_tokens_map', {})
        print(f"special_tokens_map length : {len(special_tokens_map)}")

        # 1. special_tokens_map ì²˜ë¦¬
        for token_attr, token_str in special_tokens_map.items():
            try:
                # additional_special_tokensëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
                if token_attr == 'additional_special_tokens':
                    continue
                    
                # í† í° ID ê°€ì ¸ì˜¤ê¸° (getattrë¡œ í†µì¼)
                token_id = getattr(tokenizer, f'{token_attr}_id', None)
                
                # token_idê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (ì‹¤ì œ ë¬¸ì œ ì›ì¸)
                if isinstance(token_id, list):
                    if len(token_id) > 0:
                        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ëª¨ë“  IDë¥¼ ì²˜ë¦¬
                        print(f"    ğŸ“ Token ID '{token_attr}_id' is a list: {token_id}, adding all IDs")
                        for idx, single_id in enumerate(token_id):
                            # token_attr ê·¸ëŒ€ë¡œ ì‚¬ìš© (clean_name ì‚¬ìš© ì•ˆí•¨)
                            if len(token_id) > 1:
                                # ì—¬ëŸ¬ IDê°€ ìˆëŠ” ê²½ìš° ì¸ë±ìŠ¤ ì¶”ê°€
                                token_name = f"{token_attr}_{idx}"
                            else:
                                token_name = token_attr
                            
                            self.special_token_ids[token_name] = single_id
                            self.ignore_in_loss_ids.add(single_id)
                            print(f"    âœ… {token_name}: '{token_str}' -> ID: {single_id}")
                    else:
                        print(f"    âš ï¸ Token ID '{token_attr}_id' is an empty list, skipping")
                        continue
                else:
                    # ë‹¨ì¼ IDì¸ ê²½ìš°
                    if token_id is not None:
                        # token_attr ê·¸ëŒ€ë¡œ ì‚¬ìš© (clean_name ì‚¬ìš© ì•ˆí•¨)
                        self.special_token_ids[token_attr] = token_id
                        
                        # ëª¨ë“  íŠ¹ìˆ˜ í† í°ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
                        self.ignore_in_loss_ids.add(token_id)
                        
                        print(f"    âœ… {token_attr}: '{token_str}' -> ID: {token_id}")
                    else:
                        print(f"    âš ï¸ No ID found for token '{token_attr}': '{token_str}'")
                    
            except Exception as e:
                print(f"    âŒ Error processing token '{token_attr}': {e}")
                print(f"    ğŸ” Token value type: {type(token_str)}, Token ID type: {type(token_id)}")
                print(f"    ğŸ” Token attr: '{token_attr}', Token str: {token_str}, Token ID: {token_id}")
                continue

        # 2. additional_special_tokens ì²˜ë¦¬ (í†µí•©)
        print("  ğŸ¯ Processing additional special tokens...")
        additional_tokens = getattr(tokenizer, 'additional_special_tokens', [])
        additional_token_ids = getattr(tokenizer, 'additional_special_tokens_ids', [])
        
        if additional_tokens:
            print(f"    ğŸ“ Found {len(additional_tokens)} additional special tokens")
            
            for i, token_str in enumerate(additional_tokens):
                if i < len(additional_token_ids):
                    token_id = additional_token_ids[i]
                    
                    # ê°„ë‹¨í•œ í† í° ì´ë¦„ ìƒì„±
                    clean_token = token_str.replace('<', '').replace('>', '').replace('|', '_')
                    token_name = f"special_{clean_token}"
                    
                    self.special_token_ids[token_name] = token_id
                    
                    # ëª¨ë“  ì¶”ê°€ íŠ¹ìˆ˜ í† í°ë„ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸
                    self.ignore_in_loss_ids.add(token_id)
                    
                    print(f"    âœ… {token_name}: '{token_str}' -> ID: {token_id}")

    def _verify_chat_template_compatibility(self, tokenizer):
        """apply_chat_templateê³¼ì˜ í˜¸í™˜ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        print("  ğŸ” Verifying chat template compatibility...")
        
        # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
        test_messages = [
            {"role": "user", "content": [
                {"type": "text", "text": "Test message"},
                {"type": "image"},
                {"type": "text", "text": "What do you see?"}
            ]},
            {"role": "assistant", "content": [
                {"type": "text", "text": "I see an image."}
            ]}
        ]
        
        try:
            test_text = self.processor.apply_chat_template(
                test_messages, 
                tokenize=False,
                add_generation_prompt=False
            )
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ í† í° í™•ì¸
            self._check_template_tokens(test_text, tokenizer)
            print("    âœ… Chat template compatibility verified")
                
        except Exception as e:
            print(f"    âš ï¸ Chat template test failed: {e}")

    def _check_template_tokens(self, template_text, tokenizer):
        """í…œí”Œë¦¿ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ íŠ¹ìˆ˜ í† í°ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤."""
        print(f"    ğŸ“„ Template text preview: {template_text[:100]}...")
        
        # í…œí”Œë¦¿ì—ì„œ íŠ¹ìˆ˜ í† í° íŒ¨í„´ ì°¾ê¸°
        special_token_pattern = r'<[^>]+>'
        found_tokens = re.findall(special_token_pattern, template_text)
        
        if found_tokens:
            print(f"    ğŸ¯ Found template tokens: {found_tokens}")
            
            # ë°œê²¬ëœ í† í°ë“¤ì´ ìš°ë¦¬ê°€ ê°ì§€í•œ í† í° ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
            for token in found_tokens:
                token_id = tokenizer.convert_tokens_to_ids(token)
                if token_id != tokenizer.unk_token_id:
                    # ìƒˆë¡œìš´ í† í° ë°œê²¬ì‹œ ì¶”ê°€
                    token_name = f"template_token_{token.replace('<', '').replace('>', '')}"
                    if token_id not in self.special_token_ids.values():
                        self.special_token_ids[token_name] = token_id
                        self.ignore_in_loss_ids.add(token_id)
                        print(f"    ğŸ†• Added template token: '{token}' -> ID: {token_id}")

    def _process_manual_config_if_enabled(self, tokenizer):
        """YAML ì„¤ì •ì—ì„œ manual_tokensê°€ í™œì„±í™”ëœ ê²½ìš° ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        manual_tokens = self.special_tokens_config.get('manual_tokens', {})
        
        if not manual_tokens.get('enabled', False):
            print("  â­ï¸ Manual token configuration disabled (using auto-detection only)")
            return
        
        print("  ğŸ”§ Processing manual token configuration...")
        manual_token_list = manual_tokens.get('tokens', [])
        
        if not manual_token_list:
            print("    âš ï¸ Manual tokens enabled but no tokens specified")
            return
        
        override_count = 0
        new_count = 0
        
        for token_config in manual_token_list:
            if not isinstance(token_config, dict):
                print(f"    âŒ Invalid token config (must be dict): {token_config}")
                continue
                
            token_name = token_config.get('name')
            token_text = token_config.get('token')
            ignore_in_loss = token_config.get('ignore_in_loss', True)
            
            if not token_name or not token_text:
                print(f"    âŒ Invalid token config (missing name/token): {token_config}")
                continue
            
            # í† í° ID ê³„ì‚°
            token_id = tokenizer.convert_tokens_to_ids(token_text)
            if token_id == tokenizer.unk_token_id:
                print(f"    âš ï¸ Unknown token '{token_text}' for '{token_name}' - skipping")
                continue
            
            # ê¸°ì¡´ í† í° override ë˜ëŠ” ìƒˆ í† í° ì¶”ê°€
            if token_name in self.special_token_ids:
                old_id = self.special_token_ids[token_name]
                print(f"    ğŸ”„ Override '{token_name}': {old_id} -> {token_id}")
                override_count += 1
                
                # ê¸°ì¡´ ID ì œê±°
                if old_id in self.ignore_in_loss_ids:
                    self.ignore_in_loss_ids.remove(old_id)
            else:
                print(f"    â• Add manual token '{token_name}': {token_id}")
                new_count += 1
            
            # ìƒˆ ì„¤ì • ì ìš©
            self.special_token_ids[token_name] = token_id
            if ignore_in_loss:
                self.ignore_in_loss_ids.add(token_id)
        
        if override_count > 0 or new_count > 0:
            print(f"    âœ… Manual config processed: {override_count} overrides, {new_count} new tokens")
        else:
            print("    â„¹ï¸ No valid manual tokens processed")

    def _process_image(self, image) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ PIL Image ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤."""
        if image is None:
            return None
            
        # 1. ë¬¸ìì—´(íŒŒì¼ ê²½ë¡œ)ì¸ ê²½ìš° ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            try:
                # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                image_path = Path(image)
                if not image_path.is_absolute():
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
                    image_path = Path.cwd() / image_path
                
                if not image_path.exists():
                    print(f"âš ï¸ Image file not found: {image_path}")
                    return None
                
                # PIL Imageë¡œ ë¡œë“œ
                image = Image.open(image_path)
                print(f"âœ… Successfully loaded image from path: {image_path}")
                
            except Exception as e:
                print(f"âŒ Error loading image from path '{image}': {e}")
                return None
        
        # 2. PIL Imageê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
        elif not isinstance(image, Image.Image):
            convert_method = getattr(image, 'convert', None)
            if convert_method:  # PIL-like object
                try:
                    image = convert_method('RGB')
                except Exception as e:
                    print(f"âš ï¸ Error converting PIL-like object: {e}")
                    return None
            else:
                # numpy arrayë‚˜ ë‹¤ë¥¸ í˜•ì‹ì¸ ê²½ìš°
                try:
                    import numpy as np
                    if isinstance(image, np.ndarray):
                        image = Image.fromarray(image)
                    else:
                        print(f"âš ï¸ Unsupported image type: {type(image)}")
                        return None
                except Exception as e:
                    print(f"âš ï¸ Could not convert image to PIL Image: {e}")
                    return None
        
        # 3. RGB ë³€í™˜ (ì„¤ì •ì— ë”°ë¼)
        if self.image_processing.get('convert_to_rgb', True):
            try:
                if image.mode != 'RGB':
                    image = image.convert('RGB')
            except Exception as e:
                print(f"âš ï¸ Error converting to RGB: {e}")
                return None
        
        return image
    
    def _process_video(self, video) -> Optional[List[Image.Image]]:
        """ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if video is None or not self.video_processing.get('enabled', False):
            return None
            
        # 1. ë¬¸ìì—´(íŒŒì¼ ê²½ë¡œ)ì¸ ê²½ìš° ë¹„ë””ì˜¤ ë¡œë“œ
        if isinstance(video, str):
            try:
                # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                video_path = Path(video)
                if not video_path.is_absolute():
                    video_path = Path.cwd() / video_path
                
                if not video_path.exists():
                    print(f"âš ï¸ Video file not found: {video_path}")
                    return None
                
                return self._extract_video_frames(str(video_path))
                
            except Exception as e:
                print(f"âŒ Error processing video from path '{video}': {e}")
                return None
        else:
            print(f"âš ï¸ Unsupported video type: {type(video)}")
            return None
    
    def _extract_video_frames(self, video_path: str) -> Optional[List[Image.Image]]:
        """ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        frame_config = self.video_processing.get('frame_extraction', {})
        library = frame_config.get('library', 'decord')
        num_frames = frame_config.get('num_frames', 8)
        sampling_strategy = frame_config.get('sampling_strategy', 'uniform')
        
        if library == 'decord' and DECORD_AVAILABLE:
            return self._extract_frames_with_decord(video_path, num_frames, sampling_strategy)
        elif library == 'cv2' and CV2_AVAILABLE:
            return self._extract_frames_with_cv2(video_path, num_frames, sampling_strategy)
        else:
            # Fallback to cv2 if available
            if CV2_AVAILABLE:
                print(f"âš ï¸ {library} not available, falling back to cv2")
                return self._extract_frames_with_cv2(video_path, num_frames, sampling_strategy)
            else:
                print(f"âŒ No video processing library available (decord: {DECORD_AVAILABLE}, cv2: {CV2_AVAILABLE})")
                return None
    
    def _extract_frames_with_decord(self, video_path: str, num_frames: int, sampling_strategy: str) -> Optional[List[Image.Image]]:
        """Decordë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            vr = decord.VideoReader(video_path, ctx=decord.cpu(0))
            total_frames = len(vr)
            
            if total_frames == 0:
                print(f"âš ï¸ Video has no frames: {video_path}")
                return None
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
            if sampling_strategy == 'uniform':
                if total_frames < num_frames:
                    # ë¹„ë””ì˜¤ê°€ ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ë³´ë‹¤ ì ì€ ê²½ìš° ëª¨ë“  í”„ë ˆì„ ì‚¬ìš©
                    indices = list(range(total_frames))
                else:
                    # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                    indices = torch.linspace(0, total_frames - 1, num_frames).long().tolist()
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ uniform ì‚¬ìš©
                indices = torch.linspace(0, total_frames - 1, min(num_frames, total_frames)).long().tolist()
            
            # í”„ë ˆì„ ì¶”ì¶œ
            frames = vr.get_batch(indices).asnumpy()  # Shape: (num_frames, H, W, C)
            
            # PIL Imageë¡œ ë³€í™˜
            pil_frames = []
            for frame in frames:
                pil_frame = Image.fromarray(frame)
                # RGB ë³€í™˜ ì˜µì…˜ ì ìš©
                if self.video_processing.get('file_processing', {}).get('convert_to_rgb', True):
                    if pil_frame.mode != 'RGB':
                        pil_frame = pil_frame.convert('RGB')
                pil_frames.append(pil_frame)
            
            print(f"âœ… Successfully extracted {len(pil_frames)} frames from video: {video_path}")
            return pil_frames
            
        except Exception as e:
            print(f"âŒ Error extracting frames with decord: {e}")
            return None
    
    def _extract_frames_with_cv2(self, video_path: str, num_frames: int, sampling_strategy: str) -> Optional[List[Image.Image]]:
        """OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames == 0:
                print(f"âš ï¸ Video has no frames: {video_path}")
                cap.release()
                return None
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
            if sampling_strategy == 'uniform':
                if total_frames < num_frames:
                    indices = list(range(total_frames))
                else:
                    indices = torch.linspace(0, total_frames - 1, num_frames).long().tolist()
            else:
                indices = torch.linspace(0, total_frames - 1, min(num_frames, total_frames)).long().tolist()
            
            # í”„ë ˆì„ ì¶”ì¶œ
            pil_frames = []
            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # BGR to RGB ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_frame = Image.fromarray(frame_rgb)
                    pil_frames.append(pil_frame)
                else:
                    print(f"âš ï¸ Failed to read frame at index {idx}")
            
            cap.release()
            
            if pil_frames:
                print(f"âœ… Successfully extracted {len(pil_frames)} frames from video: {video_path}")
                return pil_frames
            else:
                print(f"âŒ No frames could be extracted from video: {video_path}")
                return None
                
        except Exception as e:
            print(f"âŒ Error extracting frames with cv2: {e}")
            return None
    
    def _format_messages(self, example: dict, is_training: bool = True) -> list:
        """ì˜ˆì œë¥¼ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        # ì»¬ëŸ¼ëª… ë§¤í•‘
        image_col = self.dataset_columns.get('image_column', 'image')
        question_col = self.dataset_columns.get('question_column', 'question')
        answer_col = self.dataset_columns.get('answer_column', 'answer')
        
        # ë°ì´í„° ì¶”ì¶œ
        question = example.get(question_col, '')
        answer = example.get(answer_col, '') if is_training else ''
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = self.message_format.get('system_prompt', 'Answer briefly.')
        
        # ë©”ì‹œì§€ í…œí”Œë¦¿ ì„ íƒ
        if is_training:
            messages_template = self.message_format.get('training_messages', [])
        else:
            messages_template = self.message_format.get('evaluation_messages', [])
        
        # í…œí”Œë¦¿ì— ë°ì´í„° ì±„ìš°ê¸°
        messages = []
        for msg_template in messages_template:
            message = {
                'role': msg_template['role'],
                'content': []
            }
            
            for content_item in msg_template['content']:
                if content_item['type'] == 'text':
                    text = content_item['text'].format(
                        system_prompt=system_prompt,
                        question=question,
                        answer=answer
                    )
                    message['content'].append({
                        'type': 'text',
                        'text': text
                    })
                elif content_item['type'] == 'image':
                    message['content'].append({
                        'type': 'image'
                    })
                else:
                    # ë‹¤ë¥¸ íƒ€ì…ë“¤ (ë¹„ë””ì˜¤ ë“±) ì§€ì›
                    message['content'].append(content_item.copy())
            
            messages.append(message)
        
        return messages
    
    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        ë°°ì¹˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ - í”Œë˜ê·¸ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì œì–´
        
        Args:
            examples: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, torch.Tensor]: ëª¨ë¸ ì…ë ¥ìš© í…ì„œ ë”•ì…”ë„ˆë¦¬
        """
        texts = []
        visual_data = []  # ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        
        # ë°ì´í„° ì²˜ë¦¬ í”Œë˜ê·¸ í™•ì¸
        process_image = self.data_processing.get('image_data', True)  # ê¸°ë³¸ê°’: True
        process_video = self.data_processing.get('video_data', False)  # ê¸°ë³¸ê°’: False
        
        # ë‘ í”Œë˜ê·¸ê°€ ëª¨ë‘ í™œì„±í™”ëœ ê²½ìš° í™•ì¸ (processor í˜¸í™˜ì„± ê²€ì‚¬)
        if process_image and process_video:
            # ëŒ€ë¶€ë¶„ì˜ VLM processorëŠ” images íŒŒë¼ë¯¸í„°ì— í•˜ë‚˜ì˜ íƒ€ì…ë§Œ ë°›ì„ ìˆ˜ ìˆìŒ
            # ì—¬ê¸°ì„œëŠ” videoë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³  ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
            print("âš ï¸ Both image_data and video_data are enabled. Using video data as priority.")
            print("ğŸ’¡ Note: Most VLM processors can only handle one visual data type at a time.")
            process_image = False  # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™”
        
        # ë°°ì¹˜ì˜ ê° ì˜ˆì œ ì²˜ë¦¬
        for example in examples:
            processed_visuals = []
            
            # 1. ë¹„ë””ì˜¤ ì²˜ë¦¬ (video_data í”Œë˜ê·¸ê°€ í™œì„±í™”ëœ ê²½ìš°)
            if process_video:
                video_col = self.dataset_columns.get('video_column', 'video')
                if video_col in example and example[video_col] is not None:
                    video_frames = self._process_video(example[video_col])
                    if video_frames:
                        processed_visuals.extend(video_frames)

            
            # 2. ì´ë¯¸ì§€ ì²˜ë¦¬ (image_data í”Œë˜ê·¸ê°€ í™œì„±í™”ëœ ê²½ìš°)
            if process_image and not processed_visuals:  # ë¹„ë””ì˜¤ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ
                image_col = self.dataset_columns.get('image_column', 'image')
                if image_col in example and example[image_col] is not None:
                    processed_image = self._process_image(example[image_col])
                    if processed_image is not None:
                        processed_visuals.append(processed_image)
            
            # í”„ë¡œì„¸ì„œì— ë§ëŠ” í˜•íƒœë¡œ ë˜í•‘
            if processed_visuals:
                visual_data.append(processed_visuals)
            else:
                visual_data.append([])  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            
            # 3. ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•™ìŠµìš©)
            messages = self._format_messages(example, is_training=True)
            
            # 4. ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            try:
                text = self.processor.apply_chat_template(
                    messages, 
                    add_generation_prompt=self.text_processing.get('add_generation_prompt', False),
                    tokenize=False
                )
                texts.append(text.strip())
            except Exception as e:
                print(f"âš ï¸ Error applying chat template: {e}")
                # fallback: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²°í•©
                question = example.get(self.dataset_columns.get('question_column', 'question'), '')
                answer = example.get(self.dataset_columns.get('answer_column', 'answer'), '')
                
                # ë¹„ë””ì˜¤/ì´ë¯¸ì§€ íƒœê·¸ ì¶”ê°€
                if processed_visuals:
                    if len(processed_visuals) > 1:  # ë¹„ë””ì˜¤ (ë‹¤ì¤‘ í”„ë ˆì„)
                        visual_tag = "<video>"
                    else:  # ë‹¨ì¼ ì´ë¯¸ì§€
                        visual_tag = "<image>"
                    texts.append(f"{visual_tag}\nQuestion: {question}\nAnswer: {answer}")
                else:
                    texts.append(f"Question: {question}\nAnswer: {answer}")
        
        # 5. í”„ë¡œì„¸ì„œë¡œ ë°°ì¹˜ ì²˜ë¦¬
        try:
            # VLM ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì ìš©
            batch = self._process_with_processor(texts, visual_data)
        except Exception as e:
            print(f"âŒ Error processing batch with processor: {e}")
            # fallback: í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
            batch = self.processor(
                text=texts,
                return_tensors=self.batch_processing.get('return_tensors', 'pt'),
                padding=self.text_processing.get('padding', True),
                truncation=self.text_processing.get('truncation', True),
                max_length=self.text_processing.get('max_length', 2048)
            )
        
        # 6. ë ˆì´ë¸” ìƒì„± ë° ë§ˆìŠ¤í‚¹ (ì¼ë°˜í™”ëœ ë²„ì „)
        labels = batch["input_ids"].clone()
        ignore_index = self.label_masking.get('ignore_index', -100)
        
        # ë¯¸ë¦¬ ê³„ì‚°ëœ ignore_in_loss_ids ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ë§ˆìŠ¤í‚¹
        if self.ignore_in_loss_ids:
            # boolean ë§ˆìŠ¤í¬ ìƒì„±: labels í…ì„œì˜ ê° ìš”ì†Œê°€ ë¬´ì‹œí•  ID ì§‘í•©ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            mask = torch.isin(labels, torch.tensor(list(self.ignore_in_loss_ids), device=labels.device))
            # ë§ˆìŠ¤í¬ê°€ Trueì¸ ìœ„ì¹˜ì˜ ê°’ì„ ignore_indexë¡œ ë³€ê²½
            labels[mask] = ignore_index
            print(f"ğŸ”§ Masked {torch.sum(mask).item()} tokens in loss calculation")
        
        # (ì„ íƒì ) ì¶”ê°€ ë§ˆìŠ¤í‚¹ ë¡œì§
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹ì´ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆ: assistant ì‘ë‹µ ì‹œì‘ ì „ê¹Œì§€ì˜ ëª¨ë“  í† í°ì„ ë§ˆìŠ¤í‚¹
        if self.label_masking.get('mask_input_tokens', False):
            # ì´ ë¶€ë¶„ì€ ëª¨ë¸ë³„ chat templateì— ë”°ë¼ ë‹¤ë¥´ê²Œ êµ¬í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            print("ğŸ’¡ Input token masking is enabled but not implemented yet.")
        
        batch["labels"] = labels
        
        return batch
    
    def _process_with_processor(self, texts: List[str], visual_data: List[List[Image.Image]]) -> Dict[str, torch.Tensor]:
        """í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì‹œê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        
        # ë¹ˆ ì‹œê° ë°ì´í„° í•„í„°ë§ ë° í‰íƒ„í™”
        actual_images = []
        for visuals in visual_data:
            if visuals and len(visuals) > 0:
                # ì‹œê° ë°ì´í„°ê°€ PIL Image ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                if isinstance(visuals[0], Image.Image):
                    if len(visuals) == 1:
                        # ë‹¨ì¼ ì´ë¯¸ì§€: ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        actual_images.append(visuals[0])
                    else:
                        # ë‹¤ì¤‘ í”„ë ˆì„ (ë¹„ë””ì˜¤): ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ ì‚¬ìš© (VLM processor í˜¸í™˜ì„±)
                        actual_images.append(visuals[0])
                        print(f"ğŸ“¹ Using first frame from {len(visuals)} video frames")
        
        if actual_images:
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
            try:
                batch = self.processor(
                    text=texts,
                    images=actual_images,
                    return_tensors=self.batch_processing.get('return_tensors', 'pt'),
                    padding=self.text_processing.get('padding', True),
                    truncation=self.text_processing.get('truncation', True),
                    max_length=self.text_processing.get('max_length', 2048)
                )
                return batch
            except Exception as e:
                print(f"âš ï¸ Error processing with images, trying text-only: {e}")
        
        # ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
        batch = self.processor(
            text=texts,
            return_tensors=self.batch_processing.get('return_tensors', 'pt'),
            padding=self.text_processing.get('padding', True),
            truncation=self.text_processing.get('truncation', True),
            max_length=self.text_processing.get('max_length', 2048)
        )
        
        return batch

def create_vlm_collator(processor, config_path: str = 'vlm_collator_config.yaml'):
    """
    VLM ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        processor: VLM í”„ë¡œì„¸ì„œ
        config_path: ì½œë ˆì´í„° ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        VLMDataCollator: ì„¤ì •ëœ ë°ì´í„° ì½œë ˆì´í„°
    """
    config = load_collator_config(config_path)
    return VLMDataCollator(processor=processor, config=config)
