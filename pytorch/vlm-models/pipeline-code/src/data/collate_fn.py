#!/usr/bin/env python3
"""
VLM Data Collator
VLM ëª¨ë¸ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì½œë ˆì´í„°
ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ë©°, ì„¤ì • íŒŒì¼ì„ í†µí•´ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
"""

import os
import yaml
import torch
from pathlib import Path
from PIL import Image
from typing import List, Dict, Any, Optional, Union, Tuple
from collections import defaultdict

# ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ íƒì  import
try:
    import decord
    DECORD_AVAILABLE = True
except ImportError:
    DECORD_AVAILABLE = False
    decord = None

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

def load_collator_config(config_path: str) -> dict:
    """ì½œë ˆì´í„° ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    from configs.settings import settings
    
    config_file = settings.config_path / config_path
    print(f"Loading VLM collator config from: {config_file}")
    
    if not config_file.exists():
        raise FileNotFoundError(f"Collator config file not found: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config

class VLMDataCollator:
    """
    VLM ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì½œë ˆì´í„°
    ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ë©°, ì„¤ì • íŒŒì¼ì„ í†µí•´ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
    """
    
    def __init__(self, processor: Any, config: dict):
        """
        VLMDataCollator ì´ˆê¸°í™”
        
        Args:
            processor: VLM processor (tokenizer + image processor)
            config: ì½œë ˆì´í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.processor = processor
        self.config = config
        
        # ì„¤ì •ê°’ë“¤ì„ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì €ì¥ (íŠ¹ìˆ˜ í† í° ì„¤ì • ì „ì— ë¨¼ì € ì´ˆê¸°í™”)
        self.dataset_columns = self.config.get('dataset_columns', {})
        self.message_format = self.config.get('message_format', {})
        self.data_processing = self.config.get('data_processing', {})  # ìƒˆë¡œ ì¶”ê°€
        self.image_processing = self.config.get('image_processing', {})
        self.text_processing = self.config.get('text_processing', {})
        self.label_masking = self.config.get('label_masking', {})
        self.batch_processing = self.config.get('batch_processing', {})
        self.video_processing = self.config.get('video_processing', {})  # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€
        
        # íŠ¹ìˆ˜ í† í° ID ë¯¸ë¦¬ ê³„ì‚° (ì„¤ì • ì´ˆê¸°í™” í›„)
        self._setup_special_tokens()
    
    def _setup_special_tokens(self):
        """í† í¬ë‚˜ì´ì €ì˜ íŠ¹ìˆ˜ í† í°ì„ ê°„ë‹¨í•˜ê²Œ ê°ì§€í•˜ì—¬ ì†ì‹¤ì—ì„œ ë¬´ì‹œí•©ë‹ˆë‹¤."""
        print("ğŸ” Detecting special tokens via tokenizer.all_special_ids...")
        self.special_token_ids = {}
        self.ignore_in_loss_ids = set()

        tokenizer = getattr(self.processor, 'tokenizer', self.processor)
        all_ids = set(getattr(tokenizer, 'all_special_ids', []) or [])

        # ê°„ë‹¨í™”: ëª¨ë“  special idë¥¼ ë¬´ì‹œ ëŒ€ìƒì— ì¶”ê°€
        self.ignore_in_loss_ids.update(all_ids)
        print(f"special token ids : {self.ignore_in_loss_ids}  will be ignored in loss.")
        # ì„ íƒì ìœ¼ë¡œ ì´ë¦„ ë§¤í•‘(ë¡œê¹…ìš©) ì±„ìš°ê¸°
        try:
            special_map = getattr(tokenizer, 'special_tokens_map', {}) or {}
            print(f"special_tokens_map: {special_map}")
            for k, tok in special_map.items():
                if k == 'additional_special_tokens':
                    # list ì²˜ë¦¬
                    add_ids = getattr(tokenizer, 'additional_special_tokens_ids', []) or []
                    for i, sid in enumerate(add_ids):
                        self.special_token_ids[f"additional_special_tokens_{i}"] = sid
                else:
                    sid = getattr(tokenizer, f"{k}_id", None)
                    if isinstance(sid, list):
                        for i, s in enumerate(sid):
                            self.special_token_ids[f"{k}_{i}"] = s
                    elif sid is not None:
                        self.special_token_ids[k] = sid
            print(f"special_token_ids detected: {self.special_token_ids}")
        except Exception:
            pass

        print(f"âœ… Special tokens collected: ignore_in_loss_ids={len(self.ignore_in_loss_ids)}")

    

    def _process_image(self, image) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ PIL Image ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤."""
        if image is None:
            return None
            
        # 1. ë¬¸ìì—´(íŒŒì¼ ê²½ë¡œ)ì¸ ê²½ìš° ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            try:
                # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                image_path = Path(image)
                if not image_path.is_absolute():
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
                    image_path = Path.cwd() / image_path
                
                if not image_path.exists():
                    print(f"âš ï¸ Image file not found: {image_path}")
                    return None
                
                # PIL Imageë¡œ ë¡œë“œ
                image = Image.open(image_path)
                print(f"âœ… Successfully loaded image from path: {image_path}")
                
            except Exception as e:
                print(f"âŒ Error loading image from path '{image}': {e}")
                return None
        
        # 2. PIL Imageê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
        elif not isinstance(image, Image.Image):
            convert_method = getattr(image, 'convert', None)
            if convert_method:  # PIL-like object
                try:
                    image = convert_method('RGB')
                except Exception as e:
                    print(f"âš ï¸ Error converting PIL-like object: {e}")
                    return None
            else:
                # numpy arrayë‚˜ ë‹¤ë¥¸ í˜•ì‹ì¸ ê²½ìš°
                try:
                    import numpy as np
                    if isinstance(image, np.ndarray):
                        image = Image.fromarray(image)
                    else:
                        print(f"âš ï¸ Unsupported image type: {type(image)}")
                        return None
                except Exception as e:
                    print(f"âš ï¸ Could not convert image to PIL Image: {e}")
                    return None
        
        # 3. RGB ë³€í™˜ (ì„¤ì •ì— ë”°ë¼)
        if self.image_processing.get('convert_to_rgb', True):
            try:
                if image.mode != 'RGB':
                    image = image.convert('RGB')
            except Exception as e:
                print(f"âš ï¸ Error converting to RGB: {e}")
                return None
        
        return image
    
    def _process_video(self, video) -> Optional[List[Tuple[Image.Image, Optional[float]]]]:
        """ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ (í”„ë ˆì„, íƒ€ì„ìŠ¤íƒ¬í”„) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Returns: List of tuples (PIL.Image, timestamp_seconds or None)
        """
        if video is None or not self.video_processing.get('enabled', False):
            return None
            
        # 1. ë¬¸ìì—´(íŒŒì¼ ê²½ë¡œ)ì¸ ê²½ìš° ë¹„ë””ì˜¤ ë¡œë“œ
        if isinstance(video, str):
            try:
                # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                video_path = Path(video)
                if not video_path.is_absolute():
                    video_path = Path.cwd() / video_path
                
                if not video_path.exists():
                    print(f"âš ï¸ Video file not found: {video_path}")
                    return None
                
                return self._extract_video_frames(str(video_path))
                
            except Exception as e:
                print(f"âŒ Error processing video from path '{video}': {e}")
                return None
        else:
            print(f"âš ï¸ Unsupported video type: {type(video)}")
            return None
    
    def _extract_video_frames(self, video_path: str) -> Optional[List[Tuple[Image.Image, Optional[float]]]]:
        """ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í”„ë ˆì„ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        Returns: List of tuples (PIL.Image, timestamp_seconds or None)
        """
        frame_config = self.video_processing.get('frame_extraction', {})
        library = frame_config.get('library', 'decord')
        num_frames = frame_config.get('num_frames', 8)
        sampling_strategy = frame_config.get('sampling_strategy', 'uniform')
        
        if library == 'decord' and DECORD_AVAILABLE:
            return self._extract_frames_with_decord(video_path, num_frames, sampling_strategy)
        elif library == 'cv2' and CV2_AVAILABLE:
            return self._extract_frames_with_cv2(video_path, num_frames, sampling_strategy)
        else:
            # Fallback to cv2 if available
            if CV2_AVAILABLE:
                print(f"âš ï¸ {library} not available, falling back to cv2")
                return self._extract_frames_with_cv2(video_path, num_frames, sampling_strategy)
            else:
                print(f"âŒ No video processing library available (decord: {DECORD_AVAILABLE}, cv2: {CV2_AVAILABLE})")
                return None
    
    def _extract_frames_with_decord(self, video_path: str, num_frames: int, sampling_strategy: str) -> Optional[List[Tuple[Image.Image, Optional[float]]]]:
        """Decordë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            vr = decord.VideoReader(video_path, ctx=decord.cpu(0))
            total_frames = len(vr)
            
            if total_frames == 0:
                print(f"âš ï¸ Video has no frames: {video_path}")
                return None
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
            if sampling_strategy == 'uniform':
                if total_frames < num_frames:
                    # ë¹„ë””ì˜¤ê°€ ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ë³´ë‹¤ ì ì€ ê²½ìš° ëª¨ë“  í”„ë ˆì„ ì‚¬ìš©
                    indices = list(range(total_frames))
                else:
                    # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                    indices = torch.linspace(0, total_frames - 1, num_frames).long().tolist()
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ uniform ì‚¬ìš©
                indices = torch.linspace(0, total_frames - 1, min(num_frames, total_frames)).long().tolist()
            
            # í”„ë ˆì„ ì¶”ì¶œ
            frames = vr.get_batch(indices).asnumpy()  # Shape: (num_frames, H, W, C)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚° ì‹œë„
            timestamps: List[Optional[float]] = []
            for idx in indices:
                ts_val: Optional[float] = None
                try:
                    ts_val = float(vr.get_frame_timestamp(int(idx)))
                except Exception:
                    try:
                        fps = float(vr.get_avg_fps())
                        ts_val = (float(idx) / fps) if fps and fps > 0 else None
                    except Exception:
                        ts_val = None
                timestamps.append(ts_val)

            # PIL Imageë¡œ ë³€í™˜
            pil_frames_with_ts: List[Tuple[Image.Image, Optional[float]]] = []
            for frame, ts in zip(frames, timestamps):
                pil_frame = Image.fromarray(frame)
                # RGB ë³€í™˜ ì˜µì…˜ ì ìš©
                if self.video_processing.get('file_processing', {}).get('convert_to_rgb', True):
                    if pil_frame.mode != 'RGB':
                        pil_frame = pil_frame.convert('RGB')
                pil_frames_with_ts.append((pil_frame, ts))
            
            print(f"âœ… Successfully extracted {len(pil_frames_with_ts)} frames from video: {video_path}")
            return pil_frames_with_ts
            
        except Exception as e:
            print(f"âŒ Error extracting frames with decord: {e}")
            return None
    
    def _extract_frames_with_cv2(self, video_path: str, num_frames: int, sampling_strategy: str) -> Optional[List[Tuple[Image.Image, Optional[float]]]]:
        """OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = float(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) else 0.0
            
            if total_frames == 0:
                print(f"âš ï¸ Video has no frames: {video_path}")
                cap.release()
                return None
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°
            if sampling_strategy == 'uniform':
                if total_frames < num_frames:
                    indices = list(range(total_frames))
                else:
                    indices = torch.linspace(0, total_frames - 1, num_frames).long().tolist()
            else:
                indices = torch.linspace(0, total_frames - 1, min(num_frames, total_frames)).long().tolist()
            
            # í”„ë ˆì„ ì¶”ì¶œ
            pil_frames_with_ts: List[Tuple[Image.Image, Optional[float]]] = []
            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # BGR to RGB ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_frame = Image.fromarray(frame_rgb)
                    ts_val: Optional[float] = (float(idx) / fps) if fps and fps > 0 else None
                    pil_frames_with_ts.append((pil_frame, ts_val))
                else:
                    print(f"âš ï¸ Failed to read frame at index {idx}")
            
            cap.release()
            
            if pil_frames_with_ts:
                print(f"âœ… Successfully extracted {len(pil_frames_with_ts)} frames from video: {video_path}")
                return pil_frames_with_ts
            else:
                print(f"âŒ No frames could be extracted from video: {video_path}")
                return None
                
        except Exception as e:
            print(f"âŒ Error extracting frames with cv2: {e}")
            return None
    
    def _format_messages(self, example: dict, is_training: bool = True) -> list:
        """ë©”ì‹œì§€ í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•  í¬ë§· ë³€ìˆ˜ë¥¼ dataset_columns ì „ì²´ë¡œ ì¼ë°˜í™”í•©ë‹ˆë‹¤.

        - vlm_collator_config.yaml ì˜ dataset_columns ë”•ì…”ë„ˆë¦¬ì˜ keyë“¤ì„ ë³€ìˆ˜ëª…ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - ê° keyì— ë§¤í•‘ëœ ì‹¤ì œ ì»¬ëŸ¼ëª…(ê°’)ìœ¼ë¡œë¶€í„° exampleì—ì„œ ë°ì´í„°ë¥¼ ì¼ê´„ ì¶”ì¶œí•˜ì—¬ í¬ë§· ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        - keyê°€ *_column ìœ¼ë¡œ ëë‚˜ë©´, ì ‘ë¯¸ì‚¬ë¥¼ ì œê±°í•œ ë³„ì¹­(ì˜ˆ: question_column -> question)ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.
        - system_promptë„ í¬ë§· ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤.
        """
        # 1) í¬ë§· ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        format_ctx: Dict[str, Any] = {}
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
        format_ctx['system_prompt'] = self.message_format.get('system_prompt', 'Answer briefly.')

        # dataset_columnsì˜ ëª¨ë“  í‚¤ë¥¼ ë³€ìˆ˜ë¡œ ë…¸ì¶œí•˜ê³  exampleì—ì„œ ê°’ ì¶”ì¶œ
        for key, col_name in (self.dataset_columns or {}).items():
            if col_name is None:
                value = ''
            else:
                value = example.get(col_name, '')
            format_ctx[key] = value
            # *_column ë³„ì¹­ ì œê³µ (message í…œí”Œë¦¿ì—ì„œ ê°„ê²°í•œ ì´ë¦„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
            if key.endswith('_column'):
                base_key = key[:-7]  # remove '_column'
                if base_key and base_key not in format_ctx:
                    format_ctx[base_key] = value

        # 2) ë©”ì‹œì§€ í…œí”Œë¦¿ ì„ íƒ
        messages_template = (
            self.message_format.get('training_messages', []) if is_training
            else self.message_format.get('evaluation_messages', [])
        )

        # 3) í…œí”Œë¦¿ ë Œë”ë§ (ì´ë¯¸ì§€ placeholderëŠ” ë¬´ì‹œ; ì´í›„ ë¹„ì£¼ì–¼ ì£¼ì… ë‹¨ê³„ì—ì„œ ì²˜ë¦¬)
        messages: List[Dict[str, Any]] = []
        safe_ctx = defaultdict(lambda: '', format_ctx)
        for msg_template in messages_template:
            message = {'role': msg_template['role'], 'content': []}
            for content_item in msg_template.get('content', []) or []:
                if content_item.get('type') == 'text':
                    try:
                        text = content_item['text'].format_map(safe_ctx)
                    except Exception:
                        # í˜•ì‹ ì˜¤ë¥˜ ì‹œ ì›ë¬¸ ìœ ì§€ + ìµœì†Œí•œì˜ ì•ˆì „ ì¥ì¹˜
                        text = str(content_item.get('text', ''))
                    message['content'].append({'type': 'text', 'text': text})
                elif content_item.get('type') == 'image':
                    # ê³ ì • ì´ë¯¸ì§€ placeholderëŠ” ì—¬ê¸°ì„œ ë¬´ì‹œ (ì‹¤ì œ ì´ë¯¸ì§€ëŠ” _build_messages_with_visualsì—ì„œ ì£¼ì…)
                    continue
                else:
                    message['content'].append(content_item.copy())
            messages.append(message)

        return messages
    
    def _build_messages_with_visuals(self, base_messages: list, visuals_count: int, timestamps: Optional[List[Optional[float]]]) -> list:
        """Inject visuals for user messages: visuals first, then all user text.
        """
        updated: List[Dict[str, Any]] = []
        for msg in base_messages:
            role = msg.get('role')
            contents = msg.get('content', []) or []

            # Non-user: copy as-is
            if role != 'user' or visuals_count <= 0:
                updated.append({'role': role, 'content': [c.copy() if isinstance(c, dict) else c for c in contents]})
                continue

            # Collect text segments (ignore any pre-existing image placeholders)
            text_segments: List[Dict[str, Any]] = [
                {'type': 'text', 'text': c.get('text', '')}
                for c in contents
                if isinstance(c, dict) and c.get('type') == 'text'
            ]

            # Build new content: visuals first
            new_content: List[Dict[str, Any]] = []
            for i in range(visuals_count):
                ts = None
                if timestamps is not None and i < len(timestamps):
                    ts = timestamps[i]

                label = f"Frame {i+1} (t={ts:.2f}s):" if ts is not None else f"Image {i+1}:"
                new_content.append({"type" : "text", "text" : label})
                new_content.append({"type": "image"})

            # Then append all original user text segments (order preserved)
            new_content.extend(text_segments)

            updated.append({'role': 'user', 'content': new_content})

        return updated

    def __call__(self, examples: List[Dict[str, Any]], is_training: bool = True) -> Dict[str, torch.Tensor]:
        """
        ë°°ì¹˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ - í”Œë˜ê·¸ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì œì–´
        
        Args:
            examples: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            is_training: í•™ìŠµ ëª¨ë“œ ì—¬ë¶€ (True: í•™ìŠµ, False: í‰ê°€)
            
        Returns:
            Dict[str, torch.Tensor]: ëª¨ë¸ ì…ë ¥ìš© í…ì„œ ë”•ì…”ë„ˆë¦¬
        """

        texts: List[str] = []
        visual_data: List[List[Image.Image]] = []  # ê° ìƒ˜í”Œë³„ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸

        # ë°ì´í„° ì²˜ë¦¬ í”Œë˜ê·¸ í™•ì¸
        process_image = self.data_processing.get('image_data', True)  # ê¸°ë³¸ê°’: True
        process_video = self.data_processing.get('video_data', False)  # ê¸°ë³¸ê°’: False

        # ë‘ í”Œë˜ê·¸ê°€ ëª¨ë‘ í™œì„±í™”ëœ ê²½ìš° í™•ì¸ (processor í˜¸í™˜ì„± ê²€ì‚¬)
        if process_image and process_video:
            # ëŒ€ë¶€ë¶„ì˜ VLM processorëŠ” images íŒŒë¼ë¯¸í„°ì— í•˜ë‚˜ì˜ íƒ€ì…ë§Œ ë°›ì„ ìˆ˜ ìˆìŒ
            # ì—¬ê¸°ì„œëŠ” videoë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³  ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
            print("âš ï¸ Both image_data and video_data are enabled. Using video data as priority.")
            print("ğŸ’¡ Note: Most VLM processors can only handle one visual data type at a time.")
            process_image = False  # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™”

        # ë°°ì¹˜ì˜ ê° ì˜ˆì œ ì²˜ë¦¬
        for example in examples:
            visuals_images: List[Image.Image] = []
            frame_timestamps: Optional[List[Optional[float]]] = None

            # 1) ë¹„ë””ì˜¤ ìš°ì„  ì²˜ë¦¬
            if process_video:
                video_col = self.dataset_columns.get('video_column', 'video')
                if video_col in example and example[video_col] is not None:
                    video_frames = self._process_video(example[video_col])
                    if video_frames:
                        visuals_images = [img for (img, _ts) in video_frames]
                        frame_timestamps = [ts for (_img, ts) in video_frames]

            # 2) ì´ë¯¸ì§€ ì²˜ë¦¬ (ë¹„ë””ì˜¤ ë¯¸ì‚¬ìš©ì´ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ)
            if (not process_video) or (not visuals_images):
                if self.data_processing.get('image_data', True):
                    image_col = self.dataset_columns.get('image_column', 'image')
                    if image_col in example and example[image_col] is not None:
                        img_val = example[image_col]
                        if isinstance(img_val, (list, tuple)):
                            for one in img_val:
                                processed = self._process_image(one)
                                if processed is not None:
                                    visuals_images.append(processed)
                        else:
                            processed = self._process_image(img_val)
                            if processed is not None:
                                visuals_images.append(processed)
                        # ì´ë¯¸ì§€ ë°ì´í„°ì—ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ
                        if visuals_images and frame_timestamps is None:
                            frame_timestamps = [None] * len(visuals_images)

            # 3) ë©”ì‹œì§€ ìƒì„± ë° chat template ì ìš© (ë‹¤ì¤‘ í”„ë ˆì„/ì´ë¯¸ì§€ ì§€ì›)
            base_messages = self._format_messages(example, is_training=is_training)
            messages = self._build_messages_with_visuals(base_messages, len(visuals_images), frame_timestamps)

            try:
                # Generation prompt ì •ì±…:
                # - í•™ìŠµ(is_training=True): ë§ˆì§€ë§‰ assistant ì‘ë‹µì´ ì´ë¯¸ messagesì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ False
                # - í‰ê°€(is_training=False): ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ assistant ì‹œì‘ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•˜ë¯€ë¡œ True
                text = self.processor.apply_chat_template(
                    messages,
                    add_generation_prompt=(not is_training),
                    tokenize=False
                )
                texts.append(text.strip())
            except Exception as e:
                print(f"âš ï¸ Error applying chat template: {e}")
                # fallback: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²°í•©
                question = example.get(self.dataset_columns.get('question_column', 'question'), '')
                answer = example.get(self.dataset_columns.get('answer_column', 'answer'), '')
                if visuals_images:
                    tag = "<video>" if len(visuals_images) > 1 else "<image>"
                    texts.append(f"{tag}\nQuestion: {question}\nAnswer: {answer}")
                else:
                    texts.append(f"Question: {question}\nAnswer: {answer}")

            # 4) ì‹œê° ë°ì´í„° ì €ì¥ (ìƒ˜í”Œ ë‹¨ìœ„ì˜ ë¦¬ìŠ¤íŠ¸)
            visual_data.append(visuals_images)

        # 5. í”„ë¡œì„¸ì„œë¡œ ë°°ì¹˜ ì²˜ë¦¬
        try:
            # VLM ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì ìš© (ìƒ˜í”Œë³„ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì§€ì›)
            batch = self._process_with_processor(texts, visual_data)
        except Exception as e:
            print(f"âŒ Error processing batch with processor: {e}")
            # fallback: í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
            batch = self.processor(
                text=texts,
                return_tensors=self.batch_processing.get('return_tensors', 'pt'),
                padding=self.text_processing.get('padding', True),
                truncation=self.text_processing.get('truncation', True),
                max_length=self.text_processing.get('max_length', 2048)
            )
        if is_training:
            # 6. ë ˆì´ë¸” ìƒì„± ë° ë§ˆìŠ¤í‚¹
            # ëª©í‘œ: assistant ì‘ë‹µ í† í°ë§Œ supervised signalì„ ì£¼ê³ ,
            # system/user(í”„ë¡¬í”„íŠ¸) ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            labels = batch["input_ids"].clone()
            ignore_index = self.label_masking.get('ignore_index', -100)

            # ìš°ì„  íŠ¹ìˆ˜ í† í°ì€ í•­ìƒ ë¬´ì‹œ
            if self.ignore_in_loss_ids:
                # print(f"{len(self.ignore_in_loss_ids)} special token ids will be ignored in loss.")
                ignore_ids = sorted({int(i) for i in self.ignore_in_loss_ids if isinstance(i, (int,))})
                
                ids_tensor = torch.tensor(ignore_ids, dtype=labels.dtype, device=labels.device)
                mask_special = torch.isin(labels, ids_tensor)
                masked_count = int(mask_special.sum().item())
                # print(f"ğŸ” Masking {masked_count} special tokens in labels (from {len(ignore_ids)} ids)")
                if masked_count:
                    total = int(labels.numel())
                    # print(f"   -> {masked_count}/{total} tokens ({100.0 * masked_count / total:.2f}%) will be ignored in loss")
                    # If nearly all tokens are considered special, it's likely wrong; skip to be safe
                    labels[mask_special] = ignore_index

            # ì‘ë‹µ ì‹œì‘ ìœ„ì¹˜ ê¸°ë°˜ ë§ˆìŠ¤í‚¹: ë¬¸ìì—´ ê¸°ì¤€ìœ¼ë¡œ answer í…ìŠ¤íŠ¸ ì‹œì‘ ì§€ì ì„ ì°¾ì•„ í† í° ì˜¤í”„ì…‹ê³¼ ì •ë ¬
            if self.label_masking.get('mask_input_tokens', True):
                try:
                    tokenizer = getattr(self.processor, 'tokenizer', self.processor)
                    answer_col = self.dataset_columns.get('answer_column', 'answer')
                    answers: List[str] = []
                    answer_starts: List[int] = []
                    for ex, txt in zip(examples, texts):
                        ans = ex.get(answer_col, '')
                        ans = '' if ans is None else str(ans)
                        answers.append(ans)
                        # ë‹µë³€ ë¬¸ìì—´ì´ í…œí”Œë¦¿ ë‚´ ì–´ë””ì„œ ì‹œì‘í•˜ëŠ”ì§€ ì°¾ìŒ (ì—†ìœ¼ë©´ -1)
                        try:
                            start_idx = txt.rfind(ans) if ans else -1
                        except Exception:
                            start_idx = -1
                        answer_starts.append(start_idx)

                    # offsetsë¥¼ ì–»ê¸° ìœ„í•´ ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆ ë” í˜¸ì¶œ
                    tok_out = tokenizer(
                        texts,
                        return_offsets_mapping=True,
                        padding=self.text_processing.get('padding', True),
                        truncation=self.text_processing.get('truncation', True),
                        max_length=self.text_processing.get('max_length', 2048),
                        add_special_tokens=True
                    )
                    offsets = tok_out.get('offset_mapping')

                    if offsets is not None:
                        # ë°°ì¹˜ ì°¨ì› ì •ë ¬ í™•ì¸ (íŒ¨ë”©ìœ¼ë¡œ ê¸¸ì´ í†µì¼ë˜ì—ˆìŒ)
                        for i in range(labels.size(0)):
                            ans_start = answer_starts[i]
                            if ans_start is None or ans_start < 0:
                                # ë‹µë³€ ìœ„ì¹˜ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°: í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•˜ì§€ ì•Šê³  íŠ¹ìˆ˜ í† í°ë§Œ ë¬´ì‹œ
                                continue
                            seq_offsets = offsets[i]
                            # offsets ê¸¸ì´ê°€ labels ê¸¸ì´ì™€ ë™ì¼í•´ì•¼ í•¨
                            L = min(len(seq_offsets), labels.size(1))
                            for j in range(L):
                                # (start, end) = (0, 0)ì¸ í† í°ì€ ë³´í†µ special
                                st, ed = seq_offsets[j]
                                # ë‹µë³€ ì‹œì‘ ì´ì „(end <= ans_start)ì¸ í† í°ì€ í”„ë¡¬í”„íŠ¸ë¡œ ê°„ì£¼í•˜ê³  ë§ˆìŠ¤í‚¹
                                if ed <= ans_start:
                                    labels[i, j] = ignore_index
                    else:
                        print("âš ï¸ Token offsets not available; skipping prompt masking (only special tokens masked)")
                except Exception as e:
                    print(f"âš ï¸ Prompt masking failed: {e}; falling back to special-token-only masking")

            # # ì„ íƒì  ì¶”ê°€ ë§ˆìŠ¤í‚¹ ì˜µì…˜(ìœ ì§€)
            # if self.label_masking.get('mask_input_tokens', False):
            #     # ì´ë¯¸ ìœ„ì—ì„œ í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ë³„ë„ ë™ì‘ ë¶ˆí•„ìš”
            #     pass

            batch["labels"] = labels

        return batch
    
    def _process_with_processor(self, texts: List[str], visual_data: List[List[Image.Image]]) -> Dict[str, torch.Tensor]:
        """í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì‹œê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        - ê° ìƒ˜í”Œë³„ë¡œ ë‹¤ì¤‘ ì´ë¯¸ì§€/í”„ë ˆì„ì„ ì§€ì›í•©ë‹ˆë‹¤.
        - visual_dataëŠ” len(texts)ì™€ ë™ì¼í•œ ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ì´ë©°, ê° ì›ì†ŒëŠ” í•´ë‹¹ ìƒ˜í”Œì˜ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        """

        # ì…ë ¥ ì •ê·œí™”: None -> [] , ë‹¨ì¼ ì´ë¯¸ì§€ -> [image]
        images_per_sample: List[List[Image.Image]] = []
        has_any_image = False
        for visuals in visual_data:
            if visuals is None:
                images_per_sample.append([])
                continue
            # ì¼ë¶€ êµ¬í˜„ì—ì„œ visualsê°€ ë‹¨ì¼ ì´ë¯¸ì§€ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì •
            if isinstance(visuals, Image.Image):
                images_per_sample.append([visuals])
                has_any_image = True
            elif isinstance(visuals, (list, tuple)):
                # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ íƒ€ì… í™•ì¸ í›„ PILì´ ì•„ë‹Œ ê²½ìš° í•„í„°ë§
                valid_imgs = [v for v in visuals if isinstance(v, Image.Image)]
                images_per_sample.append(valid_imgs)
                if len(valid_imgs) > 0:
                    has_any_image = True
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… -> ë¹ˆ ì²˜ë¦¬
                images_per_sample.append([])

        if has_any_image:
            try:
                # ìƒ˜í”Œë³„ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì§€ì›: images=[ [img1,img2], [], [img3] , ... ]
                batch = self.processor(
                    text=texts,
                    images=images_per_sample,
                    return_tensors=self.batch_processing.get('return_tensors', 'pt'),
                    padding=self.text_processing.get('padding', True),
                    truncation=self.text_processing.get('truncation', True),
                    max_length=self.text_processing.get('max_length', 2048)
                )
                return batch
            except Exception as e:
                print(f"âš ï¸ Error processing with multi-image inputs, trying text-only: {e}")

        # ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
        batch = self.processor(
            text=texts,
            return_tensors=self.batch_processing.get('return_tensors', 'pt'),
            padding=self.text_processing.get('padding', True),
            truncation=self.text_processing.get('truncation', True),
            max_length=self.text_processing.get('max_length', 2048)
        )

        return batch

def create_vlm_collator(processor, config_path: str = 'vlm_collator_config.yaml'):
    """
    VLM ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        processor: VLM í”„ë¡œì„¸ì„œ
        config_path: ì½œë ˆì´í„° ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        VLMDataCollator: ì„¤ì •ëœ ë°ì´í„° ì½œë ˆì´í„°
    """
    config = load_collator_config(config_path)
    return VLMDataCollator(processor=processor, config=config)
