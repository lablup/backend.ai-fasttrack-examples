#!/usr/bin/env python3
"""
VLM Fine-tuning Trainer
VLM ëª¨ë¸ì„ ìœ„í•œ íŒŒì¸íŠœë‹ íŠ¸ë ˆì´ë„ˆ
ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬
"""

import os
import json
import argparse
import torch
import sys
import yaml
import wandb
from pathlib import Path
from trl import SFTTrainer, SFTConfig
from transformers import TrainingArguments
from datasets import DatasetDict, load_from_disk
from peft import LoraConfig

from src.models.model import ModelLoader
from src.data.collate_fn import create_vlm_collator
from configs.settings import settings

def parse_args():
    parser = argparse.ArgumentParser(description="VLM Trainer Configuration")
    parser.add_argument('--model_id', type=str, default=os.getenv('MODEL_ID'),
                        help='ID of the VLM model to load from Hugging Face Hub')
    parser.add_argument('--dataset-name', type=str, default=os.getenv('DATASET'),
                        help='Name of the dataset to load from Hugging Face Hub')
    parser.add_argument('--train_config_path', type=str, required=True,
                        help='Path to YAML file for SFTConfig (e.g., train_config.yaml)')
    parser.add_argument('--peft_config_path', type=str, required=True,
                        help='Path to YAML file for PEFT config')
    parser.add_argument('--vlm_model_config', type=str, required=True,
                        help='Path to VLM model configuration file')
    parser.add_argument('--vlm_collator_config', type=str, required=True,
                        help='Path to VLM collator configuration file')
    parser.add_argument('--wandb_token', type=str, default=os.getenv('WANDB_API_KEY'),
                        help='Weights & Biases API token for logging.')
    parser.add_argument('--wandb_project', type=str, default="vlm-finetuning",
                        help='Weights & Biases project name.')
    return parser.parse_args()

class VLMTrainer:
    """VLM ëª¨ë¸ì„ ìœ„í•œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤"""
    
    def __init__(self, model_loader, output_dir, dataset_path, vlm_collator_config='vlm_collator_config.yaml'):
        self.model_loader = model_loader
        self.model = self.model_loader.model
        self.processor = self.model_loader.processor
        self.tokenizer = self.model_loader.tokenizer
        self.output_dir = output_dir
        self.vlm_collator_config = vlm_collator_config
        
        # VLM ë°ì´í„° ì½œë ˆì´í„° ìƒì„±
        if self.processor:
            print(f"Creating VLM data collator with config: {vlm_collator_config}")
            self.data_collator = create_vlm_collator(
                processor=self.processor,
                config_path=vlm_collator_config
            )
            print("âœ… VLM data collator created successfully")
        else:
            print("âŒ Cannot create VLM data collator: processor not available")
            self.data_collator = None
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            print(f"Loading dataset from: {dataset_path}")
            self.dataset = load_from_disk(dataset_path)
            print(f"âœ… Dataset loaded successfully with splits: {list(self.dataset.keys())}")
        except FileNotFoundError:
            print(f"âŒ Dataset not found at: {dataset_path}")
            self.dataset = None

    def train(self, train_config_dict=None, peft_config_dict=None, logging_dir=None):
        """VLM ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        if not self.model or not self.processor or not self.data_collator:
            print("âŒ Model, processor, or data collator is not loaded. Cannot proceed with training.")
            return
        
        if not self.dataset:
            print("âŒ Dataset is not loaded. Cannot proceed with training.")
            return
        
        # í•™ìŠµ ì„¤ì • (self. ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€)
        if train_config_dict:
            print("Using custom VLM training arguments from YAML file...")
            train_config = SFTConfig(
                output_dir=self.output_dir,
                logging_dir=logging_dir,
                **train_config_dict
            )
        else:
            print("Using default VLM training arguments...")
            train_config = SFTConfig(
                output_dir=self.output_dir,
                logging_dir=logging_dir,
                per_device_train_batch_size=4,  # VLMì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ì‘ê²Œ ì„¤ì •
                per_device_eval_batch_size=4,
                gradient_accumulation_steps=4,  # ë°°ì¹˜ í¬ê¸° ë³´ìƒ
                gradient_checkpointing=True,
                learning_rate=1e-5,
                num_train_epochs=1,
                logging_steps=0.1,
                eval_steps=0.1,
                save_steps=0.1,
                bf16=True,
                remove_unused_columns=False,  # VLMì—ì„œëŠ” ì´ë¯¸ì§€ ë°ì´í„° ë³´ì¡´ í•„ìš”
                dataloader_pin_memory=False,
            )
        
        # PEFT ì„¤ì •
        if peft_config_dict:
            print("Using custom PEFT configuration...")
            peft_config = LoraConfig(**peft_config_dict)
        else:
            print("Using default PEFT configuration for VLM...")
            peft_config = LoraConfig(
                task_type="CAUSAL_LM",
                r=32,  # VLMì€ ë” ë³µì¡í•˜ë¯€ë¡œ r ê°’ì„ ì¡°ê¸ˆ ë†’ê²Œ
                target_modules="all-linear",
                lora_alpha=16,
                lora_dropout=0.05,
                bias="none"
            )
        
        # SFT íŠ¸ë ˆì´ë„ˆ ìƒì„± (VLM processor ì‚¬ìš©)
        # ì¤‘ìš”: Qwen2VLProcessorëŠ” ì§ì ‘ eos_token ì†ì„±ì´ ì—†ìœ¼ë¯€ë¡œ tokenizerë¥¼ ì§ì ‘ ì „ë‹¬
        trainer = SFTTrainer(
            model=self.model,  # self. ì§ì ‘ ì‚¬ìš©
            args=train_config,
            train_dataset=self.dataset['train'],
            eval_dataset=self.dataset.get('validation'),
            tokenizer=self.tokenizer,  # tokenizerë¥¼ ì§ì ‘ ì „ë‹¬ (processor ëŒ€ì‹ )
            peft_config=peft_config,
            data_collator=self.data_collator,  # VLM ì „ìš© ë°ì´í„° ì½œë ˆì´í„° ì‚¬ìš©
        )
        
        print("ğŸš€ Starting VLM fine-tuning...")
        train_result = trainer.train()
        trainer_stats = train_result.metrics
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        if logging_dir:
            log_file_path = logging_dir / "training_stats.json"
            print(f"Saving training stats to {log_file_path}")
            with open(log_file_path, 'w', encoding='utf-8') as f:
                import json
                json.dump(trainer_stats, f, ensure_ascii=False, indent=4)
        
        print("ğŸ’¾ Saving VLM PEFT adapter...")
        trainer.save_model()  # PEFT ì–´ëŒ‘í„°ë§Œ ì €ì¥
        
        # ë³‘í•©ëœ ë°°í¬ìš© ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì • (settingsì—ì„œ ê´€ë¦¬)
        deployment_model_path = settings.deployment_model_path
        deployment_model_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ”„ Merging LoRA weights and saving deployment-ready VLM model to {deployment_model_path}")
        try:
            # PEFT ëª¨ë¸ì—ì„œ ë³‘í•©ëœ ëª¨ë¸ ìƒì„±
            merged_model = trainer.model.merge_and_unload()
            
            # ë³‘í•©ëœ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ì €ì¥
            merged_model.save_pretrained(
                deployment_model_path,
                safe_serialization=True,  # ì•ˆì „í•œ í…ì„œ í¬ë§·ìœ¼ë¡œ ì €ì¥
                max_shard_size="5GB"  # í° ëª¨ë¸ì„ ìœ„í•œ ìƒ¤ë”©
            )
            
            # VLM ëª¨ë¸ì˜ ê²½ìš° processorì™€ tokenizer ëª¨ë‘ ì €ì¥
            if self.processor:
                self.processor.save_pretrained(deployment_model_path)
                print(f"âœ… VLM processor saved to {deployment_model_path}")
                
            if self.tokenizer:
                self.tokenizer.save_pretrained(deployment_model_path)
                print(f"âœ… VLM tokenizer saved to {deployment_model_path}")
            
            print(f"âœ… Successfully saved deployment-ready VLM model to {deployment_model_path}")
            print(f"âœ… VLM model is ready for deployment or distribution")
            
        except Exception as e:
            print(f"âŒ Error during VLM model merging: {e}")
            print("Falling back to adapter-only save")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (self. ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€)
        print("ğŸ§¹ Clearing GPU cache to free up memory...")
        del trainer
        del self.model  # ì¼ê´€ì„±ì„ ìœ„í•´ self.model ì§ì ‘ ì°¸ì¡°
        import torch
        torch.cuda.empty_cache()
        
        print(f"âœ… VLM fine-tuning completed successfully!")
        print(f"ğŸ“‚ PEFT adapter saved to: {self.output_dir}")
        print(f"ğŸ“‚ Deployment-ready model saved to: {deployment_model_path}")

def main():
    args = parse_args()
    
    # VLM ëª¨ë¸ ë¡œë” ìƒì„± (VLM ì„¤ì • í¬í•¨)
    model_loader = ModelLoader(args.model_id, vlm_config_path=args.vlm_model_config)

    # --- WandB ì¡°ê±´ë¶€ ì„¤ì • ---
    if args.wandb_token:
        wandb.login(key=args.wandb_token)
        os.environ["WANDB_PROJECT"] = args.wandb_project
        print(f"WandB initialized for project: {args.wandb_project}")
        report_to = ["wandb"]
    else:
        print("WandB token not provided. Skipping WandB initialization.")
        report_to = []
        
    # --- ì„¤ì • íŒŒì¼ ë° ê²½ë¡œ ë¡œë“œ (settings.py ì‚¬ìš©) ---
    print("Loading configurations from YAML files...")

    train_config_dict = None
    peft_config_dict = None

    if args.train_config_path:
        train_config_path = settings.config_path / args.train_config_path
        with open(train_config_path, 'r') as f:
            train_config_dict = yaml.safe_load(f)
        print(f"âœ… Loaded training configuration from: {train_config_path}")
    
    if args.peft_config_path:
        peft_config_path = settings.config_path / args.peft_config_path
        with open(peft_config_path, 'r') as f:
            peft_config_dict = yaml.safe_load(f)
        print(f"âœ… Loaded PEFT configuration from: {peft_config_path}")
    
    # CLI ì¡°ê±´ì— ë”°ë¼ report_to ê°’ì„ ë®ì–´ì“°ê¸°
    if train_config_dict:
        train_config_dict['report_to'] = report_to
    
    if not model_loader.model or not model_loader.processor:
        print("âŒ Failed to load VLM model or processor. Cannot proceed with training.")
        return

    print("Output directory is not specified. Using default settings.")
    output_dir = settings.save_model_path
    output_dir.mkdir(parents=True, exist_ok=True)

    logging_dir = settings.logging_dir
    logging_dir.mkdir(parents=True, exist_ok=True)

    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • - íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì´ì „ taskì˜ outputì„ input1ì—ì„œ ì½ìŒ
    if settings.is_pipeline_env:
        dataset_path = settings.pipeline_input_path
    else:
        dataset_path = settings.save_dataset_path_raw  # VLMì€ ì›ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš© (ì´ë¯¸ì§€ í¬í•¨)

    # VLM íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì‹¤í–‰
    trainer = VLMTrainer(
        model_loader, 
        output_dir, 
        dataset_path, 
        vlm_collator_config=args.vlm_collator_config
    )
    trainer.train(
        train_config_dict=train_config_dict, 
        peft_config_dict=peft_config_dict, 
        logging_dir=logging_dir
    )

if __name__ == "__main__":
    main()
