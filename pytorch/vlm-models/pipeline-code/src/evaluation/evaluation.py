import os
import argparse
import json
import torch
import logging
import sys
import re
from pathlib import Path
from datasets import load_from_disk  # 'load_dataset' ëŒ€ì‹  'load_from_disk'ë¥¼ ì‚¬ìš©
from transformers import (
    AutoModelForCausalLM,
    AutoModelForVision2Seq,
    AutoProcessor,
    pipeline,
)
from peft import PeftModel
from tqdm import tqdm
import evaluate

from src.models.model import ModelLoader
from src.data.collate_fn import create_vlm_collator
from configs.settings import settings

# tqdmì„ í†µí•œ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ ë¡œê¹… ë ˆë²¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.
logging.getLogger("transformers.pipelines.pt_utils").setLevel(logging.ERROR)

def detect_language(text_sample):
    """
    í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë¶„ì„í•˜ì—¬ ì–¸ì–´ë¥¼ ê°ì§€í•˜ëŠ” í•¨ìˆ˜ (í•œêµ­ì–´/ì˜ì–´/ê¸°íƒ€ë¡œ ê°„ì†Œí™”)
    
    Args:
        text_sample (str): ë¶„ì„í•  í…ìŠ¤íŠ¸ ìƒ˜í”Œ
        
    Returns:
        dict: {"language": str, "confidence": str}
    """
    if not text_sample or len(text_sample.strip()) == 0:
        return {"language": "other", "confidence": "low"}
    
    text = text_sample[:500]  # ì²˜ìŒ 500ìë§Œ ë¶„ì„
    
    # í•œê¸€ ë¬¸ì ê°ì§€
    korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
    
    # ì˜ì–´ ë¬¸ì ê°ì§€ (ë¼í‹´ ë¬¸ì + ì˜ì–´ ë‹¨ì–´ íŒ¨í„´)
    latin_chars = len(re.findall(r'[a-zA-Z]', text))
    english_words = len(re.findall(r'\b(the|and|or|but|in|on|at|to|for|of|with|by|is|are|was|were|a|an|this|that|will|have|has|can|could|would|should)\b', text.lower()))
    
    total_chars = len(re.findall(r'[^\s\d\W]', text))
    total_words = len(re.findall(r'\b\w+\b', text))
    
    if total_chars == 0:
        return {"language": "other", "confidence": "low"}
    
    # í•œêµ­ì–´ ë¹„ìœ¨ ê³„ì‚°
    korean_ratio = korean_chars / total_chars
    
    # ì˜ì–´ ë¹„ìœ¨ ê³„ì‚° (ë¼í‹´ ë¬¸ì ë¹„ìœ¨ + ì˜ì–´ ë‹¨ì–´ ë¹„ìœ¨)
    latin_ratio = latin_chars / total_chars
    english_word_ratio = english_words / total_words if total_words > 0 else 0
    
    # ì–¸ì–´ ê²°ì • ë¡œì§
    if korean_ratio > 0.3:  # í•œêµ­ì–´ ë¬¸ìê°€ 30% ì´ìƒ
        return {"language": "korean", "confidence": "high" if korean_ratio > 0.7 else "medium"}
    
    elif latin_ratio > 0.5 and english_word_ratio > 0.05:  # ë¼í‹´ ë¬¸ì 50% + ì˜ì–´ ë‹¨ì–´ 5% ì´ìƒ
        confidence = "high" if english_word_ratio > 0.15 else "medium"
        return {"language": "english", "confidence": confidence}
    
    else:  # ê¸°íƒ€ ì–¸ì–´ (ì¤‘êµ­ì–´, ì¼ë³¸ì–´, ì•„ëì–´ ë“±)
        return {"language": "other", "confidence": "medium"}

def get_bertscore_model_config(language_info):
    """
    ì–¸ì–´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ BERTScore ëª¨ë¸ ì„¤ì •ì„ ë°˜í™˜ (ê°„ì†Œí™”ëœ ë²„ì „)
    
    Args:
        language_info (dict): detect_language í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
        
    Returns:
        dict: {"model_type": str, "lang": str, "description": str}
    """
    language = language_info["language"]
    confidence = language_info["confidence"]
    
    # í•œêµ­ì–´: ë‹¤êµ­ì–´ BERT ì‚¬ìš© (í•œêµ­ì–´ì— ìµœì í™”)
    if language == "korean":
        return {
            "model_type": "bert-base-multilingual-cased",
            "lang": "ko",
            "description": "Multilingual BERT for Korean"
        }
    
    # ì˜ì–´: DeBERTa-v3 ì‚¬ìš© (ì˜ì–´ ì„±ëŠ¥ ìš°ìˆ˜)
    elif language == "english" and confidence == "high":
        return {
            "model_type": "microsoft/deberta-v3-large",
            "lang": "en", 
            "description": "DeBERTa-v3 for English"
        }
    
    # ê¸°íƒ€ ëª¨ë“  ì–¸ì–´ ë˜ëŠ” ë‚®ì€ ì‹ ë¢°ë„: ë‹¤êµ­ì–´ BERT ì‚¬ìš©
    else:
        return {
            "model_type": "bert-base-multilingual-cased",
            "lang": None,  # ìë™ ì–¸ì–´ ê°ì§€
            "description": "Multilingual BERT for other languages"
        }

def parse_args():
    """CLI ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Evaluate a model on a pre-processed local dataset.")
    parser.add_argument('--model_name_or_path', type=str, default=os.getenv('MODEL_ID'),required=True, help='Base model identifier')
    # '--dataset_name'ì„ '--dataset_path'ë¡œ ë³€ê²½í•˜ì—¬ ë¡œì»¬ ê²½ë¡œë¥¼ ë°›ìŠµë‹ˆë‹¤.
    parser.add_argument('--output_path', type=str, default="output.json", help='Path to save the evaluation results.')
    parser.add_argument('--max_samples', type=int, default=None, help='Optional: Maximum number of samples for quick testing.')
    parser.add_argument('--use_adapter', action='store_true', 
                        help='Use a PEFT adapter if this flag is set. If not, evaluates the base model.')
    return parser.parse_args()

def load_model_for_evaluation(model_name_or_path, use_finetuned=False):
    """ë² ì´ìŠ¤ ëª¨ë¸ ë˜ëŠ” íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (VLM ì§€ì›)"""
    
    # VLM ModelLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
    print(f"Loading VLM model for evaluation: {model_name_or_path}")
    model_loader = ModelLoader(model_name_or_path)
    
    if not model_loader.model or not model_loader.processor:
        print(f"âŒ Failed to load VLM model: {model_name_or_path}")
        return None, None
    
    if use_finetuned:
        # 1. ë¨¼ì € ë°°í¬ìš© ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        deployment_model_path = settings.deployment_model_path
        if deployment_model_path.exists() and (deployment_model_path / "config.json").exists():
            print(f"Found deployment-ready fine-tuned model at: {deployment_model_path}")
            try:
                # VLM ëª¨ë¸ì„ ë¡œë“œí•˜ê³  PEFT ì–´ëŒ‘í„°ëŠ” ë³„ë„ ì²˜ë¦¬
                finetuned_model_loader = ModelLoader(str(deployment_model_path))
                if finetuned_model_loader.model:
                    return finetuned_model_loader.model, finetuned_model_loader.processor
                else:
                    print("âš ï¸ Failed to load deployment model, trying PEFT adapter approach...")
            except Exception as e:
                print(f"âš ï¸ Error loading deployment model: {e}")
                print("ğŸ”„ Falling back to PEFT adapter approach...")
        
        # 2. PEFT ì–´ëŒ‘í„° ì ‘ê·¼ë²•
        adapter_path = settings.save_model_path
        if adapter_path.exists():
            print(f"Loading PEFT adapter from: {adapter_path}")
            try:
                # ë² ì´ìŠ¤ VLM ëª¨ë¸ ë¡œë“œ
                base_model = model_loader.model
                
                # PEFT ì–´ëŒ‘í„° ì ìš©
                from peft import PeftModel
                model = PeftModel.from_pretrained(base_model, str(adapter_path))
                print("âœ… Successfully loaded fine-tuned VLM model with PEFT adapter")
                return model, model_loader.processor
                
            except Exception as e:
                print(f"âŒ Error loading PEFT adapter: {e}")
                print("ğŸ”„ Falling back to base model")
                return model_loader.model, model_loader.processor
        else:
            print(f"âš ï¸ PEFT adapter not found at: {adapter_path}")
            print("ğŸ”„ Using base model for evaluation")
            return model_loader.model, model_loader.processor
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë°˜í™˜
    print("âœ… Using base VLM model for evaluation")
    return model_loader.model, model_loader.processor

def main():
    args = parse_args()
    print("--- Starting VLM Evaluation Script on Local Dataset ---")
    
    # 1. í‰ê°€ ì§€í‘œ ë¡œë“œ - í•œêµ­ì–´ í…ìŠ¤íŠ¸ í‰ê°€ì— ì í•©í•œ ë©”íŠ¸ë¦­ ì‚¬ìš©
    print("Loading evaluation metrics...")
    rouge_metric = evaluate.load("rouge")
    bertscore_metric = evaluate.load("bertscore")
    
    # BLEU ë©”íŠ¸ë¦­ ì¶”ê°€ (ë‹¤êµ­ì–´ ì§€ì›)
    try:
        bleu_metric = evaluate.load("bleu")
        print("âœ… BLEU metric loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to load BLEU metric: {e}")
        bleu_metric = None
    
    # Perplexity ë©”íŠ¸ë¦­ ì¶”ê°€ (ì–¸ì–´ ëª¨ë¸ í’ˆì§ˆ í‰ê°€)
    try:
        perplexity_metric = evaluate.load("perplexity", module_type="metric")
        print("âœ… Perplexity metric loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Failed to load Perplexity metric: {e}")
        perplexity_metric = None
    
    # 2. VLM ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
    print(f"Loading VLM model and processor: {args.model_name_or_path}")
    model, processor = load_model_for_evaluation(args.model_name_or_path, use_finetuned=args.use_adapter)
    
    if model is None or processor is None:
        print("âŒ Failed to load VLM model or processor")
        return

    # 3. VLM ë°ì´í„° ì½œë ˆì´í„° ìƒì„± (evaluationìš©)
    print("Creating VLM data collator for evaluation...")
    try:
        vlm_collator = create_vlm_collator(processor, config_path='vlm_collator_config.yaml')
        print("âœ… VLM collator created successfully")
    except Exception as e:
        print(f"âŒ Failed to create VLM collator: {e}")
        return
    
    # 4. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • - íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì´ì „ taskì˜ outputì„ input1ì—ì„œ ì½ìŒ
    if settings.is_pipeline_env:
        readonly_dataset_path = settings.pipeline_input_path
        # ì½ê¸° ì „ìš© ê²½ë¡œë¥¼ ì“°ê¸° ê°€ëŠ¥í•œ ì„ì‹œ ê²½ë¡œë¡œ ë³µì‚¬
        dataset_path = settings.copy_readonly_to_writable(readonly_dataset_path, 'evaluation')
    else:
        dataset_path = settings.save_dataset_path_raw
        
    try:
        # load_from_diskë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ëœ ë°ì´í„°ì…‹ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        full_dataset = load_from_disk(dataset_path)
        # í‰ê°€ì—ëŠ” 'test' ìŠ¤í”Œë¦¿ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        test_dataset = full_dataset['test']
        print(f"Successfully loaded test split from {dataset_path}")
        print(f"Test dataset columns: {test_dataset.column_names}")
        print(f"Test dataset size: {len(test_dataset)}")
    except FileNotFoundError:
        print(f"Error: Dataset directory not found at {dataset_path}")
        return
    except Exception as e:
        print(f"Failed to load dataset from disk: {e}")
        return

    # 5. í‰ê°€ ì‹¤í–‰: collate_fnì„ ì‚¬ìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬
    if args.max_samples:
        test_dataset = test_dataset.select(range(args.max_samples))
        print(f"Limited to {args.max_samples} samples for evaluation")
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ)
    batch_size = int(os.getenv('EVAL_BATCH_SIZE', 4))  # VLMì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì„
    
    all_predictions = []
    all_references = []
    all_prompts = []
    
    print(f"Evaluating on {len(test_dataset)} samples with batch size {batch_size}...")
    
    # VLM ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ìƒì„± ì„¤ì •
    generation_config = {
        "max_new_tokens": 256,
        "do_sample": False,  # deterministic generation for evaluation
        "temperature": 0.0,
    }
    
    # pad_token_idì™€ eos_token_id ì„¤ì •
    try:
        tokenizer = getattr(processor, 'tokenizer', processor)
        if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:
            generation_config["pad_token_id"] = tokenizer.pad_token_id
        if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
            generation_config["eos_token_id"] = tokenizer.eos_token_id
    except Exception as e:
        print(f"âš ï¸ Could not set token ids: {e}")
    
    # ë°ì´í„°ì…‹ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
    for i in tqdm(range(0, len(test_dataset), batch_size), desc="Evaluating"):
        try:
            # í˜„ì¬ ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            batch_data = []
            batch_references = []
            
            for j in range(i, min(i + batch_size, len(test_dataset))):
                example = test_dataset[j]
                batch_data.append(example)
                
                # ì°¸ì¡° ë‹µë³€ ì¶”ì¶œ (ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •)
                answer_col = vlm_collator.dataset_columns.get('answer_column', 'answer')
                if answer_col in example and example[answer_col]:
                    batch_references.append(str(example[answer_col]).strip())
                else:
                    # fallback: ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…ë“¤ ì‹œë„
                    possible_answer_cols = ['answer', 'text', 'label', 'response', 'target']
                    ref_found = False
                    for col in possible_answer_cols:
                        if col in example and example[col]:
                            batch_references.append(str(example[col]).strip())
                            ref_found = True
                            break
                    if not ref_found:
                        batch_references.append("[NO_REFERENCE]")
            
            if not batch_data:
                continue
                
            # collate_fnì„ ì‚¬ìš©í•˜ì—¬ evaluationìš© ë°ì´í„° ì¤€ë¹„
            # evaluation ëª¨ë“œë¡œ messages í˜•ì‹ ì„¤ì •
            vlm_collator.text_processing['add_generation_prompt'] = True  # evaluationìš© prompt ì¶”ê°€
            
            try:
                # collatorë¥¼ í†µí•´ ë°°ì¹˜ ì „ì²˜ë¦¬
                processed_batch = vlm_collator(batch_data)
                
                # ëª¨ë¸ ì¶”ë¡ 
                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v 
                         for k, v in processed_batch.items() if k != 'labels'}
                
                with torch.no_grad():
                    generated_ids = model.generate(**inputs, **generation_config)
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
                # ì…ë ¥ ê¸¸ì´ë§Œí¼ ì œê±°í•˜ê³  ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                input_length = inputs['input_ids'].shape[1]
                generated_tokens = generated_ids[:, input_length:]
                
                # ë°°ì¹˜ ë””ì½”ë”©
                batch_predictions = processor.batch_decode(
                    generated_tokens, 
                    skip_special_tokens=True
                )
                
                # ê²°ê³¼ ì •ë¦¬
                for pred in batch_predictions:
                    cleaned_pred = pred.strip() if pred.strip() else "[EMPTY_GENERATION]"
                    all_predictions.append(cleaned_pred)
                
                all_references.extend(batch_references)
                
                # í”„ë¡¬í”„íŠ¸ ì •ë³´ë„ ì €ì¥ (ë””ë²„ê¹…ìš©)
                for example in batch_data:
                    question_col = vlm_collator.dataset_columns.get('question_column', 'question')
                    question = example.get(question_col, "[NO_QUESTION]")
                    all_prompts.append(str(question))
                    
            except Exception as e:
                print(f"âš ï¸ Error processing batch {i//batch_size + 1}: {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ë¡œ ì±„ì›€
                for _ in range(len(batch_data)):
                    all_predictions.append("[PROCESSING_ERROR]")
                all_references.extend(batch_references)
                all_prompts.extend(["[ERROR]"] * len(batch_data))
                continue
                
        except Exception as e:
            print(f"âŒ Critical error in batch {i//batch_size + 1}: {e}")
            continue

    # 6. ì •ëŸ‰ì  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ë° ê²°ê³¼ ì €ì¥ (ê°œì„ ëœ í‰ê°€ ë©”íŠ¸ë¦­)
    print(f"\nCalculating quantitative metrics...")
    print(f"ğŸ“Š Total samples processed: {len(all_predictions)}")
    print(f"ğŸ“Š Valid predictions: {len([p for p in all_predictions if p not in ['[EMPTY_GENERATION]', '[PROCESSING_ERROR]']])}")
    
    # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
    if not all_predictions or not all_references:
        print("âŒ No valid predictions or references found. Evaluation cannot proceed.")
        return
    
    if len(all_predictions) != len(all_references):
        print(f"âš ï¸ Mismatch in prediction/reference counts: {len(all_predictions)} vs {len(all_references)}")
        min_len = min(len(all_predictions), len(all_references))
        all_predictions = all_predictions[:min_len]
        all_references = all_references[:min_len]
        all_prompts = all_prompts[:min_len]
    
    print(f"ğŸ“Š Valid samples for evaluation: {len(all_predictions)}")
    
    # ROUGE ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ìš”ì•½ í‰ê°€ì˜ í‘œì¤€ ë©”íŠ¸ë¦­)
    print("Computing ROUGE scores...")
    try:
        rouge_scores = rouge_metric.compute(predictions=all_predictions, references=all_references)
        
        # ROUGE ì ìˆ˜ ê²€ì¦
        for metric_name, score in rouge_scores.items():
            if not (0 <= score <= 1):
                print(f"âš ï¸ Unusual ROUGE {metric_name} score: {score:.4f}")
        
        print(f"âœ… ROUGE scores computed successfully")
        
    except Exception as e:
        print(f"âŒ ROUGE calculation failed: {e}")
        rouge_scores = {
            "rouge1": 0.0,
            "rouge2": 0.0,
            "rougeL": 0.0,
            "rougeLsum": 0.0,
            "error": str(e)
        }
    
    # BERTScore ê³„ì‚° - ë‹¤êµ­ì–´ ì§€ì› ë° ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
    print("Computing BERTScore...")
    bert_scores_result = None
    try:
        # ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¶„ì„ (ë” ë§ì€ ìƒ˜í”Œ ì‚¬ìš©)
        sample_texts = []
        sample_size = min(10, len(all_references), len(all_predictions))
        for i in range(sample_size):
            if all_references[i] and all_predictions[i]:
                sample_texts.append(all_references[i][:100] + " " + all_predictions[i][:100])
        
        combined_sample = " ".join(sample_texts)[:1000]  # ìµœëŒ€ 1000ì ë¶„ì„
        
        # ì–¸ì–´ ê°ì§€ ì‹¤í–‰
        language_info = detect_language(combined_sample)
        print(f"ï¿½ Language detection: {language_info['language']} (confidence: {language_info['confidence']})")
        print(f"ğŸ“Š Script ratios: {language_info.get('ratios', {})}")
        
        # ìµœì  BERTScore ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        model_config = get_bertscore_model_config(language_info)
        print(f"ğŸ¯ Selected model: {model_config['description']}")
        
        # BERTScore ê³„ì‚° ì‹¤í–‰
        bert_compute_kwargs = {
            "predictions": all_predictions,
            "references": all_references,
            "model_type": model_config["model_type"]
        }
        
        # ì–¸ì–´ ì½”ë“œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        if model_config["lang"]:
            bert_compute_kwargs["lang"] = model_config["lang"]
        
        bert_scores = bertscore_metric.compute(**bert_compute_kwargs)
        
        # BERTScore ê²°ê³¼ ì²˜ë¦¬ ë° ê²€ì¦
        if bert_scores and 'f1' in bert_scores and len(bert_scores['f1']) > 0:
            # ìœ íš¨í•˜ì§€ ì•Šì€ ì ìˆ˜ í•„í„°ë§ (NaN, ë¬´í•œëŒ€ ë“±)
            valid_f1 = [score for score in bert_scores['f1'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            valid_precision = [score for score in bert_scores['precision'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            valid_recall = [score for score in bert_scores['recall'] if not (torch.isnan(torch.tensor(score)) or torch.isinf(torch.tensor(score)))]
            
            if len(valid_f1) == 0:
                raise ValueError("All BERTScore F1 scores are invalid (NaN/Inf)")
            
            avg_bert_f1 = sum(valid_f1) / len(valid_f1)
            avg_bert_precision = sum(valid_precision) / len(valid_precision)
            avg_bert_recall = sum(valid_recall) / len(valid_recall)
            
            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (BERTScoreëŠ” ë³´í†µ 0~1 ë²”ìœ„)
            if not (0 <= avg_bert_f1 <= 1) or not (0 <= avg_bert_precision <= 1) or not (0 <= avg_bert_recall <= 1):
                print(f"âš ï¸ Unusual BERTScore values detected: F1={avg_bert_f1:.4f}, P={avg_bert_precision:.4f}, R={avg_bert_recall:.4f}")
            
            bert_scores_result = {
                "f1_avg": avg_bert_f1,
                "precision_avg": avg_bert_precision,
                "recall_avg": avg_bert_recall,
                "f1_scores": valid_f1[:5],  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œì˜ ê°œë³„ ì ìˆ˜
                "model_used": model_config["model_type"],
                "language_detected": language_info["language"],
                "language_confidence": language_info["confidence"],
                "valid_samples": len(valid_f1),
                "total_samples": len(bert_scores['f1'])
            }
            print(f"âœ… BERTScore computed successfully (F1: {avg_bert_f1:.4f}, P: {avg_bert_precision:.4f}, R: {avg_bert_recall:.4f})")
            print(f"ğŸ“ˆ Valid/Total samples: {len(valid_f1)}/{len(bert_scores['f1'])}")
        else:
            raise ValueError("Empty or invalid BERTScore results")
            
    except Exception as e:
        print(f"âš ï¸ Primary BERTScore calculation failed: {e}")
        print("ğŸ”„ Trying fallback models...")
        
        # í´ë°± ì‹œí€€ìŠ¤: ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„
        fallback_models = [
            {"model_type": "bert-base-multilingual-cased", "lang": None, "desc": "Multilingual BERT"},
            {"model_type": "distilbert-base-uncased", "lang": "en", "desc": "DistilBERT English"},
            {"model_type": "distilbert-base-multilingual-cased", "lang": None, "desc": "DistilBERT Multilingual"}
        ]
        
        for fallback in fallback_models:
            try:
                print(f"ğŸ”„ Trying {fallback['desc']}...")
                fallback_kwargs = {
                    "predictions": all_predictions,
                    "references": all_references,
                    "model_type": fallback["model_type"]
                }
                if fallback["lang"]:
                    fallback_kwargs["lang"] = fallback["lang"]
                
                bert_scores = bertscore_metric.compute(**fallback_kwargs)
                
                if bert_scores and 'f1' in bert_scores and len(bert_scores['f1']) > 0:
                    avg_bert_f1 = sum(bert_scores['f1']) / len(bert_scores['f1'])
                    avg_bert_precision = sum(bert_scores['precision']) / len(bert_scores['precision'])
                    avg_bert_recall = sum(bert_scores['recall']) / len(bert_scores['recall'])
                    
                    bert_scores_result = {
                        "f1_avg": avg_bert_f1,
                        "precision_avg": avg_bert_precision,
                        "recall_avg": avg_bert_recall,
                        "model_used": fallback["model_type"],
                        "language_detected": "fallback",
                        "language_confidence": "unknown",
                        "fallback_used": True
                    }
                    print(f"âœ… Fallback BERTScore computed with {fallback['desc']} (F1: {avg_bert_f1:.4f})")
                    break
                    
            except Exception as fallback_error:
                print(f"âŒ {fallback['desc']} failed: {fallback_error}")
                continue
        
        # ëª¨ë“  í´ë°± ì‹¤íŒ¨
        if bert_scores_result is None:
            print(f"âŒ All BERTScore models failed")
            bert_scores_result = {
                "f1_avg": 0.0,
                "precision_avg": 0.0,
                "recall_avg": 0.0,
                "error": "All models failed",
                "model_used": "none",
                "language_detected": "unknown",
                "language_confidence": "unknown"
            }
    
    # BLEU ì ìˆ˜ ê³„ì‚° (ê¸°ê³„ ë²ˆì—­/ìƒì„± íƒœìŠ¤í¬ì˜ í‘œì¤€ ë©”íŠ¸ë¦­)
    bleu_scores = None
    if bleu_metric:
        print("Computing BLEU scores...")
        try:
            # BLEUëŠ” referencesë¥¼ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ìš”êµ¬í•¨
            references_for_bleu = [[ref] for ref in all_references]
            bleu_scores = bleu_metric.compute(predictions=all_predictions, references=references_for_bleu)
            
            # BLEU ì ìˆ˜ ê²€ì¦ (0~1 ë²”ìœ„ì—¬ì•¼ í•¨)
            bleu_score = bleu_scores.get('bleu', 0)
            if not (0 <= bleu_score <= 1):
                print(f"âš ï¸ Unusual BLEU score: {bleu_score:.4f}")
            
            print(f"âœ… BLEU score computed: {bleu_score:.4f}")
            
        except Exception as e:
            print(f"âš ï¸ BLEU calculation failed: {e}")
            bleu_scores = {"bleu": 0.0, "error": str(e)}
    else:
        print("âš ï¸ BLEU metric not available")
        bleu_scores = {"bleu": 0.0, "error": "BLEU metric not loaded"}
    
    # ì¶”ê°€ í‰ê°€ ë©”íŠ¸ë¦­ë“¤
    print("Computing additional metrics...")
    
    # 1. í‰ê·  ê¸¸ì´ ë¹„êµ (ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ì¤‘ìš”í•œ ë©”íŠ¸ë¦­)
    pred_lengths = [len(pred.split()) for pred in all_predictions]
    ref_lengths = [len(ref.split()) for ref in all_references]
    
    avg_pred_length = sum(pred_lengths) / len(pred_lengths)
    avg_ref_length = sum(ref_lengths) / len(ref_lengths)
    length_ratio = avg_pred_length / avg_ref_length if avg_ref_length > 0 else 0.0
    
    # ê¸¸ì´ ë¶„í¬ ë¶„ì„
    import statistics
    pred_length_stats = {
        "mean": avg_pred_length,
        "median": statistics.median(pred_lengths),
        "std": statistics.stdev(pred_lengths) if len(pred_lengths) > 1 else 0,
        "min": min(pred_lengths),
        "max": max(pred_lengths)
    }
    
    ref_length_stats = {
        "mean": avg_ref_length,
        "median": statistics.median(ref_lengths),
        "std": statistics.stdev(ref_lengths) if len(ref_lengths) > 1 else 0,
        "min": min(ref_lengths),
        "max": max(ref_lengths)
    }
    
    # 2. Exact Match ë¹„ìœ¨ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ê·œí™”)
    exact_matches = 0
    normalized_exact_matches = 0
    
    for pred, ref in zip(all_predictions, all_references):
        # ì™„ì „ ì¼ì¹˜
        if pred.strip() == ref.strip():
            exact_matches += 1
        
        # ì •ê·œí™”ëœ ì¼ì¹˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ê·œí™”)
        pred_normalized = re.sub(r'\s+', ' ', pred.strip().lower())
        ref_normalized = re.sub(r'\s+', ' ', ref.strip().lower())
        if pred_normalized == ref_normalized:
            normalized_exact_matches += 1
    
    exact_match_ratio = exact_matches / len(all_predictions)
    normalized_exact_match_ratio = normalized_exact_matches / len(all_predictions)
    
    # 3. ë¹ˆ ìƒì„± ë¹„ìœ¨ ì²´í¬
    empty_predictions = sum(1 for pred in all_predictions if len(pred.strip()) == 0)
    empty_prediction_ratio = empty_predictions / len(all_predictions)
    
    # 4. ì—ëŸ¬ ìƒì„± ë¹„ìœ¨ ì²´í¬
    error_predictions = sum(1 for pred in all_predictions if pred.strip() in ["[EMPTY_GENERATION]", "[EXTRACTION_ERROR]"])
    error_prediction_ratio = error_predictions / len(all_predictions)
    
    print(f"âœ… Additional metrics computed:")
    print(f"  - Empty predictions: {empty_predictions}/{len(all_predictions)} ({empty_prediction_ratio:.4f})")
    print(f"  - Error predictions: {error_predictions}/{len(all_predictions)} ({error_prediction_ratio:.4f})")
    print(f"  - Exact matches: {exact_matches}/{len(all_predictions)} ({exact_match_ratio:.4f})")
    print(f"  - Normalized matches: {normalized_exact_matches}/{len(all_predictions)} ({normalized_exact_match_ratio:.4f})")
    
    # ê²°ê³¼ ì •ë¦¬
    summary_metrics = {
        "rouge": rouge_scores,
        "bertscore": bert_scores_result,
        "bleu": bleu_scores,
        "length_analysis": {
            "prediction_stats": pred_length_stats,
            "reference_stats": ref_length_stats,
            "length_ratio": length_ratio
        },
        "exact_match_analysis": {
            "exact_match_ratio": exact_match_ratio,
            "normalized_exact_match_ratio": normalized_exact_match_ratio,
            "exact_matches": exact_matches,
            "normalized_exact_matches": normalized_exact_matches
        },
        "quality_analysis": {
            "empty_prediction_ratio": empty_prediction_ratio,
            "error_prediction_ratio": error_prediction_ratio,
            "empty_predictions": empty_predictions,
            "error_predictions": error_predictions
        },
        "total_samples": len(all_predictions)
    }
    
    sample_results = [
        {
            "prompt": p, 
            "ground_truth": r, 
            "model_prediction": pred,
            "prediction_length": len(pred.split()),
            "reference_length": len(r.split())
        } 
        for p, r, pred in zip(all_prompts, all_references, all_predictions)
    ]

    final_results = {
        "summary_metrics": summary_metrics,
        "sample_results": sample_results
    }
    
    print("\n--- Evaluation Metrics Summary ---")
    print(f"ğŸ“Š Total Samples Evaluated: {len(all_predictions)}")
    
    print(f"\nğŸ” ROUGE Scores:")
    if 'error' in rouge_scores:
        print(f"  - âŒ ROUGE calculation failed: {rouge_scores['error']}")
    else:
        for metric, score in rouge_scores.items():
            print(f"  - {metric.upper()}: {score:.4f}")
    
    print(f"\nğŸ¯ BERTScore:")
    if bert_scores_result:
        print(f"  - Language Detected: {bert_scores_result.get('language_detected', 'unknown')}")
        print(f"  - Language Confidence: {bert_scores_result.get('language_confidence', 'unknown')}")
        print(f"  - Model Used: {bert_scores_result.get('model_used', 'unknown')}")
        print(f"  - F1 Average: {bert_scores_result.get('f1_avg', 0):.4f}")
        print(f"  - Precision Average: {bert_scores_result.get('precision_avg', 0):.4f}")
        print(f"  - Recall Average: {bert_scores_result.get('recall_avg', 0):.4f}")
        if 'valid_samples' in bert_scores_result:
            print(f"  - Valid Samples: {bert_scores_result['valid_samples']}/{bert_scores_result.get('total_samples', 'unknown')}")
        if 'fallback_used' in bert_scores_result:
            print(f"  - âš ï¸ Fallback model used")
        if 'error' in bert_scores_result:
            print(f"  - âŒ Error: {bert_scores_result['error']}")
    else:
        print("  - âŒ BERTScore calculation completely failed")
    
    if bleu_scores and 'bleu' in bleu_scores:
        print(f"\nğŸ“ BLEU Score: {bleu_scores['bleu']:.4f}")
        if 'error' in bleu_scores:
            print(f"  - âš ï¸ Error: {bleu_scores['error']}")
    else:
        print(f"\nğŸ“ BLEU Score: Failed to compute")
    
    print(f"\nğŸ“ Length Analysis:")
    print(f"  - Average Prediction Length: {pred_length_stats['mean']:.1f} words")
    print(f"  - Average Reference Length: {ref_length_stats['mean']:.1f} words")
    print(f"  - Length Ratio (pred/ref): {length_ratio:.2f}")
    print(f"  - Prediction Length Range: {pred_length_stats['min']}-{pred_length_stats['max']} words")
    print(f"  - Reference Length Range: {ref_length_stats['min']}-{ref_length_stats['max']} words")
    
    print(f"\nâœ… Match Analysis:")
    print(f"  - Exact Match Ratio: {exact_match_ratio:.4f} ({exact_matches}/{len(all_predictions)})")
    print(f"  - Normalized Match Ratio: {normalized_exact_match_ratio:.4f} ({normalized_exact_matches}/{len(all_predictions)})")
    
    print(f"\nğŸ” Quality Analysis:")
    print(f"  - Empty Predictions: {empty_prediction_ratio:.4f} ({empty_predictions}/{len(all_predictions)})")
    print(f"  - Error Predictions: {error_prediction_ratio:.4f} ({error_predictions}/{len(all_predictions)})")
    
    if empty_prediction_ratio > 0.1:
        print(f"  - âš ï¸ High empty prediction ratio detected!")
    if error_prediction_ratio > 0.05:
        print(f"  - âš ï¸ High error prediction ratio detected!")
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì • - íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì ì ˆí•œ ê²½ë¡œ ì‚¬ìš©
    if settings.is_pipeline_env:
        # íŒŒì´í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” evaluation ê²°ê³¼ë¥¼ vfrootì— ì €ì¥ (ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
        output_path = settings.evaluation_output_path / args.output_path
    else:
        output_path = settings.evaluation_output_path / args.output_path

    if not output_path.parent.exists():
        output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=4)
        
    print(f"\nEvaluation complete. Results saved to {output_path}")
    
    print("\nClearing GPU memory...")
    del model
    del pipe
    torch.cuda.empty_cache()
    print("GPU memory cleared.")

if __name__ == "__main__":
    main()