import os
import argparse
from pathlib import Path
import torch
import yaml
import importlib
from transformers import AutoModelForCausalLM, AutoModelForVision2Seq, AutoProcessor, AutoTokenizer, AutoModelForImageTextToText

def parse_args():
    parser = argparse.ArgumentParser(description="VLM Model Loader")
    parser.add_argument('--model_id', type=str, default=os.getenv('MODEL_ID'),
                        help='ID of the model to load from Hugging Face Hub')
    parser.add_argument('--vlm_config', type=str, default='vlm_model_config.yaml',
                        help='Path to VLM model configuration file')
    
    return parser.parse_args()

def load_vlm_config(config_path: str) -> dict:
    """VLM ëª¨ë¸ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    from configs.settings import settings
    
    config_file = settings.config_path / config_path
    print(f"Loading VLM model config from: {config_file}")
    
    if not config_file.exists():
        print(f"âš ï¸ VLM config file not found: {config_file}")
        print("ğŸ”„ Using default AutoModel classes")
        return None
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config

def get_model_classes(model_id: str, vlm_config: dict):
    """ëª¨ë¸ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not vlm_config:
        # ê¸°ë³¸ VLM Auto í´ë˜ìŠ¤ ì‚¬ìš©
        return {
            'model_class': AutoModelForImageTextToText,
            'processor_class': AutoProcessor,
            'loading_params': {},
            'processor_params': {}
        }
    
    # ëª¨ë¸ë³„ ì„¤ì • í™•ì¸
    model_classes = vlm_config.get('model_classes', {})
    default_fallback = vlm_config.get('default_fallback', {})
    loading_params = vlm_config.get('loading_params', {})
    processor_params = vlm_config.get('processor_params', {})
    
    if model_id in model_classes:
        model_config = model_classes[model_id]
        print(f"âœ… Found specific configuration for model: {model_id}")
    else:
        model_config = default_fallback
        print(f"âš ï¸ No specific configuration found for {model_id}, using default fallback")
    
    # í´ë˜ìŠ¤ ë™ì  ì„í¬íŠ¸
    try:
        import_path = model_config.get('import_path', 'transformers')
        module = importlib.import_module(import_path)
        
        # model_classëŠ” í•„ìˆ˜, processor_classëŠ” ì„ íƒì‚¬í•­ (ê¸°ë³¸ê°’: AutoProcessor)
        model_class = getattr(module, model_config['model_class'])
        
        # processor_classê°€ ì§€ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if 'processor_class' in model_config:
            processor_class = getattr(module, model_config['processor_class'])
            print(f"ğŸ“¦ Using specific processor class: {model_config['processor_class']}")
        else:
            processor_class = AutoProcessor
            print(f"ğŸ“¦ Using default AutoProcessor (no specific processor_class configured)")
        
        return {
            'model_class': model_class,
            'processor_class': processor_class,
            'loading_params': loading_params,
            'processor_params': processor_params
        }
        
    except (ImportError, AttributeError) as e:
        print(f"âš ï¸ Failed to import classes: {e}")
        print("ğŸ”„ Falling back to VLM Auto classes")
        return {
            'model_class': AutoModelForImageTextToText,
            'processor_class': AutoProcessor,
            'loading_params': {},
            'processor_params': {}
        }

def load_model(model_id, model_class=None, loading_params=None):
    """
    VLM ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì„¤ì •ëœ í´ë˜ìŠ¤ ë˜ëŠ” AutoModelForImageTextToTextì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if model_class is None:
        model_class = AutoModelForImageTextToText
    
    if loading_params is None:
        loading_params = {}
    
    try:
        print(f"Loading VLM model: {model_id} using {model_class.__name__}")
        
        # ê¸°ë³¸ ë¡œë”© íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'device_map': "auto",
            'torch_dtype': torch.bfloat16,
            'token': os.getenv('HF_TOKEN')
        }
        
        # YAML ì„¤ì •ì—ì„œ torch_dtype ë¬¸ìì—´ì„ ì‹¤ì œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        if 'torch_dtype' in loading_params:
            dtype_str = loading_params['torch_dtype']
            if dtype_str == 'torch.bfloat16':
                loading_params['torch_dtype'] = torch.bfloat16
            elif dtype_str == 'torch.float16':
                loading_params['torch_dtype'] = torch.float16
            elif dtype_str == 'torch.float32':
                loading_params['torch_dtype'] = torch.float32
        
        # íŒŒë¼ë¯¸í„° ë³‘í•© (YAML ì„¤ì •ì´ ê¸°ë³¸ê°’ì„ ë®ì–´ì”€)
        final_params = {**default_params, **loading_params}
        
        model = model_class.from_pretrained(model_id, **final_params)
        print(f"âœ… Successfully loaded VLM model: {model_id}")
        return model
        
    except Exception as e:
        print(f"âŒ Error loading VLM model {model_id}: {e}")
        return None

def load_processor_and_tokenizer(model_id, processor_class=None, processor_params=None):
    """
    VLM í”„ë¡œì„¸ì„œì™€ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    ì„¤ì •ëœ í´ë˜ìŠ¤ ë˜ëŠ” AutoProcessorë¥¼ ì‚¬ìš©í•˜ë©°, VLM ëª¨ë¸ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    Returns:
        tuple: (processor, tokenizer) where tokenizer is always accessible
    """
    if processor_class is None:
        processor_class = AutoProcessor
        
    if processor_params is None:
        processor_params = {}
    
    # ê¸°ë³¸ í”„ë¡œì„¸ì„œ íŒŒë¼ë¯¸í„°
    default_params = {
        'token': os.getenv('HF_TOKEN')
    }
    
    # íŒŒë¼ë¯¸í„° ë³‘í•© (processor_paramsê°€ ê¸°ë³¸ê°’ì„ ë®ì–´ì”€)
    final_params = {**default_params, **processor_params}
    
    # 1ì°¨ ì‹œë„: ì„¤ì •ëœ Processor í´ë˜ìŠ¤
    try:
        print(f"Loading VLM processor for model: {model_id} using {processor_class.__name__}")
        processor = processor_class.from_pretrained(model_id, **final_params)
        
        # VLM ëª¨ë¸ì˜ ê²½ìš° processor.tokenizer ì†ì„±ì´ ì¡´ì¬
        try:
            tokenizer = processor.tokenizer
            print("âœ… VLM model detected. Extracted tokenizer from processor.")
            
            # ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ ê´€ë ¨ ì •ë³´ ì œê³µ (ê²½ê³  ì–µì œ ì—†ì´)
            if hasattr(processor, 'video_processor') or 'video' in str(type(processor)).lower():
                print("ğŸ“¹ Video processor detected.")
                print("â„¹ï¸  Note: Video processor deprecation warnings are normal and handled automatically.")
                print("   Files are auto-renamed from preprocessor.json to video_preprocessor.json when saved.")
            
            return processor, tokenizer
        except AttributeError:
            # ì¼ë¶€ VLM ëª¨ë¸ì—ì„œëŠ” processor ìì²´ê°€ tokenizer ê¸°ëŠ¥ì„ í¬í•¨
            print("âœ… VLM processor with integrated tokenizer detected.")
            return processor, processor
            
    except Exception as e:
        print(f"âš ï¸ {processor_class.__name__} loading failed: {e}")
        print("ğŸ”„ Falling back to AutoProcessor...")
        
        # 2ì°¨ ì‹œë„: AutoProcessor (fallback)
        try:
            processor = AutoProcessor.from_pretrained(model_id, **final_params)
            
            try:
                tokenizer = processor.tokenizer
                print("âœ… AutoProcessor fallback successful. Extracted tokenizer.")
                return processor, tokenizer
            except AttributeError:
                print("âœ… AutoProcessor fallback successful. Using processor as tokenizer.")
                return processor, processor
                
        except Exception as processor_error:
            print(f"âš ï¸ AutoProcessor fallback failed: {processor_error}")
            print("ğŸ”„ Trying AutoTokenizer as last resort...")
            
            # 3ì°¨ ì‹œë„: AutoTokenizer (ìµœí›„ì˜ ìˆ˜ë‹¨)
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.getenv('HF_TOKEN'))
                print("âœ… AutoTokenizer loaded successfully (no processor available).")
                return None, tokenizer
                
            except Exception as tokenizer_error:
                print(f"âŒ All loading attempts failed. AutoTokenizer error: {tokenizer_error}")
                return None, None

class ModelLoader:
    def __init__(self, model_id, vlm_config_path='vlm_model_config.yaml'):
        self.model_id = model_id
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.vlm_config = None

        if self.model_id:
            # VLM ì„¤ì • ë¡œë“œ
            self.vlm_config = load_vlm_config(vlm_config_path)
            
            # ëª¨ë¸ë³„ í´ë˜ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            class_config = get_model_classes(self.model_id, self.vlm_config)
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = load_model(
                self.model_id, 
                class_config['model_class'],
                class_config['loading_params']
            )
            
            # í”„ë¡œì„¸ì„œì™€ í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.processor, self.tokenizer = load_processor_and_tokenizer(
                self.model_id,
                class_config['processor_class'],
                class_config['processor_params']
            )

            if not self.tokenizer:
                print(f"âŒ Failed to load tokenizer for VLM model {self.model_id}")
            elif not self.model:
                print(f"âŒ Failed to load VLM model {self.model_id}")
            else:
                # VLM í† í¬ë‚˜ì´ì € ê¸°ë³¸ ì„¤ì •
                if hasattr(self.tokenizer, 'padding_side'):
                    self.tokenizer.padding_side = "left"
                print(f"âœ… Successfully loaded VLM model and processor for {self.model_id}")
                
                # processorê°€ ì—†ì–´ë„ tokenizerê°€ ìˆìœ¼ë©´ ê²½ê³ ë§Œ ì¶œë ¥
                if not self.processor:
                    print("âš ï¸ Processor not available for this VLM model, using tokenizer only")
        else:
            print("âŒ Model ID is not provided. Please set the MODEL_ID environment variable.")