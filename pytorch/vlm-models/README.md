# VLM Fine-tuning Pipeline

μ΄ ν”„λ΅μ νΈλ” Vision-Language Model (VLM)μ— λ€ν•΄ Hugging Face λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•κ³  ν‰κ°€ν•λ” νμ΄ν”„λΌμΈμ„ μ κ³µν•©λ‹λ‹¤.

**μ£Όμ” λ³€κ²½μ‚¬ν•­**: κΈ°μ΅΄ Language Model νμ΄ν”„λΌμΈμ„ VLM νμ΄ν”„λΌμΈμΌλ΅ μ „ν™ν•μ€μµλ‹λ‹¤.

## π€ Quick Start

### ν™κ²½ μ„¤μ •

**1. μλ™ ν™κ²½ μ„¤μ • (κ¶μ¥)**

```bash
# vlm ν”„λ΅μ νΈ λ£¨νΈμ—μ„ μ‹¤ν–‰
bash setup_env.sh
```

μ΄λ―Έ κ°€μƒν™κ²½μ΄ μ΅΄μ¬ν•λ‹¤λ©΄ κ±΄λ„λ›°μ–΄λ„ λ©λ‹λ‹¤.

**2. μλ™ ν™κ²½ μ„¤μ •**

```bash
cd pipeline-code
python -m venv .vlm
source .vlm/bin/activate  # Windows: .vlm\Scripts\activate
pip install -r requirements.txt
```

**3. ν™κ²½ λ³€μ μ„¤μ •**
`pipeline-code/.env` νμΌμ—μ„ λ‹¤μ κ°’λ“¤μ„ ν™•μΈν•μ„Έμ”:

-   `HF_TOKEN`: Hugging Face ν† ν°
-   `MODEL_ID`: μ‚¬μ©ν•  VLM λ¨λΈ repository μ΄λ¦„. κΈ°λ³Έκ°’μ€ `Qwen/Qwen2-VL-2B-Instruct`
-   `DATASET`: μ‚¬μ©ν•  VQA λ°μ΄ν„°μ…‹ repository μ΄λ¦„. κΈ°λ³Έκ°’μ€ `HuggingFaceM4/VQAv2`
-   `VLM_MODEL_CONFIG`: VLM λ¨λΈ μ„¤μ • νμΌ (κΈ°λ³Έκ°’: `vlm_model_config.yaml`)
-   `VLM_COLLATOR_CONFIG`: VLM λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ • νμΌ (κΈ°λ³Έκ°’: `vlm_collator_config.yaml`)
-   `WANDB_API_KEY`: Weights & Biases API ν‚¤ (μ„ νƒμ‚¬ν•­)

### π“ VLM λ¨λΈ λ° λ°μ΄ν„°μ…‹ μ„¤μ • κ°€μ΄λ“

#### μ§€μ›λλ” VLM λ¨λΈ

ν„μ¬ μ„¤μ •λ VLM λ¨λΈλ“¤:

-   **Qwen2-VL**: `Qwen/Qwen2-VL-2B-Instruct`, `Qwen/Qwen2-VL-7B-Instruct`
-   **LLaVA**: `llava-hf/llava-1.5-7b-hf`, `llava-hf/llava-1.5-13b-hf`
-   **InternVL**: `OpenGVLab/InternVL2-2B`
-   **PaliGemma**: `google/paligemma-3b-pt-448`
-   **Phi-3-Vision**: `microsoft/Phi-3-vision-128k-instruct`

#### λ¨λΈλ³„ ν΄λμ¤ μ„¤μ •

`configs/vlm_model_config.yaml`μ—μ„ λ¨λΈλ³„ ν΄λμ¤λ¥Ό μ„¤μ •ν•  μ μμµλ‹λ‹¤:

```yaml
model_classes:
    "Qwen/Qwen2-VL-2B-Instruct":
        model_class: "Qwen2VLForConditionalGeneration"
        processor_class: "Qwen2VLProcessor"
        import_path: "transformers"
```

#### λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ •

`configs/vlm_collator_config.yaml`μ—μ„ λ‹¤μ–‘ν• VQA λ°μ΄ν„°μ…‹μ— λ§κ² μ„¤μ •ν•  μ μμµλ‹λ‹¤:

```yaml
dataset_columns:
    image_column: "image"
    question_column: "question"
    answer_column: "answer"

message_format:
    system_prompt: "Answer briefly."
    training_messages:
        - role: "user"
          content:
              - type: "text"
                text: "{system_prompt}"
              - type: "image"
              - type: "text"
                text: "{question}"
        - role: "assistant"
          content:
              - type: "text"
                text: "{answer}"
```

### VLM νμ΄ν”„λΌμΈ μ‹¤ν–‰

#### λ°©λ²• 1: μ „μ²΄ νμ΄ν”„λΌμΈ ν• λ²μ— μ‹¤ν–‰

```bash
cd pipeline-code
python scripts/vlm_cli.py pipeline \
    --train_config_path train_config.yaml \
    --peft_config_path peft_config.yaml \
    --vlm_model_config vlm_model_config.yaml \
    --vlm_collator_config vlm_collator_config.yaml
```

#### λ°©λ²• 2: κ°λ³„ νƒμ¤ν¬ μ‹¤ν–‰ (κ¶μ¥)

```bash
# Task 1: VQA λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“
python scripts/vlm_cli.py download-dataset

# Task 2: λ² μ΄μ¤ VLM λ¨λΈ ν‰κ°€
python scripts/vlm_cli.py eval-base

# Task 3: VLM λ¨λΈ νμΈνλ‹
python scripts/vlm_cli.py train \
    --train_config_path train_config.yaml \
    --peft_config_path peft_config.yaml \
    --vlm_model_config vlm_model_config.yaml \
    --vlm_collator_config vlm_collator_config.yaml

# Task 4: νμΈνλ‹λ VLM λ¨λΈ ν‰κ°€
python scripts/vlm_cli.py eval-finetuned
```

## π“ νμΌ κµ¬μ΅° λ° λ°μ΄ν„° κ²½λ΅

### κΈ°λ³Έ κ²½λ΅ μ„¤μ •

-   **λ°μ΄ν„°μ…‹ μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/dataset/`
-   **PEFT μ–΄λ‘ν„° μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/model/`
-   **λ°°ν¬μ© λ¨λΈ μ €μ¥ κ²½λ΅**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/deployment_model/`
-   **ν‰κ°€ κ²°κ³Ό μ €μ¥**: `{ν”„λ΅μ νΈ_λ£¨νΈ}/results/evaluation/`

### νμΌ κµ¬μ΅°

```
vlm-models/
β”β”€β”€ README.md
β”β”€β”€ scripts/
β”‚   β””β”€β”€ vlm_cli.py              # VLM νμ΄ν”„λΌμΈ μ‹¤ν–‰ CLI
β””β”€β”€ pipeline-code/
    β”β”€β”€ configs/
    β”‚   β”β”€β”€ vlm_model_config.yaml       # VLM λ¨λΈ ν΄λμ¤ μ„¤μ •
    β”‚   β”β”€β”€ vlm_collator_config.yaml    # VLM λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ •
    β”‚   β”β”€β”€ train_config.yaml           # ν•™μµ μ„¤μ •
    β”‚   β”β”€β”€ peft_config.yaml            # PEFT μ„¤μ •
    β”‚   β””β”€β”€ settings.py                 # μ¤‘μ•™ μ„¤μ • κ΄€λ¦¬
    β”β”€β”€ src/
    β”‚   β”β”€β”€ data/
    β”‚   β”‚   β”β”€β”€ download_dataset.py     # Task 1: λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“
    β”‚   β”‚   β””β”€β”€ collate_fn.py           # VLM λ°μ΄ν„° μ½λ μ΄ν„°
    β”‚   β”β”€β”€ models/
    β”‚   β”‚   β””β”€β”€ model.py                # VLM λ¨λΈ λ΅λ”
    β”‚   β”β”€β”€ training/
    β”‚   β”‚   β””β”€β”€ vlm_trainer.py          # Task 3: VLM νΈλ μ΄λ„
    β”‚   β””β”€β”€ evaluation/
    β”‚       β””β”€β”€ evaluation.py           # Task 2,4: VLM ν‰κ°€
    β””β”€β”€ requirements.txt                # VLM μμ΅΄μ„±
```

## π”§ νƒμ¤ν¬ μƒμ„Έ μ„¤λ…

### Task 1: VQA Dataset Download

-   **νμΌ**: `src/data/download_dataset.py`
-   **μ…λ ¥**: Hugging Face VQA λ°μ΄ν„°μ…‹ (μ: `HuggingFaceM4/VQAv2`)
-   **μ²λ¦¬**: λ°μ΄ν„°μ…‹μ„ train/validation/testλ΅ λ¶„ν• 
-   **μ¶λ ¥**: `dataset/raw/` ν΄λ”μ— μ›λ³Έ λ°μ΄ν„°μ…‹ μ €μ¥ (μ΄λ―Έμ§€ ν¬ν•¨)

### Task 2: ν•™μµ μ „ Base VLM Model Evaluation

-   **μ…λ ¥**: μ›λ³Έ VLM λ¨λΈ, VQA ν…μ¤νΈ λ°μ΄ν„°
-   **μ²λ¦¬**: VQA μ •ν™•λ„, BLEU, ROUGE λ“±μ μ§€ν‘λ΅ λ² μ΄μ¤ λ¨λΈ μ„±λ¥ ν‰κ°€
-   **μ¶λ ¥**: `base_vlm_model_evaluation.json`

### Task 3: VLM Model Fine-tuning

-   **νμΌ**: `src/training/vlm_trainer.py`
-   **μ…λ ¥**: λ² μ΄μ¤ VLM λ¨λΈ, μ›λ³Έ VQA λ°μ΄ν„° (μ΄λ―Έμ§€+ν…μ¤νΈ), μ„¤μ • νμΌλ“¤
-   **μ²λ¦¬**:
    -   VLM μ „μ© λ°μ΄ν„° μ½λ μ΄ν„°λ¥Ό ν†µν•΄ μ΄λ―Έμ§€μ™€ ν…μ¤νΈ λ™μ‹ μ²λ¦¬
    -   LoRAλ¥Ό μ‚¬μ©ν• νλΌλ―Έν„° ν¨μ¨μ  νμΈνλ‹
    -   λ¨λΈλ³„ μµμ ν™”λ ν΄λμ¤ μ‚¬μ©
-   **μ¶λ ¥**:
    -   PEFT μ–΄λ‘ν„°: `results/model/` ν΄λ”
    -   λ°°ν¬μ© μ™„μ „ν• λ¨λΈ: `results/deployment_model/` ν΄λ”

### Task 4: Fine-tuned VLM Model Evaluation

-   **μ…λ ¥**: νμΈνλ‹λ VLM λ¨λΈ, VQA ν…μ¤νΈ λ°μ΄ν„°
-   **μ²λ¦¬**: VQA μ •ν™•λ„, BLEU, ROUGE λ“±μ μ§€ν‘λ΅ νμΈνλ‹λ λ¨λΈ μ„±λ¥ ν‰κ°€
-   **μ¶λ ¥**: `finetuned_vlm_model_evaluation.json`

## π†• VLM νΉν™” κΈ°λ¥

### 1. λ¨λΈλ³„ ν΄λμ¤ μλ™ μ„ νƒ

-   `vlm_model_config.yaml`μ„ ν†µν•΄ λ¨λΈλ³„ μµμ  ν΄λμ¤ μλ™ μ„ νƒ
-   fallback μ‹μ¤ν…μΌλ΅ νΈν™μ„± λ³΄μ¥

### 2. VLM λ°μ΄ν„° μ½λ μ΄ν„°

-   μ΄λ―Έμ§€μ™€ ν…μ¤νΈλ¥Ό λ™μ‹μ— μ²λ¦¬ν•λ” μ»¤μ¤ν…€ μ½λ μ΄ν„°
-   λ‹¤μ–‘ν• VQA λ°μ΄ν„°μ…‹ ν•μ‹ μ§€μ›
-   μ„¤μ • νμΌμ„ ν†µν• μ μ—°ν• μ»¤μ¤ν„°λ§μ΄μ§•

### 3. λ©”λ¨λ¦¬ μµμ ν™”

-   VLMμ λ†’μ€ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ κ³ λ ¤ν• λ°°μΉ ν¬κΈ° μ΅°μ •
-   Gradient checkpointing λ° mixed precision μ§€μ›

### 4. μ΄λ―Έμ§€ μ „μ²λ¦¬

-   PIL μ΄λ―Έμ§€ μλ™ λ³€ν™ λ° RGB λ³€ν™
-   λ‹¤μ–‘ν• μ΄λ―Έμ§€ ν•μ‹ μ§€μ›

## π”„ Language Modelμ—μ„ VLMμΌλ΅μ μ£Όμ” λ³€κ²½μ‚¬ν•­

1. **λ¨λΈ λ΅λ”**: λ‹¤μ–‘ν• VLM λ¨λΈ ν΄λμ¤ μ§€μ›
2. **λ°μ΄ν„° μ²λ¦¬**: μ΄λ―Έμ§€+ν…μ¤νΈ λ™μ‹ μ²λ¦¬
3. **μ½λ μ΄ν„°**: κΈ°μ΅΄ text-onlyμ—μ„ multimodal μ½λ μ΄ν„°λ΅ λ³€κ²½
4. **νμ΄ν”„λΌμΈ**: 4λ‹¨κ³„ VLM νμ΄ν”„λΌμΈ (κΈ°μ΅΄ 6λ‹¨κ³„μ—μ„ λ‹¨μν™”)
5. **μ„¤μ •**: VLM νΉν™” μ„¤μ • νμΌ μ¶”κ°€

## π¨ μ£Όμμ‚¬ν•­

-   VLM λ¨λΈμ€ Language Model λ€λΉ„ λ” λ§μ€ GPU λ©”λ¨λ¦¬ ν•„μ”
-   μ΄λ―Έμ§€κ°€ ν¬ν•¨λ λ°μ΄ν„°μ…‹μ€ μ©λ‰μ΄ ν΄ μ μμ
-   μΌλ¶€ VLM λ¨λΈμ€ νΉμ • λΌμ΄μ„Όμ¤ λ™μ ν•„μ”ν•  μ μμ
